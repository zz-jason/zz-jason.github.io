<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[DuckDB] Push-Based Execution Model | Jian Zhang</title><meta name=keywords content><meta name=description content="1. 背景 DuckDB 是我非常喜欢的一个数据库，它基于 libpg_query 实现了 SQL Parser，语法和 PostgreSQL 一致，内嵌 SQLite 的 REPL CLI，编译好后可直接运行 CLI 交互式输入 SQL 得到结果。架"><meta name=author content="Jian"><link rel=canonical href=https://zz-jason.github.io/posts/duckdb-push-based-execution-model/><link href=/assets/css/stylesheet.min.bdb03579c17662815eb277a528f9b9b67a014d2778e9fb0aa5489aa14f7b2643.css integrity="sha256-vbA1ecF2YoFesnelKPm5tnoBTSd46fsKpUiaoU97JkM=" rel="preload stylesheet" as=style><link rel=icon href=https://zz-jason.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zz-jason.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zz-jason.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zz-jason.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zz-jason.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.80.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-67872953-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="[DuckDB] Push-Based Execution Model"><meta property="og:description" content="1. 背景 DuckDB 是我非常喜欢的一个数据库，它基于 libpg_query 实现了 SQL Parser，语法和 PostgreSQL 一致，内嵌 SQLite 的 REPL CLI，编译好后可直接运行 CLI 交互式输入 SQL 得到结果。架"><meta property="og:type" content="article"><meta property="og:url" content="https://zz-jason.github.io/posts/duckdb-push-based-execution-model/"><meta property="article:published_time" content="2022-11-14T08:36:00+08:00"><meta property="article:modified_time" content="2022-11-14T08:36:00+08:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[DuckDB] Push-Based Execution Model"><meta name=twitter:description content="1. 背景 DuckDB 是我非常喜欢的一个数据库，它基于 libpg_query 实现了 SQL Parser，语法和 PostgreSQL 一致，内嵌 SQLite 的 REPL CLI，编译好后可直接运行 CLI 交互式输入 SQL 得到结果。架"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zz-jason.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[DuckDB] Push-Based Execution Model","item":"https://zz-jason.github.io/posts/duckdb-push-based-execution-model/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[DuckDB] Push-Based Execution Model","name":"[DuckDB] Push-Based Execution Model","description":"1. 背景 DuckDB 是我非常喜欢的一个数据库，它基于 libpg_query 实现了 SQL Parser，语法和 PostgreSQL 一致，内嵌 SQLite 的 REPL CLI，编译好后可直接运行 CLI 交互式输入 SQL 得到结果。架","keywords":[],"articleBody":"1. 背景 DuckDB 是我非常喜欢的一个数据库，它基于 libpg_query 实现了 SQL Parser，语法和 PostgreSQL 一致，内嵌 SQLite 的 REPL CLI，编译好后可直接运行 CLI 交互式输入 SQL 得到结果。架构简单、分析性能优秀、代码干净好读，极易上手。\n10 月初偶然间翻看 duckdb 的代码，发现他的执行引擎和计算调度采用了类似 Hyper 在《Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age》中提出的 Morsel-Driven 的方式，实现了 push-based execution model，它的 pipeline breaker 语义和也 Hyper 在《Efficiently Compiling Efficient Query Plans for Modern Hardware》 中定义的一致，不同的是 Hyper 期望通过 LLVM JIT 的方式对数据一行一行计算使其尽量保存在寄存器中，DuckDB 采用向量化使一批数据尽可能保存在 CPU Cache 中。\n之前做 TiDB 时研究过很多 Hyper 和 Vectorize 的论文，也做过几次分享，一直希望实现一个简单 demo 验证下效果，正好 DuckDB 采用了类似实现，这就勾起了我浓烈的好奇心。因此利用周末时间研究了下 DuckDB 是如何实现 push-based execution model 的，这里分享给大家，希望帮助到同样感兴趣的朋友们。\n2. 执行框架概览 DuckDB 启动时会创建一个全局的 TaskScheduler，启动 nproc-1（main 函数）个后台线程。这些后台线程启动后会不停地从位于 TaskScheduler 的 Task 队列中取出和执行 Task。DuckDB 通过这个后台线程池和公共 Task 队列完成了 Query 的并发执行。\nDuckDB 基于 Query 的物理执行计划 PhysicalOperator tree 构造了 Pipeline DAG。每个 Pipeline 代表了物理执行计划中一段连续的 PhysicalOperator 算子，由 source、operators 和 sink 构成。当且仅当 Pipeline 的所有 dependency 都执行完后，该 Pipeline 才可被执行。Pipeline 的 sink 代表了需要消费掉所有输入数据才能对外返回结果的 PhysicalOperator。Pipeline DAG 可以看做是另一种视角下的物理执行计划。\nDuckDB 为每个 Pipeline 构造多个 ExecutorTask 使得 Pipeline 可以被多个线程并发执行，Pipeline 的 source 和 sink 需要是并发安全的。后台线程取得 ExecutorTask 后会通过其中的 PipelineExecutor 执行 Pipeline，当执行完一个 ExecutorTask 后，Pipeline 的一个并发任务也就执行完了。\n为了正确调度和执行 Pipeline DAG 中所有的 Pipeline 和它们对应的 ExecutorTask，我们需要能够及时知道某个 Pipeline 的所有并发是否都执行完毕，在其父亲 Pipeline 的所有依赖都被执行完后及时调度父亲 Pipeline 的所有 ExecutorTask，DuckDB 采用了 Event 来生成和调度对应的 ExecutorTask。\n每个 Pipeline 的 Event 都记录了需要执行的总并发数和完成的并发数。在构造 Pipeline DAG 后，DuckDB 会为其构造一个对应的 Event DAG，Pipeline 通过 Event 完成了 ExecutorTask 的调度和执行。每当一个 ExecutorTask 完成，该 Event 的完成并发数就会加 1，当该 Pipeline 的所有 ExecutorTask 都完成后，Event 中的总并发数和已完成并发数相等，标志着该 Event 也完成，该 Event 会通知其父亲 Event，父亲 Event 一旦检测到所有 dependency Event 都执行完，就会调度自己的 ExecutorTask，从而驱动后续的 Pipeilne 计算。\nExecutorTask 中的 Pipeline 是以 push 的方式执行的：先从 Pipeline 的 source 获取一批数据，然后将该批数据依次的通过所有中间的 operators 计算，最终由 sink 完成这一批初始数据的最终计算。典型的 sink 比如构造 hash table：当前 Pipeline 的所有 ExecutorTask 执行完后，最终的 hash table 才构造好，才能用来 probe 产生结果。\n为了返回结果给客户端，当前 Query 的主线程会不断调用 root PipelineExecutor 的 pull 接口。需要注意的是，这个接口名字的 pull 指的仅仅是从最顶层 Pipeline 拿结果数据，在计算顶层 Pipeline 的时候仍然是从 source 到最后一个 PhysicalOperator push 计算过去的。root PipelineExecutor 拿到一批 source 数据代表着 root Pipeline 依赖的所有 PipelineTask 都执行完毕，之后 root PipelineExecutor 内部以 push 的方式执行完这一批数据得到结果，将结果返回给客户端，用户就可以看到 Query 执行结果了。\n以上就是 DuckDB 执行框架的大致介绍。因为要特殊考虑一些算子的特殊优化，所以实际实现会稍微复杂一些。比如 UNION ALL，DuckDB 会在一段 PhysicalOperator 链条上构造多个 Pipeline。考虑到 partitioned hash join 的高效实现，DuckDB 也会在一段 PhysicalOperator 链条上构造多个 Pipeline，和 UNION ALL 不同的是，这些 Pipeline 之间有执行顺序的依赖关系。最终构造出来的可能就是有多个 root 的 Pipeline DAG。\n本文以当前（2022-11-14）DuckDB master 分支的 commit 为例，学习 DuckDB push-based execution model 涉及到的关键代码路径，感兴趣的朋友可以试试 clone 代码编译和调试玩玩。在 DuckDB 中，Pipeline 的构造、Event 的调度都发生在 Executor::InitializeInternal() 函数中，本文后续的内容也将围绕这里面的关键函数展开，其中几个关键的函数为：\n root_pipeline-Build(physical_plan)：top-down 的构造 Pipeline DAG ScheduleEvents(to_schedule)：基于除了 root Pipeline 以外的其他 Pipeline 构造 Event DAG，完成初始 Event 和 ExecutorTask 的调度。  Executor::InitializeInternal() 函数的完整代码如下：\nvoid Executor::InitializeInternal(PhysicalOperator *plan) { auto \u0026scheduler = TaskScheduler::GetScheduler(context); { lock_guardmutex elock(executor_lock); physical_plan = plan; this-profiler = ClientData::Get(context).profiler; profiler-Initialize(physical_plan); this-producer = scheduler.CreateProducer(); // build and ready the pipelines  PipelineBuildState state; auto root_pipeline = make_sharedMetaPipeline(*this, state, nullptr); root_pipeline-Build(physical_plan); root_pipeline-Ready(); // ready recursive cte pipelines too  for (auto \u0026rec_cte : recursive_ctes) { D_ASSERT(rec_cte-type == PhysicalOperatorType::RECURSIVE_CTE); auto \u0026rec_cte_op = (PhysicalRecursiveCTE \u0026)*rec_cte; rec_cte_op.recursive_meta_pipeline-Ready(); } // set root pipelines, i.e., all pipelines that end in the final sink  root_pipeline-GetPipelines(root_pipelines, false); root_pipeline_idx = 0; // collect all meta-pipelines from the root pipeline  vectorshared_ptrMetaPipeline to_schedule; root_pipeline-GetMetaPipelines(to_schedule, true, true); // number of 'PipelineCompleteEvent's is equal to the number of meta pipelines, so we have to set it here  total_pipelines = to_schedule.size(); // collect all pipelines from the root pipelines (recursively) for the progress bar and verify them  root_pipeline-GetPipelines(pipelines, true); // finally, verify and schedule  VerifyPipelines(); ScheduleEvents(to_schedule); } } 3. TaskScheduler 和后台线程池 在 DuckDB 启动时会创建一个全局的 TaskScheduler，在后台启动 nproc-1（main 函数）个后台线程，启动线程是在 TaskScheduler::SetThreadsInternal() 函数中进行的，从主线程启动线程池的调用堆栈如下，感兴趣的朋友们可以根据这些关键函数看看线程是如何启动起来的：\n(lldb) bt * thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1 * frame #0: 0x00000001074e29ee duckdb`duckdb::TaskScheduler::SetThreadsInternal(this=0x00006060000015e0, n=12) at task_scheduler.cpp:239:4 frame #1: 0x00000001074e5335 duckdb`duckdb::TaskScheduler::SetThreads(this=0x00006060000015e0, n=12) at task_scheduler.cpp:199:2 frame #2: 0x0000000107099f45 duckdb`duckdb::DatabaseInstance::Initialize(this=0x0000616000000698, database_path=0x0000000000000000, user_config=0x00007ff7bfefd420) at database.cpp:159:13 frame #3: 0x000000010709f2ce duckdb`duckdb::DuckDB::DuckDB(this=0x0000602000000a90, path=0x0000000000000000, new_config=0x00007ff7bfefd420) at database.cpp:170:12 frame #4: 0x000000010709f689 duckdb`duckdb::DuckDB::DuckDB(this=0x0000602000000a90, path=0x0000000000000000, new_config=0x00007ff7bfefd420) at database.cpp:169:100 ... 后台线程启动后的主逻辑在 TaskScheduler::ExecuteForever() 中，在每个后台线程的生命周期内，它们会不停从 TaskScheduler 的公共队列中取出 Task，调用 Task::Execute() 函数完成 Task 的执行：\nvoid TaskScheduler::ExecuteForever(atomicbool *marker) { #ifndef DUCKDB_NO_THREADS  unique_ptrTask task; // loop until the marker is set to false  while (*marker) { // wait for a signal with a timeout  queue-semaphore.wait(); if (queue-q.try_dequeue(task)) { task-Execute(TaskExecutionMode::PROCESS_ALL); task.reset(); } } #else  throw NotImplementedException(\"DuckDB was compiled without threads! Background thread loop is not allowed.\"); #endif } Pipeline 的各个 Task 就是这样被后台线程并发执行的。要想控制 Pipeline 之间的执行顺序和 Pipeline 内的并发度，只需要设计和控制好各个 ExecutorTask 入队的顺序即可。Pipeline 的执行主要依靠 ExecutorTask，各个算子如果需要自定义计算逻辑和调度规则也是通过实现新的 ExecutorTask 完成。\n4. ExecutorTask 和 Event 驱动的调度模型 在 Task 的执行框架内，后台线程会通过 ExecutorTask::Execute() 驱动当前 ExecutorTask 的执行。为了给各个 Pipeline 和 PhysicalOperator 提供灵活的执行方式，DuckDB 内各个 PhysicalOperator 可以各自实现特定的 ExecutorTask 用于完成自身特殊的计算任务和后续 Pipeline Task 的计算调度。ExecutorTask::Execute() 的执行会直接调用子类的 ExecutorTask::ExecuteTask() 函数完成当前 ExecutorTask 的实际执行。\n对于一般的 Pipeline 来说，会构造一个叫 PipelineTask 的 ExecutorTask 子类。PipelineTask::ExecuteTask() 的代码逻辑如下：\nTaskExecutionResult ExecuteTask(TaskExecutionMode mode) override { if (!pipeline_executor) { pipeline_executor = make_uniquePipelineExecutor(pipeline.GetClientContext(), pipeline); } if (mode == TaskExecutionMode::PROCESS_PARTIAL) { bool finished = pipeline_executor-Execute(PARTIAL_CHUNK_COUNT); if (!finished) { return TaskExecutionResult::TASK_NOT_FINISHED; } } else { pipeline_executor-Execute(); } event-FinishTask(); pipeline_executor.reset(); return TaskExecutionResult::TASK_FINISHED; } 在 PipelineTask::ExecuteTask() 中，通过 PipelineExecutor::Execute() 完成当前 ExecutorTask 的执行后它会去调用 Event::FinishTask() 函数进行 ExecutorTask 完成后各个 Event 子类自定义的收尾工作，在 Event::FinishTask() 函数如果发现当前 Event 的所有 Task 都执行完毕就会清理当前 Event 相关内容，并调用父亲 Event 的 CompleteDependency()：\nvoid Event::Finish() { D_ASSERT(!finished); FinishEvent(); finished = true; // finished processing the pipeline, now we can schedule pipelines that depend on this pipeline  for (auto \u0026parent_entry : parents) { auto parent = parent_entry.lock(); if (!parent) { // LCOV_EXCL_START  continue; } // LCOV_EXCL_STOP  // mark a dependency as completed for each of the parents  parent-CompleteDependency(); } FinalizeFinish(); } 在 Event::CompleteDependency() 中，如果发现所有 dependency Event 都已经执行完毕，则会开始调度执行父亲 Event 的 Task。如果父亲 Event 没有 task 需要执行，则会再调用父亲 Event 的 Finish() 函数直接在当前线程中完成父亲 Event 的执行和收尾：\nvoid Event::CompleteDependency() { idx_t current_finished = ++finished_dependencies; D_ASSERT(current_finished  total_dependencies); if (current_finished == total_dependencies) { // all dependencies have been completed: schedule the event  D_ASSERT(total_tasks == 0); Schedule(); if (total_tasks == 0) { Finish(); } } } 从上面代码可以看到 Event 调度 Task 是通过 Event::Schedule() 函数完成的，这是个 Event 的纯虚函数，不同的子类 Event 需要自行实现。Pipeline 执行过程中使用的 Event 类型不多，最常见的是：\n PipelineInitializeEvent：主要用来初始化当前 Pipeline 的 sink，会调度 1 个 PipelineInitializeTask PipelineEvent：主要用来表示 Pipeline 的执行操作，可能会调度多个 ExecutorTask 到执行队列中。PipelineEvent 的 Schedule() 函数主要调用 Pipeline::Schedule() 完成 ExecutorTask 的计算调度，这里不再展开，感兴趣的朋友们可以继续追踪代码看看其中的实现细节 PipelineFinishEvent：主要用来标记当前 Pipeline 执行结束，在 Event::Finish() 检测到当前 Event 结束，调用到 PipelineFinishEvent::FinishEvent() 时完成 Pipeline::Finalize()，用来做 Pipeline 的清理操作 PipelineCompleteEvent：用来更新 Executor 中已结束的 Pipeline 的 counter completed_pipelines，Executor 主线程会不断检测 completed_pipelines，当发现所有中间 Pipeline 都执行完后，主线程会开始执行 root Pipeline，返回结果给客户端。  我们在上面 Executor::InitializeInternal() 的函数中看到，DuckDB 的 Pipeline 分为了 2 部分：\n 中间 Pipelines：包含所有除了 root 以外的 Pipeline。DuckDB 基于这些 Pipeline 之间的依赖关系构建了相应的 Event DAG，通过调度最底层没有任何依赖 Event 的 ExecutorTask 初始化了 TaskScheduler 的执行队列，进而催动了所有中间 Pipeline 的执行。 root Pipelines：在构建 Event DAG 的时候不会将这部分 Pipeline 考虑进去，这部分 Pipeline 也不会被 TaskScheduler 启动的后台线程异步执行。在完成中间 Pipeline 的初始调度后，主线程后续的工作和 root Pipeline 的执行过程我们在后面的小结来看。  Pipeline 的初始调度正是由主线程执行 Executor::ScheduleEvents() 触发的，正式的调度逻辑是由 Executor::ScheduleEventsInternal() 完成的，这个函数的大致逻辑如下。概括来说就是寻找没有任何 dependency 的 Event，通过执行这些 Event.Schedule() 构造 ExecutorTask，放入 TaskScheduler 的工作队列，激活后台工作线程，开始 Pipeline 执行以及其他 Event 和 ExecutorTask 的连锁反应：\nvoid Executor::ScheduleEventsInternal(ScheduleEventData \u0026event_data) { auto \u0026events = event_data.events; D_ASSERT(events.empty()); // create all the required pipeline events  for (auto \u0026pipeline : event_data.meta_pipelines) { SchedulePipeline(pipeline, event_data); } // set up the dependencies across MetaPipelines  auto \u0026event_map = event_data.event_map; for (auto \u0026entry : event_map) { auto pipeline = entry.first; for (auto \u0026dependency : pipeline-dependencies) { auto dep = dependency.lock(); D_ASSERT(dep); auto event_map_entry = event_map.find(dep.get()); D_ASSERT(event_map_entry != event_map.end()); auto \u0026dep_entry = event_map_entry-second; D_ASSERT(dep_entry.pipeline_complete_event); entry.second.pipeline_event-AddDependency(*dep_entry.pipeline_complete_event); } } // verify that we have no cyclic dependencies  VerifyScheduledEvents(event_data); // schedule the pipelines that do not have dependencies  for (auto \u0026event : events) { if (!event-HasDependencies()) { event-Schedule(); } } } 5. PipelineExecutor 和 Pipeline 内基于 Push 的执行模型 PipelineExecutor::Execute() 函数通过调用 Pipeline 中各个 PhysicalOperator 的相应接口，以 Push 的方式完成了当前 Pipeline 的执行，执行逻辑可以概括为：\n 先调用 FetchFromSource() 从 Pipeline 的 source PhysicalOperator 中获取计算结果作为 source DataChunk，这里会调用 source 的 GetData() 接口。 再调用 ExecutePushInternal() 依次执行 Pipeline 中 operators 列表中的各个 PhysicalOperator 和最后一个 sink PhysicalOperator 完成这批数据后续的所有计算操作。对于普通 operator 会调用它的 Execute() 接口，对最后的 sink 会调用它的 Sink() 接口。PipelineExecutor::ExecutePushInternal() 可以看做是 Pipeline 内的数据消费者。 最后调用 PushFinalize() 完成当前 ExecutorTask 的执行，这里会调用 sink 的 Combine 接口，用以完成一个 ExecutorTask 结束后的收尾清理工作。  bool PipelineExecutor::Execute(idx_t max_chunks) { D_ASSERT(pipeline.sink); bool exhausted_source = false; auto \u0026source_chunk = pipeline.operators.empty() ? final_chunk : *intermediate_chunks[0]; for (idx_t i = 0; i  max_chunks; i++) { if (IsFinished()) { break; } source_chunk.Reset(); FetchFromSource(source_chunk); if (source_chunk.size() == 0) { exhausted_source = true; break; } auto result = ExecutePushInternal(source_chunk); if (result == OperatorResultType::FINISHED) { D_ASSERT(IsFinished()); break; } } if (!exhausted_source \u0026\u0026 !IsFinished()) { return false; } PushFinalize(); return true; } PhysicalOperator 同时包含了 source、operator、sink 所需要的所有接口，各个 PhysicalOperator 需要实现对应的接口完成相应的计算逻辑。比如 partitioned hash join 因为会分成 3 个阶段分别作为 sink、operator 和 source 角色，它同时实现了所有的接口。\n6. 主线程和 root Pipeline 的执行 root Pipelines 比较特殊：在构建 Event DAG 的时候不会将这部分 Pipeline 考虑进去，这部分 Pipeline 也不会被 TaskScheduler 启动的后台线程异步执行，这部分 Pipeline 要想得到执行也需要等待所有中间 Pipeline 执行结束。\nroot Pipeline 的执行是主线程通过调用 PipelineExecutor 的 Execute 函数完成的。主线程通过 TaskScheduler 启动的多个后台线程，通过 Event 触发和调度新一轮 Pipeline 的 ExecutorTask，Pipeline 就能够被后台执行了。剩下的问题就是主线程如何知道中间 Pipeline 执行结束，以及如何执行 root Pipeline 拿到最终结果返回给客户端。另外各个 Pipeline 在异步执行过程中可能会遇到一些 ERROR，主线程如何及时知道这些 ERROR 并返回给客户端也是需要处理的一个问题。\n主线程完成中间 Pipeline 的初始调度后，因为 root Pipeline 在中间结果没有准备好之前也不能计算，这时为了加速查询的执行最好的办法就是主线程也参与到中间 Pipeline 的执行当中去。我们看到主线程会停留在 PendingQueryResult::ExecuteInternal() 的 while 循环这里：\nunique_ptrQueryResult PendingQueryResult::ExecuteInternal(ClientContextLock \u0026lock) { CheckExecutableInternal(lock); while (ExecuteTaskInternal(lock) == PendingExecutionResult::RESULT_NOT_READY) { } if (HasError()) { return make_uniqueMaterializedQueryResult(error); } auto result = context-FetchResultInternal(lock, *this); Close(); return result; } PendingQueryResult::ExecuteTaskInternal() 经过几次函数调用后最终会来到 PipelineExecutor::Execute() 函数。这个函数初始一看可能会比较绕，但想要实现的功能是：\n 在所有中间 Pipeline 没有执行完之前一直和后台线程一起参与计算：如果从队列中取出来了一个 ExecutorTask 就尝试调用它的 Execute(TaskExecutionMode::PROCESS_PARTIAL) 函数完成小批量数据的计算，现在默认是 50 个 DataChunk。 如果所有 Pipeline 都执行完了，此时 completed_pipelines 与 total_pipelines（记录中间 Pipeline 的数量，不包含 root Pipeline）相等，Executor 会释放所有中间 Pipeline，标记 execution_result 为PendingExecutionResult::RESULT_READY。  在这个小 while 循环中，如果没有取到 task，或者执行了 Task 的小部分任务后，都会去检测其他线程执行过程中是否有 Error 产生，用户是否 cancel 了 query 等等，一旦遇到错误产生，就会分别通过 CancelTasks() 和 ThrowException() 取消后台异步 Task 的执行并将错误抛给主线程的上层。\nPendingExecutionResult Executor::ExecuteTask() { if (execution_result != PendingExecutionResult::RESULT_NOT_READY) { return execution_result; } // check if there are any incomplete pipelines  auto \u0026scheduler = TaskScheduler::GetScheduler(context); while (completed_pipelines  total_pipelines) { // there are! if we don't already have a task, fetch one  if (!task) { scheduler.GetTaskFromProducer(*producer, task); } if (task) { // if we have a task, partially process it  auto result = task-Execute(TaskExecutionMode::PROCESS_PARTIAL); if (result != TaskExecutionResult::TASK_NOT_FINISHED) { // if the task is finished, clean it up  task.reset(); } } if (!HasError()) { // we (partially) processed a task and no exceptions were thrown  // give back control to the caller  return PendingExecutionResult::RESULT_NOT_READY; } execution_result = PendingExecutionResult::EXECUTION_ERROR; // an exception has occurred executing one of the pipelines  // we need to cancel all tasks associated with this executor  CancelTasks(); ThrowException(); } D_ASSERT(!task); lock_guardmutex elock(executor_lock); pipelines.clear(); NextExecutor(); if (HasError()) { // LCOV_EXCL_START  // an exception has occurred executing one of the pipelines  execution_result = PendingExecutionResult::EXECUTION_ERROR; ThrowException(); } // LCOV_EXCL_STOP  execution_result = PendingExecutionResult::RESULT_READY; return execution_result; } 一旦 execution_result 的状态变为 RESULT_READY，就意味着我们结束了所有中间 Pipeline 的执行，Executor::ExecuteTask() 会一直返回 RESULT_READY，最外层的 for 循环也会退出，从而进入下一阶段，也就是执行 root Pipeline。root Pipeline 被 Executor 所持有，它的执行也是在 Executor::FetchChunk() 中完成的：\nunique_ptrDataChunk Executor::FetchChunk() { D_ASSERT(physical_plan); auto chunk = make_uniqueDataChunk(); root_executor-InitializeChunk(*chunk); while (true) { root_executor-ExecutePull(*chunk); if (chunk-size() == 0) { root_executor-PullFinalize(); if (NextExecutor()) { continue; } break; } else { break; } } return chunk; } 虽然从函数名来看 Executor 调用了 Pipeline::ExecutePull() 函数，但其实这个函数内部实现仍旧是 push 的方式，先从 source 拿到一批数据，然后再依次的经过所有 operators 的计算得到最终结果。\n7. Pipeline 的构造 Pipeline 的执行框架我们已经大概了解，最后一个问题就是 PhysicalOperator tree 是如何转换成 Pipeline DAG 的了。Pipeline 主要由 source、operators 和 sink 这三部分构成，从物理执行计划划分 Pipeline 第一个遇到的问题是如何确定 Pipeline 的 sink 和 source。\nDuckDB 采用了和 Hyper 一样的 Pipeline breaker 定义：那些需要消化掉所有孩子节点的数据后才能进行下一步计算输出结果的算子。典型的比如构造 hash join 或 hash aggregate 的 hash table，或者 sort 和 TopN 算子的排序操作，需要完全消费掉孩子节点的数据 后，才能得到正确结果进行下一阶段的数据。\n算子的具体实现决定了 Pipeline 的构造。物理执行计划转成 Pipeline 是由其中的各个 PhysicalOperator 完成的，几个关键函数：\n Executor::InitializeInternal()：把物理执行计划（PhysicalOperator tree）转成 Pipeline 的入口，所有构造出来的 Pipeline 都存储在该查询的 Executor 中。 PhysicalOperator::BuildPipelines()：构造 Pipeline 的是通过 top down 的遍历 PhysicalOperator tree 完成的，Pipeline 的 sink 会先被确定下来（要么是整个物理执行计划的根节点，要么是上一个 Pipeline 的 source 节点）。Executor 通过该函数遍历每个 PhysicalOperator，决定将其加入当前 Pipeline 的 operators 列表还是做为当前 Pipeline 的 source。遇到当前 Pipeline 的 source 时就需要结束构造当前 Pipeline 了，然后将该 source 作为下一个 Pipeline 的 sink，继续 top down 的遍历 PhysicalOperator tree 和构造新的 Pipeline。 PhysicalOperator::BuildChildPipeline()：切分 Pipeline，构造 Pipeline 之间的依赖关系。  PhysicalOperator::BuildPipelines() 是个虚函数，搜索代码可以看到，像 PhysicalJoin、PhysicalRecursiveCTE、PhysicalUnion 这些有多个孩子节点的以及一些比较特殊的算子都重载了这个虚函。其他没有重载该函数的算子，默认的 BuildPipelines() 函数如下：\nvoid PhysicalOperator::BuildPipelines(Pipeline \u0026current, MetaPipeline \u0026meta_pipeline) { op_state.reset(); auto \u0026state = meta_pipeline.GetState(); if (IsSink()) { // operator is a sink, build a pipeline  sink_state.reset(); D_ASSERT(children.size() == 1); // single operator: the operator becomes the data source of the current pipeline  state.SetPipelineSource(current, this); // we create a new pipeline starting from the child  auto child_meta_pipeline = meta_pipeline.CreateChildMetaPipeline(current, this); child_meta_pipeline-Build(children[0].get()); } else { // operator is not a sink! recurse in children  if (children.empty()) { // source  state.SetPipelineSource(current, this); } else { if (children.size() != 1) { throw InternalException(\"Operator not supported in BuildPipelines\"); } state.AddPipelineOperator(current, this); children[0]-BuildPipelines(current, meta_pipeline); } } } PhysicalOperator::BuildPipelines() 不仅构建了 PhysicalOperator 和 Pipeline 的关系，也构建了 Pipeline 之间的依赖关系：如果某个 PhysicalOperator 是 Pipeline breaker，那么它不仅会作为当前 Pipeline 的 source，也会作为下一个 Pipeline 的 sink，Pipeline breaker 算子隐含了 Pipeline 之间的计算先后关系，只有上游 Pipeline 完全完成计算后才能开启下游 Pipeline 的计算。\n一个简单的单表聚合为例，它的执行计划和对应的 Pipeline 可以表示成下图，其中 Pipeline 1 依赖 Pipeline 2：\n7.1. 从 PhysicalUnion 构造 Pipeline 我们以 Union All 为例介绍一个稍微复杂有多个 child 的情况。DuckDB 的 Union All 用 PhysicalUnion 来表示，每个 PhysicalUnion 有 2 个孩子节点。如果用户 SQL 中有 N 个表 Union All，那么就会构造出 N-1 个 PhysicalUnion 算子。PhysicalUnion 仅仅用来汇总多个数据源，传递孩子节点的数据给它的父节点完成计算。\nPhysicalUnion 有多个 child 数据源，意味着 PhysicalUnion 往下 top down 构造 Pipeline 的时候需要分别给各个孩子节点传递不同的 Pipeline，那这 2 个 Pipeline 的 sink 应该是什么呢。考虑到 PhysicalUnion 没有计算逻辑仅汇总数据的特殊性，DuckDB 让这 2 个 Pipeline 共享当前传递过来的 Pipeline 的 sink 和当前的 operators 列表，然后各自在自己的 operators 列表中新增自己的算子，设置自己的 sink。\n这样的 Pipeline 分裂可以使 PhysicalUnion 父节点的计算逻辑和对应的中间状态在这 2 个 Pipeline 之间复用，虽然 PhysicalUnion 孩子节点的计算逻辑位于不同 Pipeline 之间各自独立产生计算结果，但 PhysicalUnion 之后的计算逻辑和中间状态在不同 Pipeline 之间是共用的，可以确保计算的正确性。\n不过这样的 Pipeline 构造带来了额外的问题，我们上面提到 Pipeline breaker 确定了 Pipeline 之间的计算调度关系，并且每个 Pipeline 还可以独立设置自己的并发度。对于 PhysicalUnion 所处的 Pipeline 来说，这个 Pipeline 的 sink 同时属于多个 Pipeline（PhysicalUnion 分裂出来的），只有这些 Pipeline 都完成执行后才能执行他们的下游 Pipeline。所以后面在 Pipeline 调度的时候这里还需要特殊处理下。\n一个简单的 UNION ALL 为例，它的执行计划和对应的 Pipeline 可以表示成下图：\n7.2. 从 PhysicalJoin 构造 Pipeline 理解了 Union All 的 Pipeline 构造，我们再来看看稍微复杂点的 Join。PhysicalJoin 的 Pipeline 构造相对来说要复杂一点，需要我们先大致了解下 DuckDB 中 PhysicalJoin 的实现。\nDuckDB 的 Hash Join 采用了 partitioned hash join，当数据量比较大的时候可以通过 repartition 将数据落盘避免 OOM，这个多线程版本的 partitioned hash join，主要分为 3 个阶段：\n 并发读取和计算所有 build 端的数据，当所有数据都读完后检查总数据量是否能全部放在内存中，如果不能就将 build 端的数据 repartition，选出第一批能放在内存中的 partition 为它们构造 hash table，剩下的数据存放在磁盘上。 并发读取和计算所有 probe 端的数据，这时读上来的数据要么属于内存中的 partition，要么属于磁盘上的 partition，先把属于磁盘上的 partition 的数据落盘，用属于内存中的 partition 的数据去 probe 此时 build 端的放在内存中的 hash table，得到结果返回给上层。 并发处理磁盘上的数据：挑选一批 build 端能放入内存的 partition，构造 hash table，然后 probe 端去并发的 probe 得到结果进行下一步计算。循环这样的处理过程直到所有磁盘上的 partition 都 join 完成。  这 3 个过程也分别对应了 3 个基本的 Pipeline，可以表示成下图，其中 Pipeline 2 依赖 pipeline 1，Pipeline 3 依赖 Pipeline 2：\n8. DuckDB 执行模式的一些感受和思考 8.1. 计算调度的复杂性 这是一个最直观的感受。相比 Pull 模型，Push 模型在实现时需要多考虑如何控制 Pipeline 的计算调度，也需要考虑一个 Pipeline 内数据消费速度的问题（这个我们还没在本文涉及），这些代码增加了工程实现的复杂度，也增加了问题诊断的复杂度。\n8.1. PipelineBreaker 的作用 我喜欢把计算抽象为数据和计算两个部分，就像 CPU 的 L1 Cache 分为 L1D 和 L1I 一样，之前思考 Pipeline breaker 的时候更多是从计算性能角度，这次在思考 Pipeline 之间的依赖关系和 ExecutorTask 的调度时才意识到这个容易被忽略的地方：Pipeline 其实也反应了两个比较大的计算过程之间的先后关系。这个关系在 Volcano 模型的 Pull 中没有那么明显，只是写代码时为了保证正确性其实会应用这些依赖关系，比如 hash join 在 build side 没有结束时就不会对外返回结果。\n8.3. 除了带来性能提升外，这种并发 Push 执行模型还有其他优势吗？ 从数据库、数据仓库执行引擎的经验来看，遇到最多的线上问题可以分为两类：查询跑的慢并发，以及查询吃的内存太狠导致查询自己或者进程 OOM。DuckDB 因为有了基于 ExecutorTask 的计算调度机制，我们就有机会从源头来控制：如果内存或 CPU 资源有限就少调度些 ExecutorTask 来执行。这样至少能够把查询失败的问题变成查询变慢的问题，然后查询变慢的时候再去看资源使用率满不满，这样至少能守住服务可用性的 SLO，也比较符合一般的问题排查思路。\n9. 参考材料  issues/1583 Move to push-based execution model Push-Based Execution in DuckDB, by Mark Raasveldt: slides, video  ","wordCount":"7966","inLanguage":"en","datePublished":"2022-11-14T08:36:00+08:00","dateModified":"2022-11-14T08:36:00+08:00","author":{"@type":"Person","name":"Jian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zz-jason.github.io/posts/duckdb-push-based-execution-model/"},"publisher":{"@type":"Organization","name":"Jian Zhang","logo":{"@type":"ImageObject","url":"https://zz-jason.github.io/favicon.ico"}}}</script></head><body id=top><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://zz-jason.github.io/ accesskey=h title="Jian Zhang (Alt + H)">Jian Zhang</a>
<span class=logo-switches></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://zz-jason.github.io/ title=Home><span>Home</span></a></li><li><a href=https://zz-jason.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://zz-jason.github.io/posts/ title=Archives><span>Archives</span></a></li><li><a href=https://zz-jason.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[DuckDB] Push-Based Execution Model</h1><div class=post-meta>November 14, 2022&nbsp;·&nbsp;16 min&nbsp;·&nbsp;Jian</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#1-%e8%83%8c%e6%99%af aria-label="1. 背景">1. 背景</a></li><li><a href=#2-%e6%89%a7%e8%a1%8c%e6%a1%86%e6%9e%b6%e6%a6%82%e8%a7%88 aria-label="2. 执行框架概览">2. 执行框架概览</a></li><li><a href=#3-taskscheduler-%e5%92%8c%e5%90%8e%e5%8f%b0%e7%ba%bf%e7%a8%8b%e6%b1%a0 aria-label="3. TaskScheduler 和后台线程池">3. TaskScheduler 和后台线程池</a></li><li><a href=#4-executortask-%e5%92%8c-event-%e9%a9%b1%e5%8a%a8%e7%9a%84%e8%b0%83%e5%ba%a6%e6%a8%a1%e5%9e%8b aria-label="4. ExecutorTask 和 Event 驱动的调度模型">4. ExecutorTask 和 Event 驱动的调度模型</a></li><li><a href=#5-pipelineexecutor-%e5%92%8c-pipeline-%e5%86%85%e5%9f%ba%e4%ba%8e-push-%e7%9a%84%e6%89%a7%e8%a1%8c%e6%a8%a1%e5%9e%8b aria-label="5. PipelineExecutor 和 Pipeline 内基于 Push 的执行模型">5. PipelineExecutor 和 Pipeline 内基于 Push 的执行模型</a></li><li><a href=#6-%e4%b8%bb%e7%ba%bf%e7%a8%8b%e5%92%8c-root-pipeline-%e7%9a%84%e6%89%a7%e8%a1%8c aria-label="6. 主线程和 root Pipeline 的执行">6. 主线程和 root Pipeline 的执行</a></li><li><a href=#7-pipeline-%e7%9a%84%e6%9e%84%e9%80%a0 aria-label="7. Pipeline 的构造">7. Pipeline 的构造</a><ul><li><a href=#71-%e4%bb%8e-physicalunion-%e6%9e%84%e9%80%a0-pipeline aria-label="7.1. 从 PhysicalUnion 构造 Pipeline">7.1. 从 PhysicalUnion 构造 Pipeline</a></li><li><a href=#72-%e4%bb%8e-physicaljoin-%e6%9e%84%e9%80%a0-pipeline aria-label="7.2. 从 PhysicalJoin 构造 Pipeline">7.2. 从 PhysicalJoin 构造 Pipeline</a></li></ul></li><li><a href=#8-duckdb-%e6%89%a7%e8%a1%8c%e6%a8%a1%e5%bc%8f%e7%9a%84%e4%b8%80%e4%ba%9b%e6%84%9f%e5%8f%97%e5%92%8c%e6%80%9d%e8%80%83 aria-label="8. DuckDB 执行模式的一些感受和思考">8. DuckDB 执行模式的一些感受和思考</a><ul><li><a href=#81-%e8%ae%a1%e7%ae%97%e8%b0%83%e5%ba%a6%e7%9a%84%e5%a4%8d%e6%9d%82%e6%80%a7 aria-label="8.1. 计算调度的复杂性">8.1. 计算调度的复杂性</a></li><li><a href=#81-pipelinebreaker-%e7%9a%84%e4%bd%9c%e7%94%a8 aria-label="8.1. PipelineBreaker 的作用">8.1. PipelineBreaker 的作用</a></li><li><a href=#83-%e9%99%a4%e4%ba%86%e5%b8%a6%e6%9d%a5%e6%80%a7%e8%83%bd%e6%8f%90%e5%8d%87%e5%a4%96%e8%bf%99%e7%a7%8d%e5%b9%b6%e5%8f%91-push-%e6%89%a7%e8%a1%8c%e6%a8%a1%e5%9e%8b%e8%bf%98%e6%9c%89%e5%85%b6%e4%bb%96%e4%bc%98%e5%8a%bf%e5%90%97 aria-label="8.3. 除了带来性能提升外，这种并发 Push 执行模型还有其他优势吗？">8.3. 除了带来性能提升外，这种并发 Push 执行模型还有其他优势吗？</a></li></ul></li><li><a href=#9-%e5%8f%82%e8%80%83%e6%9d%90%e6%96%99 aria-label="9. 参考材料">9. 参考材料</a></li></ul></div></details></div><div class=post-content><h2 id=1-背景>1. 背景<a hidden class=anchor aria-hidden=true href=#1-背景>#</a></h2><p>DuckDB 是我非常喜欢的一个数据库，它基于 <a href=https://github.com/duckdb/duckdb/tree/master/third_party/libpg_query>libpg_query</a> 实现了 SQL Parser，语法和 PostgreSQL 一致，内嵌 SQLite 的 REPL CLI，编译好后可直接运行 CLI 交互式输入 SQL 得到结果。架构简单、分析性能优秀、代码干净好读，极易上手。</p><p>10 月初偶然间翻看 duckdb 的代码，发现他的执行引擎和计算调度采用了类似 Hyper 在《<a href=https://15721.courses.cs.cmu.edu/spring2016/papers/p743-leis.pdf>Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age</a>》中提出的 Morsel-Driven 的方式，实现了 push-based execution model，它的 pipeline breaker 语义和也 Hyper 在《<a href=https://www.vldb.org/pvldb/vol4/p539-neumann.pdf>Efficiently Compiling Efficient Query Plans for Modern Hardware</a>》 中定义的一致，不同的是 Hyper 期望通过 LLVM JIT 的方式对数据一行一行计算使其尽量保存在寄存器中，DuckDB 采用向量化使一批数据尽可能保存在 CPU Cache 中。</p><p>之前做 TiDB 时研究过很多 Hyper 和 Vectorize 的论文，也做过几次分享，一直希望实现一个简单 demo 验证下效果，正好 DuckDB 采用了类似实现，这就勾起了我浓烈的好奇心。因此利用周末时间研究了下 DuckDB 是如何实现 push-based execution model 的，这里分享给大家，希望帮助到同样感兴趣的朋友们。</p><h2 id=2-执行框架概览>2. 执行框架概览<a hidden class=anchor aria-hidden=true href=#2-执行框架概览>#</a></h2><p>DuckDB 启动时会创建一个全局的 TaskScheduler，启动 nproc-1（main 函数）个后台线程。这些后台线程启动后会不停地从位于 TaskScheduler 的 Task 队列中取出和执行 Task。DuckDB 通过这个后台线程池和公共 Task 队列完成了 Query 的并发执行。</p><p>DuckDB 基于 Query 的物理执行计划 PhysicalOperator tree 构造了 Pipeline DAG。每个 Pipeline 代表了物理执行计划中一段连续的 PhysicalOperator 算子，由 source、operators 和 sink 构成。当且仅当 Pipeline 的所有 dependency 都执行完后，该 Pipeline 才可被执行。Pipeline 的 sink 代表了需要消费掉所有输入数据才能对外返回结果的 PhysicalOperator。Pipeline DAG 可以看做是另一种视角下的物理执行计划。</p><p>DuckDB 为每个 Pipeline 构造多个 ExecutorTask 使得 Pipeline 可以被多个线程并发执行，Pipeline 的 source 和 sink 需要是并发安全的。后台线程取得 ExecutorTask 后会通过其中的 PipelineExecutor 执行 Pipeline，当执行完一个 ExecutorTask 后，Pipeline 的一个并发任务也就执行完了。</p><p>为了正确调度和执行 Pipeline DAG 中所有的 Pipeline 和它们对应的 ExecutorTask，我们需要能够及时知道某个 Pipeline 的所有并发是否都执行完毕，在其父亲 Pipeline 的所有依赖都被执行完后及时调度父亲 Pipeline 的所有 ExecutorTask，DuckDB 采用了 Event 来生成和调度对应的 ExecutorTask。</p><p>每个 Pipeline 的 Event 都记录了需要执行的总并发数和完成的并发数。在构造 Pipeline DAG 后，DuckDB 会为其构造一个对应的 Event DAG，Pipeline 通过 Event 完成了 ExecutorTask 的调度和执行。每当一个 ExecutorTask 完成，该 Event 的完成并发数就会加 1，当该 Pipeline 的所有 ExecutorTask 都完成后，Event 中的总并发数和已完成并发数相等，标志着该 Event 也完成，该 Event 会通知其父亲 Event，父亲 Event 一旦检测到所有 dependency Event 都执行完，就会调度自己的 ExecutorTask，从而驱动后续的 Pipeilne 计算。</p><p>ExecutorTask 中的 Pipeline 是以 push 的方式执行的：先从 Pipeline 的 source 获取一批数据，然后将该批数据依次的通过所有中间的 operators 计算，最终由 sink 完成这一批初始数据的最终计算。典型的 sink 比如构造 hash table：当前 Pipeline 的所有 ExecutorTask 执行完后，最终的 hash table 才构造好，才能用来 probe 产生结果。</p><p>为了返回结果给客户端，当前 Query 的主线程会不断调用 root PipelineExecutor 的 pull 接口。需要注意的是，这个接口名字的 pull 指的仅仅是从最顶层 Pipeline 拿结果数据，在计算顶层 Pipeline 的时候仍然是从 source 到最后一个 PhysicalOperator push 计算过去的。root PipelineExecutor 拿到一批 source 数据代表着 root Pipeline 依赖的所有 PipelineTask 都执行完毕，之后 root PipelineExecutor 内部以 push 的方式执行完这一批数据得到结果，将结果返回给客户端，用户就可以看到 Query 执行结果了。</p><p>以上就是 DuckDB 执行框架的大致介绍。因为要特殊考虑一些算子的特殊优化，所以实际实现会稍微复杂一些。比如 UNION ALL，DuckDB 会在一段 PhysicalOperator 链条上构造多个 Pipeline。考虑到 partitioned hash join 的高效实现，DuckDB 也会在一段 PhysicalOperator 链条上构造多个 Pipeline，和 UNION ALL 不同的是，这些 Pipeline 之间有执行顺序的依赖关系。最终构造出来的可能就是有多个 root 的 Pipeline DAG。</p><p>本文以当前（2022-11-14）DuckDB master 分支的 commit 为例，学习 DuckDB push-based execution model 涉及到的关键代码路径，感兴趣的朋友可以试试 clone 代码编译和调试玩玩。在 DuckDB 中，Pipeline 的构造、Event 的调度都发生在 Executor::InitializeInternal() 函数中，本文后续的内容也将围绕这里面的关键函数展开，其中几个关键的函数为：</p><ol><li>root_pipeline->Build(physical_plan)：top-down 的构造 Pipeline DAG</li><li>ScheduleEvents(to_schedule)：基于除了 root Pipeline 以外的其他 Pipeline 构造 Event DAG，完成初始 Event 和 ExecutorTask 的调度。</li></ol><p>Executor::InitializeInternal() 函数的完整代码如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>void</span> Executor<span style=color:#f92672>::</span>InitializeInternal(PhysicalOperator <span style=color:#f92672>*</span>plan) {
    <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>scheduler <span style=color:#f92672>=</span> TaskScheduler<span style=color:#f92672>::</span>GetScheduler(context);
    {
        lock_guard<span style=color:#f92672>&lt;</span>mutex<span style=color:#f92672>&gt;</span> elock(executor_lock);
        physical_plan <span style=color:#f92672>=</span> plan;

        <span style=color:#66d9ef>this</span><span style=color:#f92672>-&gt;</span>profiler <span style=color:#f92672>=</span> ClientData<span style=color:#f92672>::</span>Get(context).profiler;
        profiler<span style=color:#f92672>-&gt;</span>Initialize(physical_plan);
        <span style=color:#66d9ef>this</span><span style=color:#f92672>-&gt;</span>producer <span style=color:#f92672>=</span> scheduler.CreateProducer();

        <span style=color:#75715e>// build and ready the pipelines
</span><span style=color:#75715e></span>        PipelineBuildState state;
        <span style=color:#66d9ef>auto</span> root_pipeline <span style=color:#f92672>=</span> make_shared<span style=color:#f92672>&lt;</span>MetaPipeline<span style=color:#f92672>&gt;</span>(<span style=color:#f92672>*</span><span style=color:#66d9ef>this</span>, state, <span style=color:#66d9ef>nullptr</span>);
        root_pipeline<span style=color:#f92672>-&gt;</span>Build(physical_plan);
        root_pipeline<span style=color:#f92672>-&gt;</span>Ready();

        <span style=color:#75715e>// ready recursive cte pipelines too
</span><span style=color:#75715e></span>        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>rec_cte : recursive_ctes) {
            D_ASSERT(rec_cte<span style=color:#f92672>-&gt;</span>type <span style=color:#f92672>==</span> PhysicalOperatorType<span style=color:#f92672>::</span>RECURSIVE_CTE);
            <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>rec_cte_op <span style=color:#f92672>=</span> (PhysicalRecursiveCTE <span style=color:#f92672>&amp;</span>)<span style=color:#f92672>*</span>rec_cte;
            rec_cte_op.recursive_meta_pipeline<span style=color:#f92672>-&gt;</span>Ready();
        }

        <span style=color:#75715e>// set root pipelines, i.e., all pipelines that end in the final sink
</span><span style=color:#75715e></span>        root_pipeline<span style=color:#f92672>-&gt;</span>GetPipelines(root_pipelines, false);
        root_pipeline_idx <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>;

        <span style=color:#75715e>// collect all meta-pipelines from the root pipeline
</span><span style=color:#75715e></span>        vector<span style=color:#f92672>&lt;</span>shared_ptr<span style=color:#f92672>&lt;</span>MetaPipeline<span style=color:#f92672>&gt;&gt;</span> to_schedule;
        root_pipeline<span style=color:#f92672>-&gt;</span>GetMetaPipelines(to_schedule, true, true);

        <span style=color:#75715e>// number of &#39;PipelineCompleteEvent&#39;s is equal to the number of meta pipelines, so we have to set it here
</span><span style=color:#75715e></span>        total_pipelines <span style=color:#f92672>=</span> to_schedule.size();

        <span style=color:#75715e>// collect all pipelines from the root pipelines (recursively) for the progress bar and verify them
</span><span style=color:#75715e></span>        root_pipeline<span style=color:#f92672>-&gt;</span>GetPipelines(pipelines, true);

        <span style=color:#75715e>// finally, verify and schedule
</span><span style=color:#75715e></span>        VerifyPipelines();
        ScheduleEvents(to_schedule);
    }
}
</code></pre></div><h2 id=3-taskscheduler-和后台线程池>3. TaskScheduler 和后台线程池<a hidden class=anchor aria-hidden=true href=#3-taskscheduler-和后台线程池>#</a></h2><p>在 DuckDB 启动时会创建一个全局的 TaskScheduler，在后台启动 nproc-1（main 函数）个后台线程，启动线程是在 TaskScheduler::SetThreadsInternal() 函数中进行的，从主线程启动线程池的调用堆栈如下，感兴趣的朋友们可以根据这些关键函数看看线程是如何启动起来的：</p><pre><code>(lldb) bt
* thread #1, queue = 'com.apple.main-thread', stop reason = breakpoint 1.1
  * frame #0: 0x00000001074e29ee duckdb`duckdb::TaskScheduler::SetThreadsInternal(this=0x00006060000015e0, n=12) at task_scheduler.cpp:239:4
    frame #1: 0x00000001074e5335 duckdb`duckdb::TaskScheduler::SetThreads(this=0x00006060000015e0, n=12) at task_scheduler.cpp:199:2
    frame #2: 0x0000000107099f45 duckdb`duckdb::DatabaseInstance::Initialize(this=0x0000616000000698, database_path=0x0000000000000000, user_config=0x00007ff7bfefd420) at database.cpp:159:13
    frame #3: 0x000000010709f2ce duckdb`duckdb::DuckDB::DuckDB(this=0x0000602000000a90, path=0x0000000000000000, new_config=0x00007ff7bfefd420) at database.cpp:170:12
    frame #4: 0x000000010709f689 duckdb`duckdb::DuckDB::DuckDB(this=0x0000602000000a90, path=0x0000000000000000, new_config=0x00007ff7bfefd420) at database.cpp:169:100
    ...
</code></pre><p>后台线程启动后的主逻辑在 TaskScheduler::ExecuteForever() 中，在每个后台线程的生命周期内，它们会不停从 TaskScheduler 的公共队列中取出 Task，调用 Task::Execute() 函数完成 Task 的执行：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>void</span> TaskScheduler<span style=color:#f92672>::</span>ExecuteForever(atomic<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>bool</span><span style=color:#f92672>&gt;</span> <span style=color:#f92672>*</span>marker) {
<span style=color:#75715e>#ifndef DUCKDB_NO_THREADS
</span><span style=color:#75715e></span>    unique_ptr<span style=color:#f92672>&lt;</span>Task<span style=color:#f92672>&gt;</span> task;
    <span style=color:#75715e>// loop until the marker is set to false
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>while</span> (<span style=color:#f92672>*</span>marker) {
        <span style=color:#75715e>// wait for a signal with a timeout
</span><span style=color:#75715e></span>        queue<span style=color:#f92672>-&gt;</span>semaphore.wait();
        <span style=color:#66d9ef>if</span> (queue<span style=color:#f92672>-&gt;</span>q.try_dequeue(task)) {
            task<span style=color:#f92672>-&gt;</span>Execute(TaskExecutionMode<span style=color:#f92672>::</span>PROCESS_ALL);
            task.reset();
        }
    }
<span style=color:#75715e>#else
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>throw</span> <span style=color:#a6e22e>NotImplementedException</span>(<span style=color:#e6db74>&#34;DuckDB was compiled without threads! Background thread loop is not allowed.&#34;</span>);
<span style=color:#75715e>#endif
</span><span style=color:#75715e></span>}
</code></pre></div><p>Pipeline 的各个 Task 就是这样被后台线程并发执行的。要想控制 Pipeline 之间的执行顺序和 Pipeline 内的并发度，只需要设计和控制好各个 ExecutorTask 入队的顺序即可。Pipeline 的执行主要依靠 ExecutorTask，各个算子如果需要自定义计算逻辑和调度规则也是通过实现新的 ExecutorTask 完成。</p><h2 id=4-executortask-和-event-驱动的调度模型>4. ExecutorTask 和 Event 驱动的调度模型<a hidden class=anchor aria-hidden=true href=#4-executortask-和-event-驱动的调度模型>#</a></h2><p><img src=/images/duckdb-push-based-execution-model/Task-Event.jpg alt="Event based scheduling model"></p><p>在 Task 的执行框架内，后台线程会通过 ExecutorTask::Execute() 驱动当前 ExecutorTask 的执行。为了给各个 Pipeline 和 PhysicalOperator 提供灵活的执行方式，DuckDB 内各个 PhysicalOperator 可以各自实现特定的 ExecutorTask 用于完成自身特殊的计算任务和后续 Pipeline Task 的计算调度。ExecutorTask::Execute() 的执行会直接调用子类的 ExecutorTask::ExecuteTask() 函数完成当前 ExecutorTask 的实际执行。</p><p>对于一般的 Pipeline 来说，会构造一个叫 PipelineTask 的 ExecutorTask 子类。PipelineTask::ExecuteTask() 的代码逻辑如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>TaskExecutionResult <span style=color:#a6e22e>ExecuteTask</span>(TaskExecutionMode mode) <span style=color:#66d9ef>override</span> {
    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>pipeline_executor) {
        pipeline_executor <span style=color:#f92672>=</span> make_unique<span style=color:#f92672>&lt;</span>PipelineExecutor<span style=color:#f92672>&gt;</span>(pipeline.GetClientContext(), pipeline);
    }
    <span style=color:#66d9ef>if</span> (mode <span style=color:#f92672>==</span> TaskExecutionMode<span style=color:#f92672>::</span>PROCESS_PARTIAL) {
        <span style=color:#66d9ef>bool</span> finished <span style=color:#f92672>=</span> pipeline_executor<span style=color:#f92672>-&gt;</span>Execute(PARTIAL_CHUNK_COUNT);
        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>finished) {
            <span style=color:#66d9ef>return</span> TaskExecutionResult<span style=color:#f92672>::</span>TASK_NOT_FINISHED;
        }
    } <span style=color:#66d9ef>else</span> {
        pipeline_executor<span style=color:#f92672>-&gt;</span>Execute();
    }
    event<span style=color:#f92672>-&gt;</span>FinishTask();
    pipeline_executor.reset();
    <span style=color:#66d9ef>return</span> TaskExecutionResult<span style=color:#f92672>::</span>TASK_FINISHED;
}
</code></pre></div><p>在 PipelineTask::ExecuteTask() 中，通过 PipelineExecutor::Execute() 完成当前 ExecutorTask 的执行后它会去调用 Event::FinishTask() 函数进行 ExecutorTask 完成后各个 Event 子类自定义的收尾工作，在 Event::FinishTask() 函数如果发现当前 Event 的所有 Task 都执行完毕就会清理当前 Event 相关内容，并调用父亲 Event 的 CompleteDependency()：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>void</span> Event<span style=color:#f92672>::</span>Finish() {
    D_ASSERT(<span style=color:#f92672>!</span>finished);
    FinishEvent();
    finished <span style=color:#f92672>=</span> true;
    <span style=color:#75715e>// finished processing the pipeline, now we can schedule pipelines that depend on this pipeline
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>parent_entry : parents) {
        <span style=color:#66d9ef>auto</span> parent <span style=color:#f92672>=</span> parent_entry.lock();
        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>parent) { <span style=color:#75715e>// LCOV_EXCL_START
</span><span style=color:#75715e></span>            <span style=color:#66d9ef>continue</span>;
        } <span style=color:#75715e>// LCOV_EXCL_STOP
</span><span style=color:#75715e></span>        <span style=color:#75715e>// mark a dependency as completed for each of the parents
</span><span style=color:#75715e></span>        parent<span style=color:#f92672>-&gt;</span>CompleteDependency();
    }
    FinalizeFinish();
}
</code></pre></div><p>在 Event::CompleteDependency() 中，如果发现所有 dependency Event 都已经执行完毕，则会开始调度执行父亲 Event 的 Task。如果父亲 Event 没有 task 需要执行，则会再调用父亲 Event 的 Finish() 函数直接在当前线程中完成父亲 Event 的执行和收尾：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>void</span> Event<span style=color:#f92672>::</span>CompleteDependency() {
    idx_t current_finished <span style=color:#f92672>=</span> <span style=color:#f92672>++</span>finished_dependencies;
    D_ASSERT(current_finished <span style=color:#f92672>&lt;=</span> total_dependencies);
    <span style=color:#66d9ef>if</span> (current_finished <span style=color:#f92672>==</span> total_dependencies) {
        <span style=color:#75715e>// all dependencies have been completed: schedule the event
</span><span style=color:#75715e></span>        D_ASSERT(total_tasks <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>);
        Schedule();
        <span style=color:#66d9ef>if</span> (total_tasks <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
            Finish();
        }
    }
}
</code></pre></div><p>从上面代码可以看到 Event 调度 Task 是通过 Event::Schedule() 函数完成的，这是个 Event 的纯虚函数，不同的子类 Event 需要自行实现。Pipeline 执行过程中使用的 Event 类型不多，最常见的是：</p><ul><li>PipelineInitializeEvent：主要用来初始化当前 Pipeline 的 sink，会调度 1 个 PipelineInitializeTask</li><li>PipelineEvent：主要用来表示 Pipeline 的执行操作，可能会调度多个 ExecutorTask 到执行队列中。PipelineEvent 的 Schedule() 函数主要调用 Pipeline::Schedule() 完成 ExecutorTask 的计算调度，这里不再展开，感兴趣的朋友们可以继续追踪代码看看其中的实现细节</li><li>PipelineFinishEvent：主要用来标记当前 Pipeline 执行结束，在 Event::Finish() 检测到当前 Event 结束，调用到 PipelineFinishEvent::FinishEvent() 时完成 Pipeline::Finalize()，用来做 Pipeline 的清理操作</li><li>PipelineCompleteEvent：用来更新 Executor 中已结束的 Pipeline 的 counter completed_pipelines，Executor 主线程会不断检测 completed_pipelines，当发现所有中间 Pipeline 都执行完后，主线程会开始执行 root Pipeline，返回结果给客户端。</li></ul><p>我们在上面 Executor::InitializeInternal() 的函数中看到，DuckDB 的 Pipeline 分为了 2 部分：</p><ol><li>中间 Pipelines：包含所有除了 root 以外的 Pipeline。DuckDB 基于这些 Pipeline 之间的依赖关系构建了相应的 Event DAG，通过调度最底层没有任何依赖 Event 的 ExecutorTask 初始化了 TaskScheduler 的执行队列，进而催动了所有中间 Pipeline 的执行。</li><li>root Pipelines：在构建 Event DAG 的时候不会将这部分 Pipeline 考虑进去，这部分 Pipeline 也不会被 TaskScheduler 启动的后台线程异步执行。在完成中间 Pipeline 的初始调度后，主线程后续的工作和 root Pipeline 的执行过程我们在后面的小结来看。</li></ol><p>Pipeline 的初始调度正是由主线程执行 Executor::ScheduleEvents() 触发的，正式的调度逻辑是由 Executor::ScheduleEventsInternal() 完成的，这个函数的大致逻辑如下。概括来说就是寻找没有任何 dependency 的 Event，通过执行这些 Event.Schedule() 构造 ExecutorTask，放入 TaskScheduler 的工作队列，激活后台工作线程，开始 Pipeline 执行以及其他 Event 和 ExecutorTask 的连锁反应：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>void</span> Executor<span style=color:#f92672>::</span>ScheduleEventsInternal(ScheduleEventData <span style=color:#f92672>&amp;</span>event_data) {
    <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>events <span style=color:#f92672>=</span> event_data.events;
    D_ASSERT(events.empty());

    <span style=color:#75715e>// create all the required pipeline events
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>pipeline : event_data.meta_pipelines) {
        SchedulePipeline(pipeline, event_data);
    }

    <span style=color:#75715e>// set up the dependencies across MetaPipelines
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>event_map <span style=color:#f92672>=</span> event_data.event_map;
    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>entry : event_map) {
        <span style=color:#66d9ef>auto</span> pipeline <span style=color:#f92672>=</span> entry.first;
        <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>dependency : pipeline<span style=color:#f92672>-&gt;</span>dependencies) {
            <span style=color:#66d9ef>auto</span> dep <span style=color:#f92672>=</span> dependency.lock();
            D_ASSERT(dep);
            <span style=color:#66d9ef>auto</span> event_map_entry <span style=color:#f92672>=</span> event_map.find(dep.get());
            D_ASSERT(event_map_entry <span style=color:#f92672>!=</span> event_map.end());
            <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>dep_entry <span style=color:#f92672>=</span> event_map_entry<span style=color:#f92672>-&gt;</span>second;
            D_ASSERT(dep_entry.pipeline_complete_event);
            entry.second.pipeline_event<span style=color:#f92672>-&gt;</span>AddDependency(<span style=color:#f92672>*</span>dep_entry.pipeline_complete_event);
        }
    }

    <span style=color:#75715e>// verify that we have no cyclic dependencies
</span><span style=color:#75715e></span>    VerifyScheduledEvents(event_data);

    <span style=color:#75715e>// schedule the pipelines that do not have dependencies
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>for</span> (<span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>event : events) {
        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>event<span style=color:#f92672>-&gt;</span>HasDependencies()) {
            event<span style=color:#f92672>-&gt;</span>Schedule();
        }
    }
}
</code></pre></div><h2 id=5-pipelineexecutor-和-pipeline-内基于-push-的执行模型>5. PipelineExecutor 和 Pipeline 内基于 Push 的执行模型<a hidden class=anchor aria-hidden=true href=#5-pipelineexecutor-和-pipeline-内基于-push-的执行模型>#</a></h2><p>PipelineExecutor::Execute() 函数通过调用 Pipeline 中各个 PhysicalOperator 的相应接口，以 Push 的方式完成了当前 Pipeline 的执行，执行逻辑可以概括为：</p><ul><li>先调用 FetchFromSource() 从 Pipeline 的 source PhysicalOperator 中获取计算结果作为 source DataChunk，这里会调用 source 的 GetData() 接口。</li><li>再调用 ExecutePushInternal() 依次执行 Pipeline 中 operators 列表中的各个 PhysicalOperator 和最后一个 sink PhysicalOperator 完成这批数据后续的所有计算操作。对于普通 operator 会调用它的 Execute() 接口，对最后的 sink 会调用它的 Sink() 接口。PipelineExecutor::ExecutePushInternal() 可以看做是 Pipeline 内的数据消费者。</li><li>最后调用 PushFinalize() 完成当前 ExecutorTask 的执行，这里会调用 sink 的 Combine 接口，用以完成一个 ExecutorTask 结束后的收尾清理工作。</li></ul><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>bool</span> PipelineExecutor<span style=color:#f92672>::</span>Execute(idx_t max_chunks) {
    D_ASSERT(pipeline.sink);
    <span style=color:#66d9ef>bool</span> exhausted_source <span style=color:#f92672>=</span> false;
    <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>source_chunk <span style=color:#f92672>=</span> pipeline.operators.empty() <span style=color:#f92672>?</span> final_chunk : <span style=color:#f92672>*</span>intermediate_chunks[<span style=color:#ae81ff>0</span>];
    <span style=color:#66d9ef>for</span> (idx_t i <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>; i <span style=color:#f92672>&lt;</span> max_chunks; i<span style=color:#f92672>++</span>) {
        <span style=color:#66d9ef>if</span> (IsFinished()) {
            <span style=color:#66d9ef>break</span>;
        }
        source_chunk.Reset();
        FetchFromSource(source_chunk);
        <span style=color:#66d9ef>if</span> (source_chunk.size() <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
            exhausted_source <span style=color:#f92672>=</span> true;
            <span style=color:#66d9ef>break</span>;
        }
        <span style=color:#66d9ef>auto</span> result <span style=color:#f92672>=</span> ExecutePushInternal(source_chunk);
        <span style=color:#66d9ef>if</span> (result <span style=color:#f92672>==</span> OperatorResultType<span style=color:#f92672>::</span>FINISHED) {
            D_ASSERT(IsFinished());
            <span style=color:#66d9ef>break</span>;
        }
    }
    <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>exhausted_source <span style=color:#f92672>&amp;&amp;</span> <span style=color:#f92672>!</span>IsFinished()) {
        <span style=color:#66d9ef>return</span> false;
    }
    PushFinalize();
    <span style=color:#66d9ef>return</span> true;
}
</code></pre></div><p>PhysicalOperator 同时包含了 source、operator、sink 所需要的所有接口，各个 PhysicalOperator 需要实现对应的接口完成相应的计算逻辑。比如 partitioned hash join 因为会分成 3 个阶段分别作为 sink、operator 和 source 角色，它同时实现了所有的接口。</p><h2 id=6-主线程和-root-pipeline-的执行>6. 主线程和 root Pipeline 的执行<a hidden class=anchor aria-hidden=true href=#6-主线程和-root-pipeline-的执行>#</a></h2><p>root Pipelines 比较特殊：在构建 Event DAG 的时候不会将这部分 Pipeline 考虑进去，这部分 Pipeline 也不会被 TaskScheduler 启动的后台线程异步执行，这部分 Pipeline 要想得到执行也需要等待所有中间 Pipeline 执行结束。</p><p>root Pipeline 的执行是主线程通过调用 PipelineExecutor 的 Execute 函数完成的。主线程通过 TaskScheduler 启动的多个后台线程，通过 Event 触发和调度新一轮 Pipeline 的 ExecutorTask，Pipeline 就能够被后台执行了。剩下的问题就是主线程如何知道中间 Pipeline 执行结束，以及如何执行 root Pipeline 拿到最终结果返回给客户端。另外各个 Pipeline 在异步执行过程中可能会遇到一些 ERROR，主线程如何及时知道这些 ERROR 并返回给客户端也是需要处理的一个问题。</p><p>主线程完成中间 Pipeline 的初始调度后，因为 root Pipeline 在中间结果没有准备好之前也不能计算，这时为了加速查询的执行最好的办法就是主线程也参与到中间 Pipeline 的执行当中去。我们看到主线程会停留在 PendingQueryResult::ExecuteInternal() 的 while 循环这里：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>unique_ptr<span style=color:#f92672>&lt;</span>QueryResult<span style=color:#f92672>&gt;</span> PendingQueryResult<span style=color:#f92672>::</span>ExecuteInternal(ClientContextLock <span style=color:#f92672>&amp;</span>lock) {
    CheckExecutableInternal(lock);
    <span style=color:#66d9ef>while</span> (ExecuteTaskInternal(lock) <span style=color:#f92672>==</span> PendingExecutionResult<span style=color:#f92672>::</span>RESULT_NOT_READY) {
    }
    <span style=color:#66d9ef>if</span> (HasError()) {
        <span style=color:#66d9ef>return</span> make_unique<span style=color:#f92672>&lt;</span>MaterializedQueryResult<span style=color:#f92672>&gt;</span>(error);
    }
    <span style=color:#66d9ef>auto</span> result <span style=color:#f92672>=</span> context<span style=color:#f92672>-&gt;</span>FetchResultInternal(lock, <span style=color:#f92672>*</span><span style=color:#66d9ef>this</span>);
    Close();
    <span style=color:#66d9ef>return</span> result;
}
</code></pre></div><p>PendingQueryResult::ExecuteTaskInternal() 经过几次函数调用后最终会来到 PipelineExecutor::Execute() 函数。这个函数初始一看可能会比较绕，但想要实现的功能是：</p><ol><li>在所有中间 Pipeline 没有执行完之前一直和后台线程一起参与计算：如果从队列中取出来了一个 ExecutorTask 就尝试调用它的 Execute(TaskExecutionMode::PROCESS_PARTIAL) 函数完成小批量数据的计算，现在默认是 50 个 DataChunk。</li><li>如果所有 Pipeline 都执行完了，此时 completed_pipelines 与 total_pipelines（记录中间 Pipeline 的数量，不包含 root Pipeline）相等，Executor 会释放所有中间 Pipeline，标记 execution_result 为PendingExecutionResult::RESULT_READY。</li></ol><p>在这个小 while 循环中，如果没有取到 task，或者执行了 Task 的小部分任务后，都会去检测其他线程执行过程中是否有 Error 产生，用户是否 cancel 了 query 等等，一旦遇到错误产生，就会分别通过 CancelTasks() 和 ThrowException() 取消后台异步 Task 的执行并将错误抛给主线程的上层。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>PendingExecutionResult Executor<span style=color:#f92672>::</span>ExecuteTask() {
    <span style=color:#66d9ef>if</span> (execution_result <span style=color:#f92672>!=</span> PendingExecutionResult<span style=color:#f92672>::</span>RESULT_NOT_READY) {
        <span style=color:#66d9ef>return</span> execution_result;
    }
    <span style=color:#75715e>// check if there are any incomplete pipelines
</span><span style=color:#75715e></span>    <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>scheduler <span style=color:#f92672>=</span> TaskScheduler<span style=color:#f92672>::</span>GetScheduler(context);
    <span style=color:#66d9ef>while</span> (completed_pipelines <span style=color:#f92672>&lt;</span> total_pipelines) {
        <span style=color:#75715e>// there are! if we don&#39;t already have a task, fetch one
</span><span style=color:#75715e></span>        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>task) {
            scheduler.GetTaskFromProducer(<span style=color:#f92672>*</span>producer, task);
        }
        <span style=color:#66d9ef>if</span> (task) {
            <span style=color:#75715e>// if we have a task, partially process it
</span><span style=color:#75715e></span>            <span style=color:#66d9ef>auto</span> result <span style=color:#f92672>=</span> task<span style=color:#f92672>-&gt;</span>Execute(TaskExecutionMode<span style=color:#f92672>::</span>PROCESS_PARTIAL);
            <span style=color:#66d9ef>if</span> (result <span style=color:#f92672>!=</span> TaskExecutionResult<span style=color:#f92672>::</span>TASK_NOT_FINISHED) {
                <span style=color:#75715e>// if the task is finished, clean it up
</span><span style=color:#75715e></span>                task.reset();
            }
        }
        <span style=color:#66d9ef>if</span> (<span style=color:#f92672>!</span>HasError()) {
            <span style=color:#75715e>// we (partially) processed a task and no exceptions were thrown
</span><span style=color:#75715e></span>            <span style=color:#75715e>// give back control to the caller
</span><span style=color:#75715e></span>            <span style=color:#66d9ef>return</span> PendingExecutionResult<span style=color:#f92672>::</span>RESULT_NOT_READY;
        }
        execution_result <span style=color:#f92672>=</span> PendingExecutionResult<span style=color:#f92672>::</span>EXECUTION_ERROR;

        <span style=color:#75715e>// an exception has occurred executing one of the pipelines
</span><span style=color:#75715e></span>        <span style=color:#75715e>// we need to cancel all tasks associated with this executor
</span><span style=color:#75715e></span>        CancelTasks();
        ThrowException();
    }
    D_ASSERT(<span style=color:#f92672>!</span>task);

    lock_guard<span style=color:#f92672>&lt;</span>mutex<span style=color:#f92672>&gt;</span> elock(executor_lock);
    pipelines.clear();
    NextExecutor();
    <span style=color:#66d9ef>if</span> (HasError()) { <span style=color:#75715e>// LCOV_EXCL_START
</span><span style=color:#75715e></span>        <span style=color:#75715e>// an exception has occurred executing one of the pipelines
</span><span style=color:#75715e></span>        execution_result <span style=color:#f92672>=</span> PendingExecutionResult<span style=color:#f92672>::</span>EXECUTION_ERROR;
        ThrowException();
    } <span style=color:#75715e>// LCOV_EXCL_STOP
</span><span style=color:#75715e></span>    execution_result <span style=color:#f92672>=</span> PendingExecutionResult<span style=color:#f92672>::</span>RESULT_READY;
    <span style=color:#66d9ef>return</span> execution_result;
}
</code></pre></div><p>一旦 execution_result 的状态变为 RESULT_READY，就意味着我们结束了所有中间 Pipeline 的执行，Executor::ExecuteTask() 会一直返回 RESULT_READY，最外层的 for 循环也会退出，从而进入下一阶段，也就是执行 root Pipeline。root Pipeline 被 Executor 所持有，它的执行也是在 Executor::FetchChunk() 中完成的：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp>unique_ptr<span style=color:#f92672>&lt;</span>DataChunk<span style=color:#f92672>&gt;</span> Executor<span style=color:#f92672>::</span>FetchChunk() {
    D_ASSERT(physical_plan);

    <span style=color:#66d9ef>auto</span> chunk <span style=color:#f92672>=</span> make_unique<span style=color:#f92672>&lt;</span>DataChunk<span style=color:#f92672>&gt;</span>();
    root_executor<span style=color:#f92672>-&gt;</span>InitializeChunk(<span style=color:#f92672>*</span>chunk);
    <span style=color:#66d9ef>while</span> (true) {
        root_executor<span style=color:#f92672>-&gt;</span>ExecutePull(<span style=color:#f92672>*</span>chunk);
        <span style=color:#66d9ef>if</span> (chunk<span style=color:#f92672>-&gt;</span>size() <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>) {
            root_executor<span style=color:#f92672>-&gt;</span>PullFinalize();
            <span style=color:#66d9ef>if</span> (NextExecutor()) {
                <span style=color:#66d9ef>continue</span>;
            }
            <span style=color:#66d9ef>break</span>;
        } <span style=color:#66d9ef>else</span> {
            <span style=color:#66d9ef>break</span>;
        }
    }
    <span style=color:#66d9ef>return</span> chunk;
}
</code></pre></div><p>虽然从函数名来看 Executor 调用了 Pipeline::ExecutePull() 函数，但其实这个函数内部实现仍旧是 push 的方式，先从 source 拿到一批数据，然后再依次的经过所有 operators 的计算得到最终结果。</p><h2 id=7-pipeline-的构造>7. Pipeline 的构造<a hidden class=anchor aria-hidden=true href=#7-pipeline-的构造>#</a></h2><p>Pipeline 的执行框架我们已经大概了解，最后一个问题就是 PhysicalOperator tree 是如何转换成 Pipeline DAG 的了。Pipeline 主要由 source、operators 和 sink 这三部分构成，从物理执行计划划分 Pipeline 第一个遇到的问题是如何确定 Pipeline 的 sink 和 source。</p><p>DuckDB 采用了和 Hyper 一样的 Pipeline breaker 定义：那些需要消化掉所有孩子节点的数据后才能进行下一步计算输出结果的算子。典型的比如构造 hash join 或 hash aggregate 的 hash table，或者 sort 和 TopN 算子的排序操作，需要完全消费掉孩子节点的数据 后，才能得到正确结果进行下一阶段的数据。</p><p>算子的具体实现决定了 Pipeline 的构造。物理执行计划转成 Pipeline 是由其中的各个 PhysicalOperator 完成的，几个关键函数：</p><ul><li>Executor::InitializeInternal()：把物理执行计划（PhysicalOperator tree）转成 Pipeline 的入口，所有构造出来的 Pipeline 都存储在该查询的 Executor 中。</li><li>PhysicalOperator::BuildPipelines()：构造 Pipeline 的是通过 top down 的遍历 PhysicalOperator tree 完成的，Pipeline 的 sink 会先被确定下来（要么是整个物理执行计划的根节点，要么是上一个 Pipeline 的 source 节点）。Executor 通过该函数遍历每个 PhysicalOperator，决定将其加入当前 Pipeline 的 operators 列表还是做为当前 Pipeline 的 source。遇到当前 Pipeline 的 source 时就需要结束构造当前 Pipeline 了，然后将该 source 作为下一个 Pipeline 的 sink，继续 top down 的遍历 PhysicalOperator tree 和构造新的 Pipeline。</li><li>PhysicalOperator::BuildChildPipeline()：切分 Pipeline，构造 Pipeline 之间的依赖关系。</li></ul><p>PhysicalOperator::BuildPipelines() 是个虚函数，搜索代码可以看到，像 PhysicalJoin、PhysicalRecursiveCTE、PhysicalUnion 这些有多个孩子节点的以及一些比较特殊的算子都重载了这个虚函。其他没有重载该函数的算子，默认的 BuildPipelines() 函数如下：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-cpp data-lang=cpp><span style=color:#66d9ef>void</span> PhysicalOperator<span style=color:#f92672>::</span>BuildPipelines(Pipeline <span style=color:#f92672>&amp;</span>current, MetaPipeline <span style=color:#f92672>&amp;</span>meta_pipeline) {
    op_state.reset();

    <span style=color:#66d9ef>auto</span> <span style=color:#f92672>&amp;</span>state <span style=color:#f92672>=</span> meta_pipeline.GetState();
    <span style=color:#66d9ef>if</span> (IsSink()) {
        <span style=color:#75715e>// operator is a sink, build a pipeline
</span><span style=color:#75715e></span>        sink_state.reset();
        D_ASSERT(children.size() <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>);

        <span style=color:#75715e>// single operator: the operator becomes the data source of the current pipeline
</span><span style=color:#75715e></span>        state.SetPipelineSource(current, <span style=color:#66d9ef>this</span>);

        <span style=color:#75715e>// we create a new pipeline starting from the child
</span><span style=color:#75715e></span>        <span style=color:#66d9ef>auto</span> child_meta_pipeline <span style=color:#f92672>=</span> meta_pipeline.CreateChildMetaPipeline(current, <span style=color:#66d9ef>this</span>);
        child_meta_pipeline<span style=color:#f92672>-&gt;</span>Build(children[<span style=color:#ae81ff>0</span>].get());
    } <span style=color:#66d9ef>else</span> {
        <span style=color:#75715e>// operator is not a sink! recurse in children
</span><span style=color:#75715e></span>        <span style=color:#66d9ef>if</span> (children.empty()) {
            <span style=color:#75715e>// source
</span><span style=color:#75715e></span>            state.SetPipelineSource(current, <span style=color:#66d9ef>this</span>);
        } <span style=color:#66d9ef>else</span> {
            <span style=color:#66d9ef>if</span> (children.size() <span style=color:#f92672>!=</span> <span style=color:#ae81ff>1</span>) {
                <span style=color:#66d9ef>throw</span> <span style=color:#a6e22e>InternalException</span>(<span style=color:#e6db74>&#34;Operator not supported in BuildPipelines&#34;</span>);
            }
            state.AddPipelineOperator(current, <span style=color:#66d9ef>this</span>);
            children[<span style=color:#ae81ff>0</span>]<span style=color:#f92672>-&gt;</span>BuildPipelines(current, meta_pipeline);
        }
    }
}
</code></pre></div><p>PhysicalOperator::BuildPipelines() 不仅构建了 PhysicalOperator 和 Pipeline 的关系，也构建了 Pipeline 之间的依赖关系：如果某个 PhysicalOperator 是 Pipeline breaker，那么它不仅会作为当前 Pipeline 的 source，也会作为下一个 Pipeline 的 sink，Pipeline breaker 算子隐含了 Pipeline 之间的计算先后关系，只有上游 Pipeline 完全完成计算后才能开启下游 Pipeline 的计算。</p><p>一个简单的单表聚合为例，它的执行计划和对应的 Pipeline 可以表示成下图，其中 Pipeline 1 依赖 Pipeline 2：</p><p><img src=/images/duckdb-push-based-execution-model/topn-and-aggregate.jpg alt="TopN and Aggregate to Pipelines"></p><h3 id=71-从-physicalunion-构造-pipeline>7.1. 从 PhysicalUnion 构造 Pipeline<a hidden class=anchor aria-hidden=true href=#71-从-physicalunion-构造-pipeline>#</a></h3><p>我们以 Union All 为例介绍一个稍微复杂有多个 child 的情况。DuckDB 的 Union All 用 PhysicalUnion 来表示，每个 PhysicalUnion 有 2 个孩子节点。如果用户 SQL 中有 N 个表 Union All，那么就会构造出 N-1 个 PhysicalUnion 算子。PhysicalUnion 仅仅用来汇总多个数据源，传递孩子节点的数据给它的父节点完成计算。</p><p>PhysicalUnion 有多个 child 数据源，意味着 PhysicalUnion 往下 top down 构造 Pipeline 的时候需要分别给各个孩子节点传递不同的 Pipeline，那这 2 个 Pipeline 的 sink 应该是什么呢。考虑到 PhysicalUnion 没有计算逻辑仅汇总数据的特殊性，DuckDB 让这 2 个 Pipeline 共享当前传递过来的 Pipeline 的 sink 和当前的 operators 列表，然后各自在自己的 operators 列表中新增自己的算子，设置自己的 sink。</p><p>这样的 Pipeline 分裂可以使 PhysicalUnion 父节点的计算逻辑和对应的中间状态在这 2 个 Pipeline 之间复用，虽然 PhysicalUnion 孩子节点的计算逻辑位于不同 Pipeline 之间各自独立产生计算结果，但 PhysicalUnion 之后的计算逻辑和中间状态在不同 Pipeline 之间是共用的，可以确保计算的正确性。</p><p>不过这样的 Pipeline 构造带来了额外的问题，我们上面提到 Pipeline breaker 确定了 Pipeline 之间的计算调度关系，并且每个 Pipeline 还可以独立设置自己的并发度。对于 PhysicalUnion 所处的 Pipeline 来说，这个 Pipeline 的 sink 同时属于多个 Pipeline（PhysicalUnion 分裂出来的），只有这些 Pipeline 都完成执行后才能执行他们的下游 Pipeline。所以后面在 Pipeline 调度的时候这里还需要特殊处理下。</p><p>一个简单的 UNION ALL 为例，它的执行计划和对应的 Pipeline 可以表示成下图：</p><p><img src=/images/duckdb-push-based-execution-model/unionall.jpg alt="Union All to Pipelines"></p><h3 id=72-从-physicaljoin-构造-pipeline>7.2. 从 PhysicalJoin 构造 Pipeline<a hidden class=anchor aria-hidden=true href=#72-从-physicaljoin-构造-pipeline>#</a></h3><p>理解了 Union All 的 Pipeline 构造，我们再来看看稍微复杂点的 Join。PhysicalJoin 的 Pipeline 构造相对来说要复杂一点，需要我们先大致了解下 DuckDB 中 PhysicalJoin 的实现。</p><p>DuckDB 的 Hash Join 采用了 partitioned hash join，当数据量比较大的时候可以通过 repartition 将数据落盘避免 OOM，这个多线程版本的 partitioned hash join，主要分为 3 个阶段：</p><ol><li>并发读取和计算所有 build 端的数据，当所有数据都读完后检查总数据量是否能全部放在内存中，如果不能就将 build 端的数据 repartition，选出第一批能放在内存中的 partition 为它们构造 hash table，剩下的数据存放在磁盘上。</li><li>并发读取和计算所有 probe 端的数据，这时读上来的数据要么属于内存中的 partition，要么属于磁盘上的 partition，先把属于磁盘上的 partition 的数据落盘，用属于内存中的 partition 的数据去 probe 此时 build 端的放在内存中的 hash table，得到结果返回给上层。</li><li>并发处理磁盘上的数据：挑选一批 build 端能放入内存的 partition，构造 hash table，然后 probe 端去并发的 probe 得到结果进行下一步计算。循环这样的处理过程直到所有磁盘上的 partition 都 join 完成。</li></ol><p>这 3 个过程也分别对应了 3 个基本的 Pipeline，可以表示成下图，其中 Pipeline 2 依赖 pipeline 1，Pipeline 3 依赖 Pipeline 2：</p><p><img src=/images/duckdb-push-based-execution-model/join.jpg alt="Hash Join to Pipelines"></p><h2 id=8-duckdb-执行模式的一些感受和思考>8. DuckDB 执行模式的一些感受和思考<a hidden class=anchor aria-hidden=true href=#8-duckdb-执行模式的一些感受和思考>#</a></h2><h3 id=81-计算调度的复杂性>8.1. 计算调度的复杂性<a hidden class=anchor aria-hidden=true href=#81-计算调度的复杂性>#</a></h3><p>这是一个最直观的感受。相比 Pull 模型，Push 模型在实现时需要多考虑如何控制 Pipeline 的计算调度，也需要考虑一个 Pipeline 内数据消费速度的问题（这个我们还没在本文涉及），这些代码增加了工程实现的复杂度，也增加了问题诊断的复杂度。</p><h3 id=81-pipelinebreaker-的作用>8.1. PipelineBreaker 的作用<a hidden class=anchor aria-hidden=true href=#81-pipelinebreaker-的作用>#</a></h3><p>我喜欢把计算抽象为数据和计算两个部分，就像 CPU 的 L1 Cache 分为 L1D 和 L1I 一样，之前思考 Pipeline breaker 的时候更多是从计算性能角度，这次在思考 Pipeline 之间的依赖关系和 ExecutorTask 的调度时才意识到这个容易被忽略的地方：Pipeline 其实也反应了两个比较大的计算过程之间的先后关系。这个关系在 Volcano 模型的 Pull 中没有那么明显，只是写代码时为了保证正确性其实会应用这些依赖关系，比如 hash join 在 build side 没有结束时就不会对外返回结果。</p><h3 id=83-除了带来性能提升外这种并发-push-执行模型还有其他优势吗>8.3. 除了带来性能提升外，这种并发 Push 执行模型还有其他优势吗？<a hidden class=anchor aria-hidden=true href=#83-除了带来性能提升外这种并发-push-执行模型还有其他优势吗>#</a></h3><p>从数据库、数据仓库执行引擎的经验来看，遇到最多的线上问题可以分为两类：查询跑的慢并发，以及查询吃的内存太狠导致查询自己或者进程 OOM。DuckDB 因为有了基于 ExecutorTask 的计算调度机制，我们就有机会从源头来控制：如果内存或 CPU 资源有限就少调度些 ExecutorTask 来执行。这样至少能够把查询失败的问题变成查询变慢的问题，然后查询变慢的时候再去看资源使用率满不满，这样至少能守住服务可用性的 SLO，也比较符合一般的问题排查思路。</p><h2 id=9-参考材料>9. 参考材料<a hidden class=anchor aria-hidden=true href=#9-参考材料>#</a></h2><ul><li><a href=https://github.com/duckdb/duckdb/issues/1583>issues/1583</a> Move to push-based execution model</li><li>Push-Based Execution in DuckDB, by Mark Raasveldt: <a href=https://dsdsd.da.cwi.nl/slides/dsdsd-duckdb-push-based-execution.pdf>slides</a>, <a href="https://www.youtube.com/watch?v=1kDrPgRUuEI">video</a></li></ul></div><footer class=post-footer><nav class=paginav><a class=prev href=https://zz-jason.github.io/posts/orthogonal-optimization-of-subqueries-and-aggregation/><span class=title>« Prev Page</span><br><span>[SIGMOD 2001] Orthogonal Optimization of Subqueries and Aggregation</span></a>
<a class=next href=https://zz-jason.github.io/posts/2021-books/><span class=title>Next Page »</span><br><span>2021 读书总结</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [DuckDB] Push-Based Execution Model on twitter" href="https://twitter.com/intent/tweet/?text=%5bDuckDB%5d%20Push-Based%20Execution%20Model&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fduckdb-push-based-execution-model%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [DuckDB] Push-Based Execution Model on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fduckdb-push-based-execution-model%2f&title=%5bDuckDB%5d%20Push-Based%20Execution%20Model&summary=%5bDuckDB%5d%20Push-Based%20Execution%20Model&source=https%3a%2f%2fzz-jason.github.io%2fposts%2fduckdb-push-based-execution-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [DuckDB] Push-Based Execution Model on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fzz-jason.github.io%2fposts%2fduckdb-push-based-execution-model%2f&title=%5bDuckDB%5d%20Push-Based%20Execution%20Model"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [DuckDB] Push-Based Execution Model on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fzz-jason.github.io%2fposts%2fduckdb-push-based-execution-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [DuckDB] Push-Based Execution Model on whatsapp" href="https://api.whatsapp.com/send?text=%5bDuckDB%5d%20Push-Based%20Execution%20Model%20-%20https%3a%2f%2fzz-jason.github.io%2fposts%2fduckdb-push-based-execution-model%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [DuckDB] Push-Based Execution Model on telegram" href="https://telegram.me/share/url?text=%5bDuckDB%5d%20Push-Based%20Execution%20Model&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fduckdb-push-based-execution-model%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=zz-jason/zz-jason.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://zz-jason.github.io/>Jian Zhang</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script></body></html>