<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[VLDB 2023] Scalable and Robust Latches for Database Systems | Jian Zhang</title><meta name=keywords content><meta name=description content="Introduction Efficient and scalable synchronization is one of the key require- ments for systems that run on modern multi-core processors. 但是虽然有很多类型的 Lock，但是却没有人详细研究过什么样的 Lock 适用于数据库的工作负载，主要原因是数据"><meta name=author content="Jian"><link rel=canonical href=https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-latches/><link href=/assets/css/stylesheet.min.bdb03579c17662815eb277a528f9b9b67a014d2778e9fb0aa5489aa14f7b2643.css integrity="sha256-vbA1ecF2YoFesnelKPm5tnoBTSd46fsKpUiaoU97JkM=" rel="preload stylesheet" as=style><link rel=icon href=https://zz-jason.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zz-jason.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zz-jason.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zz-jason.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zz-jason.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.80.0"><script type=application/javascript>var doNotTrack=false;if(!doNotTrack){window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;ga('create','UA-67872953-1','auto');ga('send','pageview');}</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="[VLDB 2023] Scalable and Robust Latches for Database Systems"><meta property="og:description" content="Introduction Efficient and scalable synchronization is one of the key require- ments for systems that run on modern multi-core processors. 但是虽然有很多类型的 Lock，但是却没有人详细研究过什么样的 Lock 适用于数据库的工作负载，主要原因是数据"><meta property="og:type" content="article"><meta property="og:url" content="https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-latches/"><meta property="article:published_time" content="2023-04-05T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-05T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[VLDB 2023] Scalable and Robust Latches for Database Systems"><meta name=twitter:description content="Introduction Efficient and scalable synchronization is one of the key require- ments for systems that run on modern multi-core processors. 但是虽然有很多类型的 Lock，但是却没有人详细研究过什么样的 Lock 适用于数据库的工作负载，主要原因是数据"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zz-jason.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[VLDB 2023] Scalable and Robust Latches for Database Systems","item":"https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-latches/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[VLDB 2023] Scalable and Robust Latches for Database Systems","name":"[VLDB 2023] Scalable and Robust Latches for Database Systems","description":"Introduction Efficient and scalable synchronization is one of the key require- ments for systems that run on modern multi-core processors. 但是虽然有很多类型的 Lock，但是却没有人详细研究过什么样的 Lock 适用于数 …","keywords":[],"articleBody":"Introduction Efficient and scalable synchronization is one of the key require- ments for systems that run on modern multi-core processors.\n但是虽然有很多类型的 Lock，但是却没有人详细研究过什么样的 Lock 适用于数据库的工作负载，主要原因是数据库的工作负载范围很大，有 write-heavy OLTP 事务，也有 read-only 的 OLAP 查询，甚至是两者都有的 HTAP 负载。\n在设计 Umbra 的时候，作者开始调研不同的锁机制和他们之间的效率差异。首先第一个发现是不可能有一种锁机制在所有工作负载和所有硬件环境中都是最优的。How- ever, we noticed that there are some re-occurring best practices for locking and synchronization。于是作者们首先总结了数据库在 Lock 上的需求，然后 address them by analyzing and evaluating different locking techniques accordingly。\n那对数据库友好的 Lock 需要具备什么样的功能呢？\n读是需要又快有能 scale 的：\n In general, most database workloads, even OLTP transac- tions, mostly read data, and thus reading should be fast and scalable.\n 在 LLVM JIT 的情况下，纯粹使用 OS 提供的 Lock 不太行：\n Many modern in-memory database systems compile queries to efficient machine code to keep the latency as low as possible [26]. A lock should therefore integrate well with query compilation and avoid external function calls. This requirement makes pure OS- based locks unattractive for frequent usage during query execution.\n 因为 Lock 需要保护一些细粒度的数据结构，Lock 本身的空间开销应该尽量小：\n To protect fine-granular data like index nodes, or hash table buckets, the lock itself should be space efficient. This does not necessarily mean minimal, but it should also not waste unreasonable amount of space. For instance, a std::mutex (40-80 bytes) would almost double the size required for an ART node [17].\n Lock 还需要能够高效的处理锁竞争：\n While we assume that heavy contention is usually rare in a well-designed DBMS, some workloads make it unavoidable. The lock should, thus, handle contention gracefully without sacrificing its fast, uncondented path.\n 同时最好也要能够周期性的检查是否有被 cancel：\n for example, that the user wants to cancel a long-running query, but the working thread is currently sleeping while waiting for a lock. Waiting too long can lead to an unpleasant user experience. For this reason, it would be desirable if the lock’s API would allow one to incorporate periodic cancellation checks while a thread is waiting.\n Locking Technioques 在数据库中，我们对 lock 的需求是既要最小化 overhead 又要最大化 scalability。最近的一些研究结果表明，除了在 write-heavy 的场景里 pessimistic locking 的优势会更大以外，其他场景中 Optimistic Locking 相比 pessimistic locking 或者 lock-free 都有更好的性能和其他优势。这里作者罗列了几个常见的 Optimistic locking 和它们的优缺点，详细介绍了目前在 Umbra 中使用的 Hybrid Lock 设计和实现细节。\nOptimistic Locking Optimistic Locking 的基本思路是检查在读数据的时候没有其他线程正在修改它。Optimistic locking 会维护一个版本号，每次修改数据都增加这个版本号。读的时候如果发现 encode 在版本号中的 lock 比特位为 1，或者读取数据后版本号发生了变化，这次读操作就需要重试。上图的伪代码展示了一个可能 fallback 到 pessimistic lock 的 optimistic lock 实现，推测 isLocked(preVersion) 就是在检查 lock 比特位。\nOptimistic Locking 和 Pessimistic Lock 的性能差异原因主要出现在 cache line 上：\n Optimistic locking avoids atomic writes and its cache line stays in shared mode. Pessimistic locks must always modify the cache line and thus their performance is bound by cache-coherency latencies\n Optimistic locking 特别适合用在经常读的热点数据上，比如 B Tree 的 root 节点。使用时需要注意几个问题：\n Optimistic locking 在碰到写的时候会重试正在进行的读操作，需要确保重试时不会发生异常。比如读取某个 B Tree 节点，可能别的线程触发了 B Tree Merge 操作导致当前要读的节点被删除了，需要确保重试时访问这个节点不会出现访问 null pointer 的情况。LeanStore 和作者提到的 Adaptive Radix Tree 可以通过 Epoch 机制来确保不会发生这个问题 注意上面伪代码传入的是一个 readCallBack，这个 callback 每次重试都会被调用，如果在里面更新一些值，比如求 count，那么可能就会因为重试得到错误结果。最安全的做法是仅通过这个 callback 获取值后就 buffer 住，等整个读操作返回后才用读到的值去做下一步的计算 当很多线程并发写时，optimistic lock 很容易被饿死，所以就像伪代码描述的那样，在经历了最大重试次数后需要能够 fallback 到 pessimistic lock  Speculative Locking (HTM) Speculative locking 是 Intel 在硬件上支持的一种 optimistic locking。即使是多个写线程也可以同时持有这个 lock，只要它们没有出现写冲突。这个 lock 有很多限制：\n All conflicts are detected on L1-cache line granularity (usually 64 bytes) and the addresses of the joint read/write-set must fit into L1 cache. Additionally, the critical section should be short to avoid interrupts or context switches and must avoid certain system calls.\n Speculative locking 这种基于硬件的 lock 最主要的缺点就是它和硬件绑定，不够通用。只有比较新的 Intel 和 ARM 处理器才支持类似的功能。考虑到上面提到的使用限制，以及一些处理器可能没有这个功能，使用者通常都需要实现一个 fallback 到传统 lock 的机制来应对这些问题。\nHybrid Locking 重读少写的场景可以使用 optimistic locking 获取最好的性能，读写混合的场景就需要用到 pessimistic locking 了。从上面的表格来看，不管是 write-heavy 还是 write-only 场景，使用 shared lock 都是一个更好的 pessimistic locking 选择。\n要在不同场景都获得最好的性能，就需要根据上下文对同一个数据上不同的锁。比如 B Tree，访问 read-contended 上层节点就可以使用 optimistic locking，而访问叶子结点去 scan 上面的数据时就最好使用 pessimistic locking 避免代价高昂的冲突重试。\n为了达到这个目标，作者设计了一个 Hybrid-Lock。如上图所示，这个 Hybrid-Lock 内部同时包含 RWMutex 和 atomic 分别用来做 Pessimistic Lock 和 Optimistic Lock。\n理论上也可以把这个 RWMutex 和 version 通过一个 64 位整数来实现。把他们分开后，实现起来简单直接不容易出问题，为 Hybrid-Lock 实现各种 RWMutex 已有的接口也比较简单，比如下图 HybridLock 的 lockShared()、unlockShared()、lockExclusive()、unlockExclusive() 就直接直接使用了内部的 RWMutex()。\n还是以 B Tree 为例考虑这么一种情况：一个读线程通过调用 tryReadOptimistically() 来访问某个中间节点，tryReadOptimistically() 刚开始执行时没有任何其他线程读写这个节点，该函数顺利执行到了 readCallback()，但在 readCallback() 执行中另一个线程通过 lockExcluseive() 对这个节点上锁，修改这个节点的内容，最后通过 unlockExclusive() 释放锁。如果 unlockExclusive() 先释放了 RWMutex 的 lock 而没有完成 version +1，那么读线程因为是先检查 lock 再检查 version 就可能读到错误的值。\n需要特别注意 unlockExclusive() 里面两个操作的顺序。因为 tryReadOptimistically() 在执行完 readCallBack 后先检查 RWMutex 再检查 version，unlockExclusive() 里面就需要先 version +1 再释放 RWMutex，确保读操作一定能够检测到这个读写冲突并重试。在 Intel 平台上也可以使用 CMPXCHG16B 指令来同时更新 version 和释放锁。\nreadOptimisticIfPossible() 和一开始在 Optimistic Locking 中看到的伪代码工作机制稍微有点区别。它会在 tryReadOptimistically() 失败后直接从回退到 pessimistic locking 模式，使用 lockShared() 和 unLockShared() 完成这次读操作。\nHybrid-Lock 再加上后面提出的 ParkingLot 的 lock contention 处理策略就是目前 Umbra 中使用的锁实现，替代了之前提到的 Versioned Latch 方案。下面就是 Hybrid Lock 的伪代码实现。\nContention Handling 锁冲突是很难避免的，比如在 write-heavy 场景所有线程都使用 pessimistic locking 时，Hybrid Lock 中的 RWMutex 上就可能发生锁冲突。如何高效处理锁冲突，并且在线程等锁期间能够识别到 query 被 cancel 及时停止 query 执行呢？\n这个章节作者分析了 Hybrid Lock 可能的 RWMutex 实现，最终采用了 Parking Lot 的方案。\nBusy-Waiting/Spinning Spinning 是一种常见的处理方式，和 Spinlock 一样。Spinning 的一些缺点：\n Spinning can lead to priority inversion, as spinning threads seem very busy to a scheduler they might receive higher priority than a thread that does useful work. Especially in the case of over-subscription, this can cause critical problems Heavy spinning wastes resources and energy [6] and increases cache pollution, which is caused by additional bus traffic. 作者通过 cache line 的例子详细解释了这个问题：Following the MESI-protocol, every atomic write needs to invalidate all existing copies in other cores. Ideally, a core owns a cache line exclusively and does not need to send any invalidation messages. However, if other threads are spinning on the same lock, they constantly request this cache line, causing contention. The negative effects are worst when the waiting thread does write-for-ownership cycles, as those cause expensive invalida- tion messages. For this reason, a waiting thread should use the test-test-and-set pattern and only do the write-for-ownership cycle when it sees that the lock is available. In other words, it only reads the lock state in the busy loop to keep the lock’s cache line in shared mode. 和 lock 处于同一 cache line 的数据也会受到影响，也就是 false-sharing 的问题。spinning can still lead to cache pollution when the protected data is on the same cache line as the lock itself (cf. Figure 3). By spinning on the lock the waiting thread 𝑇𝑤𝑎𝑖𝑡 constantly loads the cache line in shared mode. Whenever the lock owning 𝑇h𝑎𝑠𝐿𝑜𝑐𝑘 updates the protected data, it must invalidate 𝑇𝑤𝑎𝑖𝑡 ’s copy of the cache line. Having to send these invalidation messages, slows down 𝑇h𝑎𝑠𝐿𝑜𝑐𝑘 and increases the time spent in the critical section.  虽然现有方案通过 backoff 策略可以缓解 spinlock 的这些问题，但这些策略本身也不是很完美：\n there exist several backoff strate- gies that add pause instructions to put the CPU into a lower power state, or call sched_yield to encourage the scheduler to switch to another thread. However, since the scheduler cannot guess when the thread wants to proceed, yielding is generally not recommended as its behavior is largely unpredictable [31].\n Local Spinning using Queuing spinning 带来的 cache contention 问题在 NUMA 架构上会更严重。为了解决这个问题，一些 spinlock 的实现只 spin 这个 lock 的 thread-local 副本，比如 MCS-lock 或者 Krieger et al. 在 [13, 25] 提出的 read-write mutex。\n这种 lock 的工作方式：\n When acquiring a lock, every thread creates a thread-local instance of the lock structure including its lock state and a next pointer to build a linked list of waiting threads.3 Then, it exchanges the next pointer of the global lock, making it point to its own local instance. If the previous next entry was nil, the lock acquisition was successful. Otherwise, if the entry already pointed to another instance, the thread enqueues itself in the wait- ing list by updating the next-pointer of the found instance (current tail) to itself.\n Ticket Spinlock ticket spinlock 是另一种 spinlock，guarantees fairness without using queues。\n它的工作方式：\n maintaining two counters: next-ticket and now-serving. A thread gets a ticket using an atomic fetch_and_add and waits until its ticket number matches that of now-serving.\n 除了能够保证 fairness 以外，它还有其他优点：\n this also enables more precise backoff in case of contention by estimating the wait time. The wait time can be estimated by multiplying the position in the queue and the expected time spent in the critical section. Mellor-Crummey and Scott argue that it is best to use the minimal possible time for the critical section, as overshooting in backoff will delay all other threads in line due to the FIFO nature\n Kernel-Supported ParkingLot 上面提到的 spinlock 始终存在 over-subscription 或者 waste of energy 的问题。因此很多库的锁实现（比如 pthread mutex）都基于 Linux 内核提供的 kernel-level locking 来 suspend 当前这个线程直到拿到这个 lock 为止。\n但内核上的系统调用开销是很高的，所以也有一些自适应的锁实现，仅锁冲突的时候才调用 kernel 阻塞当前线程，比如 Linux 提供的 futex。\n基于 futex 的思路，WebKit 提出了一个叫 Parking Lot 的自适应锁。也是 Umbra 目前正在使用的锁实现。Parking Lot 用一个全局哈希表来存储 lock 到 wait queue 的映射关系。和 Linux futex 不一样，这种实现方式更加通用，可移植性强，不依赖其非标准的或者特定平台的系统调用。它也更加灵活，比如在 Parking 的时候可以执行某个 callback 函数。Umbra 利用这个特性在锁等待时检查查询是否被取消，Page 是否已经被缓存替换等。\n上图描述了 Umbra 中实现的 Parking Lot 锁。当线程获取锁后会将 lock bit (L) 设置为 1。当另一个线程再次获取锁时，它会在 parking lot 中等待。此时它会把锁的 wait bit (W) 设置为 1 表示有人正在等锁，然后使用这个锁的地址在哈希表中找到该锁对应的 parking space。如果仍旧满足用户自定义的 wait condition，该线程开始等待这个 condition variable。当第 1 个线程释放锁后，它发现 wait bit (W) 为 1 知道有其他线程正在等锁，它会找到这个锁对应的 parking space，将所有等待的线程都唤醒。为了避免 parking space 的 data race 问题，每个 parking space 都有一个 mutex 来保护。\n关于 lock bit 和 wait bit，作者在论文的 2.3 小结介绍完 Hybrid Lock 后有个补充说明，放到这里我们了解到 parking lot 后就比较容易理解了：\n wait bit：encode 在 Hybrid-Lock 的 RWMutex 上，用来表示有其他线程等锁 lock bit：encode 在 Hybrid-Lock 的 version 上，检测到有锁后当前线程需要调用下面伪代码中的 park() 函数进入 parking 状态，直到被 condition variable 唤醒。  Parking lot 本质上就是个固定 512 槽位的哈希表，因为冲突的锁数量最多不会超过使用的线程数，所以 512 个槽位就足够用了。采用拉链法解决哈希冲突。\n当执行用户 query 的线程在 parking space 中等待时，每 10ms 会被唤醒检查当前 query 是否被取消了，以便停止等待及时结束当前 query 的执行。\n下面是 parking lot 的伪代码。虽然非常简单直接，但使用需要特别小心。需要确保造成线程等待的信息不会丢失导致线程无限期的等待下去，任何修改这些数据的线程都需要检查锁的 wait bit，在必要时唤醒所有等待的线程。作者这里没举例子，我能想到的一个场景是 B Tree Node 被缓存替换的场景。\n论文到这（第 5 页）核心内容就结束了，后面花了大量的篇幅介绍和展示作者的测试结果。\nEvaluation TPC-C and TPC-H Lock Granularity Space Consumption Efficiency of Lock Acquisition Contention Handling Strategies RELATED WORK CONCLUSION ","wordCount":"4195","inLanguage":"en","datePublished":"2023-04-05T00:00:00Z","dateModified":"2023-04-05T00:00:00Z","author":{"@type":"Person","name":"Jian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-latches/"},"publisher":{"@type":"Organization","name":"Jian Zhang","logo":{"@type":"ImageObject","url":"https://zz-jason.github.io/favicon.ico"}}}</script></head><body id=top><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://zz-jason.github.io/ accesskey=h title="Jian Zhang (Alt + H)">Jian Zhang</a>
<span class=logo-switches></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://zz-jason.github.io/ title=Home><span>Home</span></a></li><li><a href=https://zz-jason.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://zz-jason.github.io/posts/ title=Archives><span>Archives</span></a></li><li><a href=https://zz-jason.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[VLDB 2023] Scalable and Robust Latches for Database Systems</h1><div class=post-meta>April 5, 2023&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Jian</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#locking-technioques aria-label="Locking Technioques">Locking Technioques</a><ul><li><a href=#optimistic-locking aria-label="Optimistic Locking">Optimistic Locking</a></li><li><a href=#speculative-locking-htm aria-label="Speculative Locking (HTM)">Speculative Locking (HTM)</a></li><li><a href=#hybrid-locking aria-label="Hybrid Locking">Hybrid Locking</a></li></ul></li><li><a href=#contention-handling aria-label="Contention Handling">Contention Handling</a><ul><li><a href=#busy-waitingspinning aria-label=Busy-Waiting/Spinning>Busy-Waiting/Spinning</a></li><li><a href=#local-spinning-using-queuing aria-label="Local Spinning using Queuing">Local Spinning using Queuing</a></li><li><a href=#ticket-spinlock aria-label="Ticket Spinlock">Ticket Spinlock</a></li><li><a href=#kernel-supported-parkinglot aria-label="Kernel-Supported ParkingLot">Kernel-Supported ParkingLot</a></li></ul></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a><ul><li><a href=#tpc-c-and-tpc-h aria-label="TPC-C and TPC-H">TPC-C and TPC-H</a></li><li><a href=#lock-granularity aria-label="Lock Granularity">Lock Granularity</a></li><li><a href=#space-consumption aria-label="Space Consumption">Space Consumption</a></li><li><a href=#efficiency-of-lock-acquisition aria-label="Efficiency of Lock Acquisition">Efficiency of Lock Acquisition</a></li><li><a href=#contention-handling-strategies aria-label="Contention Handling Strategies">Contention Handling Strategies</a></li></ul></li><li><a href=#related-work aria-label="RELATED WORK">RELATED WORK</a></li><li><a href=#conclusion aria-label=CONCLUSION>CONCLUSION</a></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Efficient and scalable synchronization is one of the key require- ments for systems that run on modern multi-core processors.</p><p>但是虽然有很多类型的 Lock，但是却没有人详细研究过什么样的 Lock 适用于数据库的工作负载，主要原因是数据库的工作负载范围很大，有 write-heavy OLTP 事务，也有 read-only 的 OLAP 查询，甚至是两者都有的 HTAP 负载。</p><p>在设计 Umbra 的时候，作者开始调研不同的锁机制和他们之间的效率差异。首先第一个发现是不可能有一种锁机制在所有工作负载和所有硬件环境中都是最优的。How- ever, we noticed that there are some re-occurring best practices for locking and synchronization。于是作者们首先总结了数据库在 Lock 上的需求，然后 address them by analyzing and evaluating different locking techniques accordingly。</p><p>那对数据库友好的 Lock 需要具备什么样的功能呢？</p><p>读是需要又快有能 scale 的：</p><blockquote><p>In general, most database workloads, even OLTP transac- tions, mostly read data, and thus reading should be fast and scalable.</p></blockquote><p>在 LLVM JIT 的情况下，纯粹使用 OS 提供的 Lock 不太行：</p><blockquote><p>Many modern in-memory database systems compile queries to efficient machine code to keep the latency as low as possible [26]. A lock should therefore integrate well with query compilation and avoid external function calls. This requirement makes pure OS- based locks unattractive for frequent usage during query execution.</p></blockquote><p>因为 Lock 需要保护一些细粒度的数据结构，Lock 本身的空间开销应该尽量小：</p><blockquote><p>To protect fine-granular data like index nodes, or hash table buckets, the lock itself should be space efficient. This does not necessarily mean minimal, but it should also not waste unreasonable amount of space. For instance, a std::mutex (40-80 bytes) would almost double the size required for an ART node [17].</p></blockquote><p>Lock 还需要能够高效的处理锁竞争：</p><blockquote><p>While we assume that heavy contention is usually rare in a well-designed DBMS, some workloads make it unavoidable. The lock should, thus, handle contention gracefully without sacrificing its fast, uncondented path.</p></blockquote><p>同时最好也要能够周期性的检查是否有被 cancel：</p><blockquote><p>for example, that the user wants to cancel a long-running query, but the working thread is currently sleeping while waiting for a lock. Waiting too long can lead to an unpleasant user experience. For this reason, it would be desirable if the lock’s API would allow one to incorporate periodic cancellation checks while a thread is waiting.</p></blockquote><h2 id=locking-technioques>Locking Technioques<a hidden class=anchor aria-hidden=true href=#locking-technioques>#</a></h2><p>在数据库中，我们对 lock 的需求是既要最小化 overhead 又要最大化 scalability。最近的一些研究结果表明，除了在 write-heavy 的场景里 pessimistic locking 的优势会更大以外，其他场景中 Optimistic Locking 相比 pessimistic locking 或者 lock-free 都有更好的性能和其他优势。这里作者罗列了几个常见的 Optimistic locking 和它们的优缺点，详细介绍了目前在 Umbra 中使用的 Hybrid Lock 设计和实现细节。</p><h3 id=optimistic-locking>Optimistic Locking<a hidden class=anchor aria-hidden=true href=#optimistic-locking>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304052144085.png alt="Listing 1: Optimistic Locking"></p><p>Optimistic Locking 的基本思路是检查在读数据的时候没有其他线程正在修改它。Optimistic locking 会维护一个版本号，每次修改数据都增加这个版本号。读的时候如果发现 encode 在版本号中的 lock 比特位为 1，或者读取数据后版本号发生了变化，这次读操作就需要重试。上图的伪代码展示了一个可能 fallback 到 pessimistic lock 的 optimistic lock 实现，推测 <code>isLocked(preVersion)</code> 就是在检查 lock 比特位。</p><p>Optimistic Locking 和 Pessimistic Lock 的性能差异原因主要出现在 cache line 上：</p><blockquote><p>Optimistic locking avoids atomic writes and its cache line stays in shared mode. Pessimistic locks must always modify the cache line and thus their performance is bound by cache-coherency latencies</p></blockquote><p>Optimistic locking 特别适合用在经常读的热点数据上，比如 B Tree 的 root 节点。使用时需要注意几个问题：</p><ol><li>Optimistic locking 在碰到写的时候会重试正在进行的读操作，需要确保重试时不会发生异常。比如读取某个 B Tree 节点，可能别的线程触发了 B Tree Merge 操作导致当前要读的节点被删除了，需要确保重试时访问这个节点不会出现访问 null pointer 的情况。LeanStore 和作者提到的 Adaptive Radix Tree 可以通过 Epoch 机制来确保不会发生这个问题</li><li>注意上面伪代码传入的是一个 readCallBack，这个 callback 每次重试都会被调用，如果在里面更新一些值，比如求 count，那么可能就会因为重试得到错误结果。最安全的做法是仅通过这个 callback 获取值后就 buffer 住，等整个读操作返回后才用读到的值去做下一步的计算</li><li>当很多线程并发写时，optimistic lock 很容易被饿死，所以就像伪代码描述的那样，在经历了最大重试次数后需要能够 fallback 到 pessimistic lock</li></ol><h3 id=speculative-locking-htm>Speculative Locking (HTM)<a hidden class=anchor aria-hidden=true href=#speculative-locking-htm>#</a></h3><p>Speculative locking 是 Intel 在硬件上支持的一种 optimistic locking。即使是多个写线程也可以同时持有这个 lock，只要它们没有出现写冲突。这个 lock 有很多限制：</p><blockquote><p>All conflicts are detected on L1-cache line granularity (usually 64 bytes) and the addresses of the joint read/write-set must fit into L1 cache. Additionally, the critical section should be short to avoid interrupts or context switches and must avoid certain system calls.</p></blockquote><p>Speculative locking 这种基于硬件的 lock 最主要的缺点就是它和硬件绑定，不够通用。只有比较新的 Intel 和 ARM 处理器才支持类似的功能。考虑到上面提到的使用限制，以及一些处理器可能没有这个功能，使用者通常都需要实现一个 fallback 到传统 lock 的机制来应对这些问题。</p><h3 id=hybrid-locking>Hybrid Locking<a hidden class=anchor aria-hidden=true href=#hybrid-locking>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304062216400.png alt=Table1:Qualitativeoverview></p><p>重读少写的场景可以使用 optimistic locking 获取最好的性能，读写混合的场景就需要用到 pessimistic locking 了。从上面的表格来看，不管是 write-heavy 还是 write-only 场景，使用 shared lock 都是一个更好的 pessimistic locking 选择。</p><p>要在不同场景都获得最好的性能，就需要根据上下文对同一个数据上不同的锁。比如 B Tree，访问 read-contended 上层节点就可以使用 optimistic locking，而访问叶子结点去 scan 上面的数据时就最好使用 pessimistic locking 避免代价高昂的冲突重试。</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304062235593.png alt="Figure 2: Hybrid-Lock"></p><p>为了达到这个目标，作者设计了一个 Hybrid-Lock。如上图所示，这个 Hybrid-Lock 内部同时包含 RWMutex 和 atomic&lt;uint64_t> 分别用来做 Pessimistic Lock 和 Optimistic Lock。</p><p>理论上也可以把这个 RWMutex 和 version 通过一个 64 位整数来实现。把他们分开后，实现起来简单直接不容易出问题，为 Hybrid-Lock 实现各种 RWMutex 已有的接口也比较简单，比如下图 HybridLock 的 lockShared()、unlockShared()、lockExclusive()、unlockExclusive() 就直接直接使用了内部的 RWMutex()。</p><p>还是以 B Tree 为例考虑这么一种情况：一个读线程通过调用 tryReadOptimistically() 来访问某个中间节点，tryReadOptimistically() 刚开始执行时没有任何其他线程读写这个节点，该函数顺利执行到了 readCallback()，但在 readCallback() 执行中另一个线程通过 lockExcluseive() 对这个节点上锁，修改这个节点的内容，最后通过 unlockExclusive() 释放锁。如果 unlockExclusive() 先释放了 RWMutex 的 lock 而没有完成 version +1，那么读线程因为是先检查 lock 再检查 version 就可能读到错误的值。</p><p>需要特别注意 unlockExclusive() 里面两个操作的顺序。因为 tryReadOptimistically() 在执行完 readCallBack 后先检查 RWMutex 再检查 version，unlockExclusive() 里面就需要先 version +1 再释放 RWMutex，确保读操作一定能够检测到这个读写冲突并重试。在 Intel 平台上也可以使用 <code>CMPXCHG16B</code> 指令来同时更新 version 和释放锁。</p><p>readOptimisticIfPossible() 和一开始在 Optimistic Locking 中看到的伪代码工作机制稍微有点区别。它会在 tryReadOptimistically() 失败后直接从回退到 pessimistic locking 模式，使用 lockShared() 和 unLockShared() 完成这次读操作。</p><p>Hybrid-Lock 再加上后面提出的 ParkingLot 的 lock contention 处理策略就是目前 Umbra 中使用的锁实现，替代了之前提到的 Versioned Latch 方案。下面就是 Hybrid Lock 的伪代码实现。</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304052230691.png alt="Listing 2: Hybrid Locking"></p><h2 id=contention-handling>Contention Handling<a hidden class=anchor aria-hidden=true href=#contention-handling>#</a></h2><p>锁冲突是很难避免的，比如在 write-heavy 场景所有线程都使用 pessimistic locking 时，Hybrid Lock 中的 RWMutex 上就可能发生锁冲突。如何高效处理锁冲突，并且在线程等锁期间能够识别到 query 被 cancel 及时停止 query 执行呢？</p><p>这个章节作者分析了 Hybrid Lock 可能的 RWMutex 实现，最终采用了 Parking Lot 的方案。</p><h3 id=busy-waitingspinning>Busy-Waiting/Spinning<a hidden class=anchor aria-hidden=true href=#busy-waitingspinning>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304061335157.png alt="Figure 3: False-sharing"></p><p>Spinning 是一种常见的处理方式，和 <a href=https://en.wikipedia.org/wiki/Spinlock>Spinlock</a> 一样。Spinning 的一些缺点：</p><ol><li>Spinning can lead to priority inversion, as spinning threads seem very busy to a scheduler they might receive higher priority than a thread that does useful work. Especially in the case of over-subscription, this can cause critical problems</li><li>Heavy spinning wastes resources and energy [6] and increases cache pollution, which is caused by additional bus traffic. 作者通过 cache line 的例子详细解释了这个问题：Following the MESI-protocol, every atomic write needs to invalidate all existing copies in other cores. Ideally, a core owns a cache line exclusively and does not need to send any invalidation messages. However, if other threads are spinning on the same lock, they constantly request this cache line, causing contention. The negative effects are worst when the waiting thread does write-for-ownership cycles, as those cause expensive invalida- tion messages. For this reason, a waiting thread should use the test-test-and-set pattern and only do the write-for-ownership cycle when it sees that the lock is available. In other words, it only reads the lock state in the busy loop to keep the lock’s cache line in shared mode.</li><li>和 lock 处于同一 cache line 的数据也会受到影响，也就是 false-sharing 的问题。spinning can still lead to cache pollution when the protected data is on the same cache line as the lock itself (cf. Figure 3). By spinning on the lock the waiting thread 𝑇𝑤𝑎𝑖𝑡 constantly loads the cache line in shared mode. Whenever the lock owning 𝑇h𝑎𝑠𝐿𝑜𝑐𝑘 updates the protected data, it must invalidate 𝑇𝑤𝑎𝑖𝑡 ’s copy of the cache line. Having to send these invalidation messages, slows down 𝑇h𝑎𝑠𝐿𝑜𝑐𝑘 and increases the time spent in the critical section.</li></ol><p>虽然现有方案通过 backoff 策略可以缓解 spinlock 的这些问题，但这些策略本身也不是很完美：</p><blockquote><p>there exist several backoff strate- gies that add pause instructions to put the CPU into a lower power state, or call sched_yield to encourage the scheduler to switch to another thread. However, since the scheduler cannot guess when the thread wants to proceed, yielding is generally not recommended as its behavior is largely unpredictable [31].</p></blockquote><h3 id=local-spinning-using-queuing>Local Spinning using Queuing<a hidden class=anchor aria-hidden=true href=#local-spinning-using-queuing>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304061340149.png alt="Figure 4: Queuing lock"></p><p>spinning 带来的 cache contention 问题在 NUMA 架构上会更严重。为了解决这个问题，一些 spinlock 的实现只 spin 这个 lock 的 thread-local 副本，比如 MCS-lock 或者 Krieger et al. 在 [13, 25] 提出的 read-write mutex。</p><p>这种 lock 的工作方式：</p><blockquote><p>When acquiring a lock, every thread creates a thread-local instance of the lock structure including its lock state and a next pointer to build a linked list of waiting threads.3 Then, it exchanges the next pointer of the global lock, making it point to its own local instance. If the previous next entry was nil, the lock acquisition was successful. Otherwise, if the entry already pointed to another instance, the thread enqueues itself in the wait- ing list by updating the next-pointer of the found instance (current tail) to itself.</p></blockquote><h3 id=ticket-spinlock>Ticket Spinlock<a hidden class=anchor aria-hidden=true href=#ticket-spinlock>#</a></h3><p>ticket spinlock 是另一种 spinlock，guarantees fairness without using queues。</p><p>它的工作方式：</p><blockquote><p>maintaining two counters: next-ticket and now-serving. A thread gets a ticket using an atomic fetch_and_add and waits until its ticket number matches that of now-serving.</p></blockquote><p>除了能够保证 fairness 以外，它还有其他优点：</p><blockquote><p>this also enables more precise backoff in case of contention by estimating the wait time. The wait time can be estimated by multiplying the position in the queue and the expected time spent in the critical section. Mellor-Crummey and Scott argue that it is best to use the minimal possible time for the critical section, as overshooting in backoff will delay all other threads in line due to the FIFO nature</p></blockquote><h3 id=kernel-supported-parkinglot>Kernel-Supported ParkingLot<a hidden class=anchor aria-hidden=true href=#kernel-supported-parkinglot>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304061355223.png alt="Figure 5: Parking Lot"></p><p>上面提到的 spinlock 始终存在 over-subscription 或者 waste of energy 的问题。因此很多库的锁实现（比如 pthread mutex）都基于 Linux 内核提供的 kernel-level locking 来 suspend 当前这个线程直到拿到这个 lock 为止。</p><p>但内核上的系统调用开销是很高的，所以也有一些自适应的锁实现，仅锁冲突的时候才调用 kernel 阻塞当前线程，比如 Linux 提供的 futex。</p><p>基于 futex 的思路，WebKit 提出了一个叫 Parking Lot 的自适应锁。也是 Umbra 目前正在使用的锁实现。Parking Lot 用一个全局哈希表来存储 lock 到 wait queue 的映射关系。和 Linux futex 不一样，这种实现方式更加通用，可移植性强，不依赖其非标准的或者特定平台的系统调用。它也更加灵活，比如在 Parking 的时候可以执行某个 callback 函数。Umbra 利用这个特性在锁等待时检查查询是否被取消，Page 是否已经被缓存替换等。</p><p>上图描述了 Umbra 中实现的 Parking Lot 锁。当线程获取锁后会将 lock bit (L) 设置为 1。当另一个线程再次获取锁时，它会在 parking lot 中等待。此时它会把锁的 wait bit (W) 设置为 1 表示有人正在等锁，然后使用这个锁的地址在哈希表中找到该锁对应的 parking space。如果仍旧满足用户自定义的 wait condition，该线程开始等待这个 condition variable。当第 1 个线程释放锁后，它发现 wait bit (W) 为 1 知道有其他线程正在等锁，它会找到这个锁对应的 parking space，将所有等待的线程都唤醒。为了避免 parking space 的 data race 问题，每个 parking space 都有一个 mutex 来保护。</p><p>关于 lock bit 和 wait bit，作者在论文的 2.3 小结介绍完 Hybrid Lock 后有个补充说明，放到这里我们了解到 parking lot 后就比较容易理解了：</p><ul><li>wait bit：encode 在 Hybrid-Lock 的 RWMutex 上，用来表示有其他线程等锁</li><li>lock bit：encode 在 Hybrid-Lock 的 version 上，检测到有锁后当前线程需要调用下面伪代码中的 <code>park()</code> 函数进入 parking 状态，直到被 condition variable 唤醒。</li></ul><p>Parking lot 本质上就是个固定 512 槽位的哈希表，因为冲突的锁数量最多不会超过使用的线程数，所以 512 个槽位就足够用了。采用拉链法解决哈希冲突。</p><p>当执行用户 query 的线程在 parking space 中等待时，每 10ms 会被唤醒检查当前 query 是否被取消了，以便停止等待及时结束当前 query 的执行。</p><p>下面是 parking lot 的伪代码。虽然非常简单直接，但使用需要特别小心。需要确保造成线程等待的信息不会丢失导致线程无限期的等待下去，任何修改这些数据的线程都需要检查锁的 wait bit，在必要时唤醒所有等待的线程。作者这里没举例子，我能想到的一个场景是 B Tree Node 被缓存替换的场景。</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304061357401.png alt="Listing 3: Parking Lot Implementation"></p><p>论文到这（第 5 页）核心内容就结束了，后面花了大量的篇幅介绍和展示作者的测试结果。</p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><h3 id=tpc-c-and-tpc-h>TPC-C and TPC-H<a hidden class=anchor aria-hidden=true href=#tpc-c-and-tpc-h>#</a></h3><h3 id=lock-granularity>Lock Granularity<a hidden class=anchor aria-hidden=true href=#lock-granularity>#</a></h3><h3 id=space-consumption>Space Consumption<a hidden class=anchor aria-hidden=true href=#space-consumption>#</a></h3><h3 id=efficiency-of-lock-acquisition>Efficiency of Lock Acquisition<a hidden class=anchor aria-hidden=true href=#efficiency-of-lock-acquisition>#</a></h3><h3 id=contention-handling-strategies>Contention Handling Strategies<a hidden class=anchor aria-hidden=true href=#contention-handling-strategies>#</a></h3><h2 id=related-work>RELATED WORK<a hidden class=anchor aria-hidden=true href=#related-work>#</a></h2><h2 id=conclusion>CONCLUSION<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2></div><footer class=post-footer><nav class=paginav><a class=prev href=https://zz-jason.github.io/posts/vldb-2022-memory-opotimized-mvcc/><span class=title>« Prev Page</span><br><span>[VLDB 2022] Memory-Optimized Multi-Version Concurrency Control for Disk-Based Database Systems</span></a>
<a class=next href=https://zz-jason.github.io/posts/icde-2018-leanstore/><span class=title>Next Page »</span><br><span>[ICDE 2018] LeanStore: In-Memory Data Management Beyond Main Memory</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Latches for Database Systems on twitter" href="https://twitter.com/intent/tweet/?text=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Latches%20for%20Database%20Systems&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-latches%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-253.927 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Latches for Database Systems on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-latches%2f&title=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Latches%20for%20Database%20Systems&summary=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Latches%20for%20Database%20Systems&source=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-latches%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0v-129.439c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02v-126.056c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768h75.024zm-307.552-334.556c-25.674.0-42.448 16.879-42.448 39.002.0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Latches for Database Systems on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-latches%2f&title=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Latches%20for%20Database%20Systems"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zm-119.474 108.193c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zm-160.386-29.702c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Latches for Database Systems on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-latches%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978v-192.915h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Latches for Database Systems on whatsapp" href="https://api.whatsapp.com/send?text=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Latches%20for%20Database%20Systems%20-%20https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-latches%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512h-386.892c-34.524.0-62.554-28.03-62.554-62.554v-386.892c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23-13.314-11.876-22.304-26.542-24.916-31.026s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Latches for Database Systems on telegram" href="https://telegram.me/share/url?text=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Latches%20for%20Database%20Systems&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-latches%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47A3.38 3.38.0 0126.49 29.86zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=zz-jason/zz-jason.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://zz-jason.github.io/>Jian Zhang</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.27cd435cc9ed6abb4b496581b151804f79f366c412620272bb94e2f5f598ebcc.js integrity="sha256-J81DXMntartLSWWBsVGAT3nzZsQSYgJyu5Ti9fWY68w=" onload=hljs.initHighlightingOnLoad();></script><script>window.onload=function(){if(localStorage.getItem("menu-scroll-position")){document.getElementById('menu').scrollLeft=localStorage.getItem("menu-scroll-position");}}
document.querySelectorAll('a[href^="#"]').forEach(anchor=>{anchor.addEventListener("click",function(e){e.preventDefault();var id=this.getAttribute("href").substr(1);if(!window.matchMedia('(prefers-reduced-motion: reduce)').matches){document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({behavior:"smooth"});}else{document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();}
if(id==="top"){history.replaceState(null,null," ");}else{history.pushState(null,null,`#${id}`);}});});var mybutton=document.getElementById("top-link");window.onscroll=function(){if(document.body.scrollTop>800||document.documentElement.scrollTop>800){mybutton.style.visibility="visible";mybutton.style.opacity="1";}else{mybutton.style.visibility="hidden";mybutton.style.opacity="0";}};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById('menu').scrollLeft);}</script></body></html>