<!doctype html><html lang=en dir=ltr class=scroll-smooth data-default-appearance=dark data-auto-appearance=true><head><meta charset=utf-8><meta http-equiv=content-language content="en"><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="ie=edge"><title>[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I/O for High-Performance Storage Engines &#183; Jian Zhang</title><meta name=title content="[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I/O for High-Performance Storage Engines &#183; Jian Zhang"><meta name=description content="My personal blog"><link rel=canonical href=https://zz-jason.github.io/posts/vldb-2023-high-perf-nvme-io/><link type=text/css rel=stylesheet href=/css/main.bundle.min.41a504aed144b348a0001b6ff29d5b65569e83147d6ebb893db567d1f44b1b5c592e6639ff2e3d15baf6b00e0ccc9b7f1665346d3a08480aabe11083f276b25d.css integrity="sha512-QaUErtFEs0igABtv8p1bZVaegxR9bruJPbVn0fRLG1xZLmY5/y49Fbr2sA4MzJt/FmU0bToISAqr4RCD8nayXQ=="><script type=text/javascript src=/js/appearance.min.516a16745bea5a9bd011138d254cc0fd3973cd55ce6e15f3dec763e7c7c2c7448f8fe7b54cca811cb821b0c7e12cd161caace1dd794ac3d34d40937cbcc9ee12.js integrity="sha512-UWoWdFvqWpvQERONJUzA/TlzzVXObhXz3sdj58fCx0SPj+e1TMqBHLghsMfhLNFhyqzh3XlKw9NNQJN8vMnuEg=="></script>
<script src=/js/zoom.min.js></script>
<link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/site.webmanifest><meta property="og:title" content="[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I/O for High-Performance Storage Engines"><meta property="og:description" content="Manang, Nepal, 2024"><meta property="og:type" content="article"><meta property="og:url" content="https://zz-jason.github.io/posts/vldb-2023-high-perf-nvme-io/"><meta property="og:image" content="https://zz-jason.github.io/posts/vldb-2023-high-perf-nvme-io/featured.jpg"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-28T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-28T00:00:00+00:00"><meta property="og:site_name" content="Jian Zhang"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://zz-jason.github.io/posts/vldb-2023-high-perf-nvme-io/featured.jpg"><meta name=twitter:title content="[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I/O for High-Performance Storage Engines"><meta name=twitter:description content="Manang, Nepal, 2024"><script type=application/ld+json>[{"@context":"https://schema.org","@type":"Article","articleSection":"Posts","name":"[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I\/O for High-Performance Storage Engines","headline":"[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I\/O for High-Performance Storage Engines","abstract":"Manang, Nepal, 2024","inLanguage":"en","url":"https:\/\/zz-jason.github.io\/posts\/vldb-2023-high-perf-nvme-io\/","author":{"@type":"Person","name":"Jian Zhang"},"copyrightYear":"2024","dateCreated":"2024-05-28T00:00:00\u002b00:00","datePublished":"2024-05-28T00:00:00\u002b00:00","dateModified":"2024-05-28T00:00:00\u002b00:00","mainEntityOfPage":"true","wordCount":"1086"}]</script><meta name=author content="Jian Zhang"><link href=zjsariel@gmail.com rel=me><link href=https://github.com/zz-jason rel=me><link href=https://linkedin.com/in/zhangjian1012 rel=me><link href=https://twitter.com/zhangjian1012 rel=me><script src=/lib/jquery/jquery.slim.min.js integrity></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-1KC0CPJVWV"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1KC0CPJVWV")</script><meta name=theme-color></head><body class="flex flex-col h-screen px-6 m-auto text-lg leading-7 max-w-7xl bg-neutral text-neutral-900 dark:bg-neutral-800 dark:text-neutral sm:px-14 md:px-24 lg:px-32"><div id=the-top class="absolute flex self-center"><a class="px-3 py-1 text-sm -translate-y-8 rounded-b-lg bg-primary-200 focus:translate-y-0 dark:bg-neutral-600" href=#main-content><span class="font-bold text-primary-600 ltr:pr-2 rtl:pl-2 dark:text-primary-400">&darr;</span>Skip to main content</a></div><div style=padding-left:0;padding-right:0;padding-top:2px;padding-bottom:3px class="main-menu flex items-center justify-between px-4 py-6 sm:px-6 md:justify-start space-x-3"><div class="flex flex-1 items-center justify-between"><nav class="flex space-x-3"><a href=/ class="text-base font-medium text-gray-500 hover:text-gray-900">Jian Zhang</a></nav><nav class="hidden md:flex items-center space-x-5 md:ml-12"><a href=/posts/ class="flex items-center"><p class="text-base font-medium text-gray-500 hover:text-gray-900" title=Posts>Achieves</p></a><a href=/categories/ class="flex items-center"><p class="text-base font-medium text-gray-500 hover:text-gray-900" title=Categories>Categories</p></a><a href=/about/ class="flex items-center"><p class="text-base font-medium text-gray-500 hover:text-gray-900" title="About Me">About</p></a></nav><div class="flex md:hidden items-center space-x-5 md:ml-12"><span></span></div></div><div class="-my-2 -mr-2 md:hidden"><label id=menu-button for=menu-controller class=block><input type=checkbox id=menu-controller class=hidden><div class="cursor-pointer hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M0 96C0 78.33 14.33 64 32 64H416c17.7.0 32 14.33 32 32 0 17.7-14.3 32-32 32H32C14.33 128 0 113.7.0 96zM0 256c0-17.7 14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32H32c-17.67.0-32-14.3-32-32zM416 448H32c-17.67.0-32-14.3-32-32s14.33-32 32-32H416c17.7.0 32 14.3 32 32s-14.3 32-32 32z"/></svg></span></div><div id=menu-wrapper style=padding-top:5px class="fixed inset-0 z-30 invisible w-screen h-screen m-auto overflow-auto transition-opacity opacity-0 cursor-default bg-neutral-100/50 backdrop-blur-sm dark:bg-neutral-900/50"><ul class="flex space-y-2 mt-3 flex-col items-end w-full px-6 py-6 mx-auto overflow-visible list-none ltr:text-right rtl:text-left max-w-7xl"><li><span class="cursor-pointer inline-block align-text-bottom hover:text-primary-600 dark:hover:text-primary-400"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 320 512"><path fill="currentcolor" d="M310.6 361.4c12.5 12.5 12.5 32.75.0 45.25C304.4 412.9 296.2 416 288 416s-16.38-3.125-22.62-9.375L160 301.3 54.63 406.6C48.38 412.9 40.19 416 32 416S15.63 412.9 9.375 406.6c-12.5-12.5-12.5-32.75.0-45.25l105.4-105.4L9.375 150.6c-12.5-12.5-12.5-32.75.0-45.25s32.75-12.5 45.25.0L160 210.8l105.4-105.4c12.5-12.5 32.75-12.5 45.25.0s12.5 32.75.0 45.25l-105.4 105.4L310.6 361.4z"/></svg></span></span></li><li class=mt-1><a href=/posts/ class="flex items-center"><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title=Posts>Achieves</p></a></li><li class=mt-1><a href=/categories/ class="flex items-center"><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title=Categories>Categories</p></a></li><li class=mt-1><a href=/about/ class="flex items-center"><p class="text-bg font-bg text-gray-500 hover:text-gray-900" title="About Me">About</p></a></li></ul></div></label></div></div><script>(function(){var e=$(".main-menu"),t=window.location.pathname;e.find('a[href="'+t+'"]').each(function(e,t){$(t).children("p").addClass("active")})})()</script><div class="relative flex flex-col grow"><main id=main-content class=grow><article><div class="fixed inset-x-0 top-0 h-[800px] single_hero_background nozoom" style=background-image:url(/posts/vldb-2023-high-perf-nvme-io/featured_hu8bf0bac915afc22fbfc054a989bdd0c6_9322291_1200x0_resize_q75_box.jpg)><div class="absolute inset-0 bg-gradient-to-t from-neutral dark:from-neutral-800 to-transparent mix-blend-normal"></div><div class="absolute inset-0 opacity-60 bg-gradient-to-t from-neutral dark:from-neutral-800 to-neutral-100 dark:to-neutral-800 mix-blend-normal"></div></div><div id=background-blur class="fixed opacity-0 inset-x-0 top-0 h-full single_hero_background nozoom backdrop-blur-2xl"></div><script>window.addEventListener("scroll",function(){var t=window.pageYOffset||document.documentElement.scrollTop||document.body.scrollTop||0,n=document.getElementById("background-blur");n.style.opacity=t/300})</script><header id=single_header class="mt-5 max-w-prose"><h1 class="mt-0 text-4xl font-extrabold text-neutral-900 dark:text-neutral">[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I/O for High-Performance Storage Engines</h1><div class="mt-1 mb-6 text-base text-neutral-500 dark:text-neutral-400 print:hidden"><div class="flex flex-row flex-wrap items-center"><time datetime="2024-05-28 00:00:00 +0000 UTC">28 May 2024</time><span class="px-2 text-primary-500">&#183;</span><span>1086 words</span><span class="px-2 text-primary-500">&#183;</span><span title="Reading time">6 mins</span></div><div class="flex flex-row flex-wrap items-center"><span style=margin-top:.5rem class=mr-2 onclick='window.open("/categories/paper-reading/","_self")'><span class=flex><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Paper Reading</span></span></span>
<span style=margin-top:.5rem class=mr-2 onclick='window.open("/categories/storage/","_self")'><span class=flex><span class="rounded-md border border-primary-400 px-1 py-[1px] text-xs font-normal text-primary-700 dark:border-primary-600 dark:text-primary-400">Storage</span></span></span></div></div></header><section class="flex flex-col max-w-full mt-0 prose dark:prose-invert lg:flex-row"><div class="order-first sm:max-w-prose lg:ml-auto px-0 lg:order-last lg:max-w-xs ltr:lg:pl-8 rtl:lg:pr-8"><div class="toc ltr:pl-5 rtl:pr-5 print:hidden lg:sticky lg:top-10"><details open class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 hidden lg:block"><summary class="block py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="min-w-[220px] py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#what-modern-nvme-storage-can-do>What Modern NVMe Storage Can Do</a><ul><li><a href=#drive-scalability>Drive Scalability</a></li><li><a href=#the-case-for-4kb-pages>The Case for 4KB Pages</a></li><li><a href=#ssd-parallelism>SSD Parallelism</a></li><li><a href=#io-interfaces>I/O Interfaces</a></li><li><a href=#a-tight-cpu-budget>A Tight CPU Budget</a></li><li><a href=#implications-for-high-performance-storage-engines>Implications for High-Performance Storage Engines</a></li></ul></li><li><a href=#how-to-exploit-nvme-storage>How to Exploit NVMe Storage</a><ul><li><a href=#design-overview-and-outline>Design Overview and Outline</a></li><li><a href=#dbms-managed-multitasking>DBMS-Managed Multitasking</a></li><li><a href=#background-work-through-system-tasks>Background Work Through System Tasks</a></li><li><a href=#managing-io>Managing I/O</a></li><li><a href=#cpu-optimizations-and-scalability>CPU Optimizations and Scalability</a></li></ul></li><li><a href=#evaluation>EVALUATION</a><ul><li><a href=#system-comparison>System Comparison</a></li><li><a href=#out-of-memory-scalability>Out-Of-Memory Scalability</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></details><details class="mt-0 overflow-hidden rounded-lg ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 lg:hidden"><summary class="py-1 text-lg font-semibold cursor-pointer bg-neutral-100 text-neutral-800 ltr:-ml-5 ltr:pl-5 rtl:-mr-5 rtl:pr-5 dark:bg-neutral-700 dark:text-neutral-100 lg:hidden">Table of Contents</summary><div class="py-2 border-dotted border-neutral-300 ltr:-ml-5 ltr:border-l ltr:pl-5 rtl:-mr-5 rtl:border-r rtl:pr-5 dark:border-neutral-600"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#what-modern-nvme-storage-can-do>What Modern NVMe Storage Can Do</a><ul><li><a href=#drive-scalability>Drive Scalability</a></li><li><a href=#the-case-for-4kb-pages>The Case for 4KB Pages</a></li><li><a href=#ssd-parallelism>SSD Parallelism</a></li><li><a href=#io-interfaces>I/O Interfaces</a></li><li><a href=#a-tight-cpu-budget>A Tight CPU Budget</a></li><li><a href=#implications-for-high-performance-storage-engines>Implications for High-Performance Storage Engines</a></li></ul></li><li><a href=#how-to-exploit-nvme-storage>How to Exploit NVMe Storage</a><ul><li><a href=#design-overview-and-outline>Design Overview and Outline</a></li><li><a href=#dbms-managed-multitasking>DBMS-Managed Multitasking</a></li><li><a href=#background-work-through-system-tasks>Background Work Through System Tasks</a></li><li><a href=#managing-io>Managing I/O</a></li><li><a href=#cpu-optimizations-and-scalability>CPU Optimizations and Scalability</a></li></ul></li><li><a href=#evaluation>EVALUATION</a><ul><li><a href=#system-comparison>System Comparison</a></li><li><a href=#out-of-memory-scalability>Out-Of-Memory Scalability</a></li></ul></li><li><a href=#summary>Summary</a></li></ul></nav></div></details><script>(function(){var t,e=$("#TableOfContents");if(e.length>0){t=$(window);function n(){var s,o=t.scrollTop(),i=$(".anchor"),n="";if(i.each(function(e,t){t=$(t),t.offset().top-$(window).height()/3<=o&&(n=t.attr("id"))}),s=e.find("a.active"),s.length==1&&s.eq(0).attr("href")=="#"+n)return!0;s.each(function(e,t){$(t).removeClass("active")}),e.find('a[href="#'+n+'"]').addClass("active"),e.find('a[href="#'+n+'"]').parentsUntil("#TableOfContents").each(function(e,t){$(t).children("a").parents("ul").show()})}t.on("scroll",n),$(document).ready(function(){n()})}})()</script></div></div><div class="min-w-0 min-h-0 max-w-prose"><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/featured_hu8bf0bac915afc22fbfc054a989bdd0c6_9322291_330x0_resize_q75_box.jpg 330w,
/posts/vldb-2023-high-perf-nvme-io/featured_hu8bf0bac915afc22fbfc054a989bdd0c6_9322291_660x0_resize_q75_box.jpg 660w,
/posts/vldb-2023-high-perf-nvme-io/featured_hu8bf0bac915afc22fbfc054a989bdd0c6_9322291_1024x0_resize_q75_box.jpg 1024w,
/posts/vldb-2023-high-perf-nvme-io/featured_hu8bf0bac915afc22fbfc054a989bdd0c6_9322291_1320x0_resize_q75_box.jpg 2x" src=/posts/vldb-2023-high-perf-nvme-io/featured_hu8bf0bac915afc22fbfc054a989bdd0c6_9322291_660x0_resize_q75_box.jpg alt=featured.jpg></figure></p><blockquote><p>Manang, Nepal, 2024</p></blockquote><div id=introduction class=anchor></div><h2 class="relative group">Introduction
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#introduction aria-label=Anchor>#</a></span></h2><p>闪存的性能越来越强，价格也越来越便宜：一块 PCIe 4.0 SSD 有超过 1M 的随机读 IOPS，7GB/s 的总带宽，而新一代 PCIe 5.0 SSD 拥有 12GB/s 的总带宽，快要接近内存了。价格方面，企业级 SSD 的价格大约 $200/TB，比内存便宜了 10-50 倍。作者相信，随着 Optane 的商业失败，闪存在数据存储方面会发挥越来越大的作用，成为 cost-efficient 的唯一选择。</p><p>如下表所示，假设总共有 $15000 的预算，一半用来配置 64 核 CPU、512GB 内存，剩下的预算可以用来加 2TB 内存，或者加 8 块 4TB 的 PCIe 4.0 NVMe SSD 以实现更大的存储容量：</p><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/20240520004101_hu1fd30e0240f44296045d179c791a4372_67796_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/20240520004101_hu1fd30e0240f44296045d179c791a4372_67796_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/20240520004101_hu1fd30e0240f44296045d179c791a4372_67796_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/20240520004101_hu1fd30e0240f44296045d179c791a4372_67796_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/20240520004101_hu1fd30e0240f44296045d179c791a4372_67796_660x0_resize_box_3.png alt></figure></p><p>虽然可以配置多块 SSD 实现更大存储容量，但现有存储引擎设计却并不能完全发挥它们的总 IOPS 和带宽。每块 SSD 的 4KB 随机读能达到 1.5M IOPS ，8 块 SSD 可以达到 12M 的总 IOPS。如下图所示，作者测试了 5 个系统的随机读性能，总数据量 100GB，buffer pool 10GB，即使表现最好的 LeanStore 距离理论上限也还有 3.5 倍的差距：</p><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/20240428081401_hu1c33bf55d1cbd3cf5c08820a45869e53_338364_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/20240428081401_hu1c33bf55d1cbd3cf5c08820a45869e53_338364_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/20240428081401_hu1c33bf55d1cbd3cf5c08820a45869e53_338364_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/20240428081401_hu1c33bf55d1cbd3cf5c08820a45869e53_338364_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/20240428081401_hu1c33bf55d1cbd3cf5c08820a45869e53_338364_660x0_resize_box_3.png alt></figure></p><p>对一组 NVMe SSD 来说，怎样才能发挥它们性能上限？应该使用什么 IO 库，pread/pwrite、libaio、io_uring 还是 SPDK？既要减少 IO 放大又要提高性能，应该使用多大的 page size？如何实现和管理几百万的 IOPS？应该使用什么样的 IO 线程模型，是专门的 IO 线程池还是在每个工作线程中进行 IO？</p><p>以上这些问题就是这篇论文希望解决的。作者讨论了 NVMe 闪存的硬件特点，针对这些特点重新设计 LeanStore 使其能够充分发挥多块 SSD 的总 IOPS，最大化系统的读写吞吐。</p><div id=what-modern-nvme-storage-can-do class=anchor></div><h2 class="relative group">What Modern NVMe Storage Can Do
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#what-modern-nvme-storage-can-do aria-label=Anchor>#</a></span></h2><p>作者通过各种 micro benchmark 揭示了 SSD 的硬件特性和如何充分发挥它的性能。所有实验都基于 64 核 AMD zen4 CPU + 8 块三星 PM1733 SSD。</p><div id=drive-scalability class=anchor></div><h3 class="relative group">Drive Scalability
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#drive-scalability aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/fig-2_hu824ad87e95fc63e242a22083e25d9f92_176783_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/fig-2_hu824ad87e95fc63e242a22083e25d9f92_176783_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/fig-2_hu824ad87e95fc63e242a22083e25d9f92_176783_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/fig-2_hu824ad87e95fc63e242a22083e25d9f92_176783_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/fig-2_hu824ad87e95fc63e242a22083e25d9f92_176783_660x0_resize_box_3.png alt></figure></p><p>上图是 4KB 随机读和读写混合的测试结果，揭示了不同读写比例的总 IOPS 上限。完全随机写的情况下 8 块 SSD 的总 IOPS 为 4.7M，90% 随机读的总 IOPS 提升到了 8.9M，完全随机读的总 IOPS 可以达到 12.5M。</p><div id=the-case-for-4kb-pages class=anchor></div><h3 class="relative group">The Case for 4KB Pages
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#the-case-for-4kb-pages aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/fig-3_hu50e0557eb24d21979ffab181fe521eef_184576_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/fig-3_hu50e0557eb24d21979ffab181fe521eef_184576_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/fig-3_hu50e0557eb24d21979ffab181fe521eef_184576_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/fig-3_hu50e0557eb24d21979ffab181fe521eef_184576_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/fig-3_hu50e0557eb24d21979ffab181fe521eef_184576_660x0_resize_box_3.png alt></figure></p><p>对数据库来说 page size 的选择很重要，许多数据库使用了比 4KB 更大的 page size，比如 PG、SQL Server 为 8KB，MySQL 为 16KB，WiredTiger 为 32KB。更大的 page size 对 in-memory 工作负载更有利（比如减少 B+ 树的高度），也能减少 buffer pool 的 page 数量降低缓存维护负担。但 page size 过大的会导致严重的 IO 放大，可以说 page size 的选择是各种 trade off 后的结果。</p><p>上图是不同 page size 的随机读性能测试结果，4KB 的 page size 能最大化 IOPS，最小化 IO 延迟。虽然在吞吐方面还不是最优，比如 16KB page size 才能最大化读带宽，但是从发挥磁盘 IOPS 提高存储引擎性能的角度看，4KB page size 是最优选择。</p><div id=ssd-parallelism class=anchor></div><h3 class="relative group">SSD Parallelism
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#ssd-parallelism aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/fig-4_huab2887da8dcdd6ff10a971166c067a27_120403_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/fig-4_huab2887da8dcdd6ff10a971166c067a27_120403_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/fig-4_huab2887da8dcdd6ff10a971166c067a27_120403_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/fig-4_huab2887da8dcdd6ff10a971166c067a27_120403_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/fig-4_huab2887da8dcdd6ff10a971166c067a27_120403_660x0_resize_box_3.png alt></figure></p><p>光调整 page size 到 4KB 还不够。SSD 是个内部高度并行化的设备，提供了多个可以同时读写的数据通道。它的随机读延迟在 100us 级别，一次一个 page 的同步 IO 只能获得 10K IOPS，也就是 40MB/s 的带宽，距离单盘 1.5M IOPS 的上限相去甚远。</p><p>上图测试了 8 块 SSD 在不同 IO depth 下的 IOPS。当 IO depth 为 3000 时才能达到极限的 12.5M IOPS。换言之，从一次一个 page 的同步 IO 视角来看，需要 3000 并发这样的同步 IO 才能耗尽所有 SSD 的 IOPS，而在单机数据库里实现和管理这 3000 并发是个非常大的挑战。</p><div id=io-interfaces class=anchor></div><h3 class="relative group">I/O Interfaces
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#io-interfaces aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/fig-5_hua280327c503d6473d70f52dae41ba728_291417_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/fig-5_hua280327c503d6473d70f52dae41ba728_291417_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/fig-5_hua280327c503d6473d70f52dae41ba728_291417_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/fig-5_hua280327c503d6473d70f52dae41ba728_291417_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/fig-5_hua280327c503d6473d70f52dae41ba728_291417_660x0_resize_box_3.png alt></figure></p><p>作者讨论了 4 个 Linux 上常用的 IO 库：POSIX pread/pwrite、libaio、io_uring 以及 SPDK。不管使用哪个库，最终都需要将用户的 IO 请求发给 SSD 的 submission queue，SSD 处理完后会将 completion event 发送到 completion queue，通知应用 IO 完成。</p><p>POSIX pread/pwrite 是一种同步接口，每次处理一个 IO 请求，IO 未完成会被阻塞，每次请求都会产生 context switch 和系统调用。</p><p>libaio 是一种异步接口，通过一次 io_submit() 系统调用提交多个 IO 请求，IO 处理不阻塞用户程序，通过 get_events() 获取 completion events 来判断之前提交的 IO 请求是否已经完成。libaio 降低了系统调用和 context switch，单线程即可同时处理多个 IO 请求。</p><p>io_uring 是 libaio 的继任者，也是一种异步接口。通过用户态和内核态共享的 submission/completion queue（图 C 蓝色部分）来提交和收割 IO 请求。用户通过 io_uring_enter() 提交请求，内核的处理过程和其他接口一样各个 layer 都要走一遍，直到最后把 IO 请求提交到 SSD 的 submission queue 中。io_uring 有个 SQPOLL 模式，开启后会在内核中启动后台线程 kernel-worker 拉取和处理用户 submission queue（图 C 蓝色部分）中的 IO 请求。SQPOLL 模式下不需要任何系统调用。io_uring 还有个 IOPOLL 模式用于通知用户 IO 处理完成。之前所有介绍的 IO 库都是通过硬件中断来通知 IO 完成的，在 io_uring 的 IOPOLL + SQPOLL 模式下，可以完全省去系统调用和硬件中断，理论上比 libaio 性能更好，开销更低。</p><p>SPDK 全称是 Intel Storage Performance Development Kit，SPDK NVMe driver 是 NVMe SSD 的用户态驱动，用户可以通过 SPDK bypass 内核直接和 SSD 的 submission/completion queue 交互，也不会有硬件中断，理论上拥有最好的性能和最低的开销。</p><div id=a-tight-cpu-budget class=anchor></div><h3 class="relative group">A Tight CPU Budget
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#a-tight-cpu-budget aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/fig-6_huf3f015a86866349c565a28bf1be1a922_186618_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/fig-6_huf3f015a86866349c565a28bf1be1a922_186618_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/fig-6_huf3f015a86866349c565a28bf1be1a922_186618_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/fig-6_huf3f015a86866349c565a28bf1be1a922_186618_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/fig-6_huf3f015a86866349c565a28bf1be1a922_186618_660x0_resize_box_3.png alt></figure></p><p>要打满 12M IOPS 对 CPU 的消耗也很高，按照作者使用的 AMD 2.5GHz 64 核 CPU 来算，平均每 13K（2.5G*64/12M）个时钟周期就需要完成一个 IO 请求。</p><p>上图测试了不同 IO 库实现相同 IOPS 时的 CPU 开销。一些结论：</p><ol><li>libaio 和普通 io_uring 在使用 32 线程后 IOPS 就上不去了，大多时间都消耗在了无法省去的内核处理上。</li><li>SQPOLL + IOPOLL 模式的 io_uring，禁掉一些内核功能比如文件系统，RAID，OS page cache 后性能会好很多，使用 32 线程可以打满 IOPS。</li><li>SPDK 的性能最好，CPU 开销最低，3 线程就可以打满 IOPS。</li></ol><p>这里基本就宣告只能使用 SPDK 了，因为即使 io_uring 的 SQPOLL + IOPOLL 模式也需要 32 线程才能耗尽 IOPS，而剩下的 CPU，也就是 6.5K 个时钟周期需要用来进行查询处理、索引遍历、并发控制、缓存替换、日志记录等，肯定是不够用的。</p><div id=implications-for-high-performance-storage-engines class=anchor></div><h3 class="relative group">Implications for High-Performance Storage Engines
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#implications-for-high-performance-storage-engines aria-label=Anchor>#</a></span></h3><p>LeanStore 虽然专门面向 NVMe SSD 优化，但还是不能充分发挥多块 SSD 的 IOPS，上面的分析也给出了差距来源和优化思路。</p><p>LeanStore 将工作线程和 page provider 分开。工作线程负责处理用户事务，当需要的 page 不在 buffer pool 时通过 pread 从磁盘读取。page provider 负责缓存替换，将 buffer pool 中不常使用的 page 写回磁盘，为工作线程提供空闲 buffer frame。</p><p>工作线程的 pread 导致每个 page IO 都有一次系统调用，需要同步等待内核返回结果。按之前的分析，这种模式要想打满 12M IOPS 需要上千个工作线程，而真有这么多工作线程时，又会有更多 CPU 用于 context switch 和线程调度。</p><div id=how-to-exploit-nvme-storage class=anchor></div><h2 class="relative group">How to Exploit NVMe Storage
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#how-to-exploit-nvme-storage aria-label=Anchor>#</a></span></h2><p>作者优化了 LeanStore，使其完全发挥所有 SSD 的总 IOPS，在只有 64 核 128 线程的情况下，使系统能处理上千并发的 IO request。</p><div id=design-overview-and-outline class=anchor></div><h3 class="relative group">Design Overview and Outline
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#design-overview-and-outline aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/fig-7_hu38353f33e07b9c3b5298c8dfffb7fa97_221420_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/fig-7_hu38353f33e07b9c3b5298c8dfffb7fa97_221420_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/fig-7_hu38353f33e07b9c3b5298c8dfffb7fa97_221420_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/fig-7_hu38353f33e07b9c3b5298c8dfffb7fa97_221420_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/fig-7_hu38353f33e07b9c3b5298c8dfffb7fa97_221420_660x0_resize_box_3.png alt></figure></p><p>上图是 IO request 视角的系统设计概览，优化后的 LeanStore 使用协程调度了上千并发的 IO request，同时设计了高效的缓存替换和异步脏页回写使系统可以提供足够的空闲 buffer frame 供前端工作线程使用，所有 SSD 的读写操作由 IO backend 通过 SPDK 完成。</p><div id=dbms-managed-multitasking class=anchor></div><h3 class="relative group">DBMS-Managed Multitasking
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#dbms-managed-multitasking aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/fig-8_hu8775307bbdb02fbd5fc5f54c2fd2082f_284638_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/fig-8_hu8775307bbdb02fbd5fc5f54c2fd2082f_284638_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/fig-8_hu8775307bbdb02fbd5fc5f54c2fd2082f_284638_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/fig-8_hu8775307bbdb02fbd5fc5f54c2fd2082f_284638_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/fig-8_hu8775307bbdb02fbd5fc5f54c2fd2082f_284638_660x0_resize_box_3.png alt></figure></p><p>如上图所示，系统在每个 CPU core 上启动一个工作线程，工作线程执行一批由 boost coroutine 实现的 user task，coroutine 调度由工作线程的 scheduler 负责，coroutine 调度开销远低于 context switch，只需 ~20 CPU 时钟周期。</p><p>coroutine 切换发生在 page fault（此时需要根据 page id 从磁盘读取对应的 page 上来），或者缺乏空闲 buffer frame（此时需要将内存中某些 page 写回磁盘以得到可用的空闲 buffer frame），或者锁等待（为了避免 coroutine 因为等锁而阻塞，作者修改了所有 latch 实现，使当前 coroutine 能够被切换走），或者 user task 结束。</p><p>系统采用非阻塞 IO。user task 发生 page fault 后会将 IO 请求提交到 IO backend，之后就暂停执行。scheduler 会继续调度执行下一个 user task。当重新调度到该 user task 时如果 IO 请求已经完成，则可继续恢复执行。</p><div id=background-work-through-system-tasks class=anchor></div><h3 class="relative group">Background Work Through System Tasks
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#background-work-through-system-tasks aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/20240522203325_hu74d09937fac3d2d773bc992440b6bf29_374399_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/20240522203325_hu74d09937fac3d2d773bc992440b6bf29_374399_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/20240522203325_hu74d09937fac3d2d773bc992440b6bf29_374399_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/20240522203325_hu74d09937fac3d2d773bc992440b6bf29_374399_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/20240522203325_hu74d09937fac3d2d773bc992440b6bf29_374399_660x0_resize_box_3.png alt></figure></p><p>老版本的 LeanStore 使用后台线程 page provider 进行缓存替换，要满足上百万 IOPS 就需要多个 page provider，但具体数量很难提前预知，同时也很难适应动态变化的工作负载。</p><p>新版本中，像 page provider 这样的后台任务和 user task 一样采用 boost coroutine 实现为 system task，和 user task 一样由 scheduler 调度执行。以 page eviction 为例，每次调度它时如果发现空闲 buffer frame 不足，则调度执行 page eviction task 以获取更多空闲 buffer frame。如果瓶颈在缓存替换，scheduler 就会将更多 CPU 时间调度给 page eviction 使系统有足够的空闲 buffer frame 供 user task 使用。</p><p>上图展示了一个 scheduler 调度 user task 1、2 和 system task 的例子。scheduler 按照 runTask、submitIO、eviction、pollIO 的顺序调度 user task 和 system task：</p><ul><li>① - ④：调度执行 user task 1，执行其中的 B+ Tree lookup，在遍历时遇到了 page fault，将对应的 page read request 提交给 IO backend，将 task 状态置为 waitIO，然后 suspend 当前 user task，由 scheduler 继续调度下个任务</li><li>⑤ ：接下来是 system task submitIO 和 eviction。先调用 IO backend 的 submit 接口把所有累积的 IO request 提交到对应的 SSD submission queue 中。然后执行 page eviction 进行缓存替换确保有足够的空闲 buffer frame</li><li>⑥：进入下一个调度循环</li><li>⑦：这次执行的 user task 是 user task 2，它没有遇到 page fault，也没有锁等待，所有执行过程都在 CPU 中完成，执行结束后，继续调度后面的 system task</li><li>⑧：在 pollIO 时发现上个 user task 1 的 IO request 已经执行结束，调用 callback 将其任务状态置为 IOdone</li><li>⑨：下轮调度开始，scheduler 重新调度到 user task 1，发现 IO 已经完成，resume 后从上次 suspend 的地方继续执行</li></ul><div id=managing-io class=anchor></div><h3 class="relative group">Managing I/O
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#managing-io aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/20240522232155_hubc269d836562803b0de2f1c2918f62ae_203349_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/20240522232155_hubc269d836562803b0de2f1c2918f62ae_203349_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/20240522232155_hubc269d836562803b0de2f1c2918f62ae_203349_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/20240522232155_hubc269d836562803b0de2f1c2918f62ae_203349_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/20240522232155_hubc269d836562803b0de2f1c2918f62ae_203349_660x0_resize_box_3.png alt></figure></p><p>IO backend 是一个 async IO 的封装，支持包括 libaio、io_uring 和 SPDK 在内的所有主流异步 IO 库，它也对所有 SSD 做了类似 RAID 0 的封装使其看起来像是一块 SSD。没用 Linux 的 RAID 0 一方面是为了更好的性能，另一方面是为了使用 SPDK。</p><p>上图展示了 3 种可能的 IO 线程模型：Dedicated IO threads、SSD Assignment 和 All-to-All。优化后的 LeanStore 采用了 All-to-All 模型。</p><p>Dedicated IO threads 模型和 io_uring 的 SQPOLL 模式类似，工作线程需要和 IO 线程交互，每个 IO request 都会有额外的消息传递开销。Dedicated IO threads 的问题在之前的实验中揭示过，一方面不好确定线程池大小，另一方面这些线程也不能完全发挥 SSD IOPS。</p><p>SSD Assignment 模型中，IO benchmark 时通常采用这种模型，它的 cache locality 最好，polling 调用最少。每个工作线程负责一个 SSD。但是数据库工作负载和 IO benchmark 不一样，在数据库负载中可能需要读写其他线程上的 SSD 的数据，带来了额外的线程同步开销。</p><p>All-to-All 模型中，每个工作线程都可以读写所有 SSD，工作线程为每个 SSD 都准备了独立的 submission/completion queue，不需要在工作线程间传递消息和线程同步，没有任何特殊角色的线程，只需要 1 个线程就可以把系统跑起来。</p><p>下图测试对比了 SSD Assignment 和 All-to-All 两种模型，二者性能几乎一致，由于 All-to-All 模型的各种优点，IO backend 采用了该线程模型。</p><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/20240522234922_hub4148a67ac758f3fbb315a2e7d01bebb_115635_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/20240522234922_hub4148a67ac758f3fbb315a2e7d01bebb_115635_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/20240522234922_hub4148a67ac758f3fbb315a2e7d01bebb_115635_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/20240522234922_hub4148a67ac758f3fbb315a2e7d01bebb_115635_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/20240522234922_hub4148a67ac758f3fbb315a2e7d01bebb_115635_660x0_resize_box_3.png alt></figure></p><div id=cpu-optimizations-and-scalability class=anchor></div><h3 class="relative group">CPU Optimizations and Scalability
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#cpu-optimizations-and-scalability aria-label=Anchor>#</a></span></h3><p>当所有 IO 优化实现后，作者发现系统的 CPU 成为新的瓶颈，IOPS 仍旧没有打满。这个小结提了几个 LeanStore 的计算优化。</p><p>在多核 scalability 方面锁竞争是最大的瓶颈。早期 LeanStore 采用了全局大锁保护所有正在进行的 IO 操作，但在多块高性能 SSD 的场景下，这把大锁很快成为了性能瓶颈。作者将 inflight IO 按照 page id 进行分区，每个分区使用各自的小锁，通过减小锁粒度减少了锁竞争，提升了 multi-core scalability。</p><p>另一个显著的开销是 findParent。LeanStore 采用了 pointer swizzling，在 page eviction 时需要在当前 page 的父节点中更新其状态，由于节点中没有存储 parent pointer，需要调用 findParent 从根节点开始遍历。当 IOPS 很高，频繁进行 page eviction 时，findParent 的 CPU 开销就凸显出来了。LeanStore 采用了 optimistic parent pointer 的优化，主要思路是在当前节点缓存 parent pointer，避免重复的 findParent 开销。</p><div id=evaluation class=anchor></div><h2 class="relative group">EVALUATION
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#evaluation aria-label=Anchor>#</a></span></h2><p>还是一开始的实验环境：AMD EPYC 7713 CPU（64 核 128 线程），512GB 内存，8 块三星 PM1733 SSD，每块 3.84TB，每块 SSD 有 1.5M 4KB 随机读 IOPS。在测试所有 SSD 时需要设置内核参数 amd_iommu=off 用以发挥最佳性能，每次测试前都擦盘清零。</p><p>测试中关闭了 wal，使用了最低可用的事务隔离级别，主要关注整系统的 IO 吞吐。所有系统都配置成 4KB page size，采用 DIRECT IO，8 byte key + 120 byte value。</p><div id=system-comparison class=anchor></div><h3 class="relative group">System Comparison
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#system-comparison aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/20240523005504_hu6406cc311505000ac92e9cd7394c3600_265049_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/20240523005504_hu6406cc311505000ac92e9cd7394c3600_265049_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/20240523005504_hu6406cc311505000ac92e9cd7394c3600_265049_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/20240523005504_hu6406cc311505000ac92e9cd7394c3600_265049_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/20240523005504_hu6406cc311505000ac92e9cd7394c3600_265049_660x0_resize_box_3.png alt></figure></p><p>上图是 TPC-C 和 random lookup 负载下的吞吐测试。TPC-C 1000 warehouse，大约 160GB，buffer pool 10GB，90% 的数据在磁盘上，LeanStore 采用 SPDK 作为 IO backend，使用 64 线程 TPS 1.07M。在 random lookup benchmark 中，key 均匀分布，WiredTiger、RocksDB 和 LeanStore 的 QPS 分别是 1.8M、2.8M 以及 13.2M。</p><div id=out-of-memory-scalability class=anchor></div><h3 class="relative group">Out-Of-Memory Scalability
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#out-of-memory-scalability aria-label=Anchor>#</a></span></h3><p><figure><img class="my-0 rounded-md" srcset="/posts/vldb-2023-high-perf-nvme-io/20240523005658_huadd4e90bba2e85c6331427ffde7bf93b_127392_330x0_resize_box_3.png 330w,
/posts/vldb-2023-high-perf-nvme-io/20240523005658_huadd4e90bba2e85c6331427ffde7bf93b_127392_660x0_resize_box_3.png 660w,
/posts/vldb-2023-high-perf-nvme-io/20240523005658_huadd4e90bba2e85c6331427ffde7bf93b_127392_1024x0_resize_box_3.png 1024w,
/posts/vldb-2023-high-perf-nvme-io/20240523005658_huadd4e90bba2e85c6331427ffde7bf93b_127392_1320x0_resize_box_3.png 2x" src=/posts/vldb-2023-high-perf-nvme-io/20240523005658_huadd4e90bba2e85c6331427ffde7bf93b_127392_660x0_resize_box_3.png alt></figure></p><p>buffer pool 固定在 400GB，不断增加 TPC-C 数据量， 测试 LeanStore 在 out-of-memory 工作负载下的性能表现。当数据完全在内存中时 64 线程可实现 2M 多 TPS，当 25K warehouse，总数据 10 倍于 buffer pool 时，TPC-C 仍旧有 1.1M 的吞吐，100K warehouse 总数据量 40 倍于 buffer pool 时，TPC-C 仍旧有 400K 的吞吐。</p><p>除了上面这些测试以外，作者在 &ldquo;Incremental Performance Improvements&rdquo; 中测试了各个策略对 IOPS 吞吐的影响，在 &ldquo;IO Interface&rdquo; 中测试了不同 IO 库的性能，在 &ldquo;CPU Usage&rdquo; 中测试了系统各个组件的 CPU 开销，在 &ldquo;Latency&rdquo; 中测试了使用 SPDK 时 TPC-C 和 random lookup 的 P99 延迟表现，这里不再展开。</p><div id=summary class=anchor></div><h2 class="relative group">Summary
<span class="absolute top-0 w-6 transition-opacity opacity-0 ltr:-left-6 rtl:-right-6 not-prose group-hover:opacity-100"><a class="group-hover:text-primary-300 dark:group-hover:text-neutral-700" style=text-decoration-line:none!important href=#summary aria-label=Anchor>#</a></span></h2><p>PCIe 4.0 NVMe SSD 有很高的 4KB 随机读写 IOPS，作者通过一系列 IO benchmark 揭示了 SSD 的硬件特性以及如何发挥多块 SSD 的总 IOPS，使得这些 SSD 不仅能用来提升系统容量，也能用来提升系统吞吐，值得借鉴。</p></br></br></div><script>var oid="views_posts/vldb-2023-high-perf-nvme-io/index.md",oid_likes="likes_posts/vldb-2023-high-perf-nvme-io/index.md"</script><script type=text/javascript src=/js/page.min.0e49973b4ad0a382c7c6012d8bff8226316642daabc4f8a20477bd08674f3da6e2fa993bc20ad4f51e7c5bb68e6f913a207a7c4fe37ea0e7b806894afce0a64e.js integrity="sha512-DkmXO0rQo4LHxgEti/+CJjFmQtqrxPiiBHe9CGdPPabi+pk7wgrU9R58W7aOb5E6IHp8T+N+oOe4BolK/OCmTg=="></script></section><footer class="pt-8 max-w-prose print:hidden"><div class=flex><img class="!mt-0 !mb-0 h-24 w-24 rounded-full ltr:mr-4 rtl:ml-4" width=96 height=96 alt="Jian Zhang" src=/img/author_hu2e2e832894596e8fd219c1697ebbdb2b_59617_192x192_fill_q75_box_center.jpeg><div class=place-self-center><div class="text-[0.6rem] uppercase leading-3 text-neutral-500 dark:text-neutral-400">Author</div><div class="font-semibold leading-6 text-neutral-800 dark:text-neutral-300">Jian Zhang</div><div class="text-sm text-neutral-700 dark:text-neutral-400">We always overestimate the change that will occur in the next two years and underestimate the change that will occur in the next ten. Don’t let yourself be lulled into inaction</div><div class="text-2xl sm:text-lg"><div class="flex flex-wrap text-neutral-400 dark:text-neutral-500"><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=zjsariel@gmail.com target=_blank aria-label=Email rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M207.8 20.73c-93.45 18.32-168.7 93.66-187 187.1-27.64 140.9 68.65 266.2 199.1 285.1 19.01 2.888 36.17-12.26 36.17-31.49l1e-4-.6631c0-15.74-11.44-28.88-26.84-31.24-84.35-12.98-149.2-86.13-149.2-174.2.0-102.9 88.61-185.5 193.4-175.4 91.54 8.869 158.6 91.25 158.6 183.2v16.16c0 22.09-17.94 40.05-40 40.05s-40.01-17.96-40.01-40.05v-120.1c0-8.847-7.161-16.02-16.01-16.02l-31.98.0036c-7.299.0-13.2 4.992-15.12 11.68-24.85-12.15-54.24-16.38-86.06-5.106-38.75 13.73-68.12 48.91-73.72 89.64-9.483 69.01 43.81 128 110.9 128 26.44.0 50.43-9.544 69.59-24.88 24 31.3 65.23 48.69 109.4 37.49C465.2 369.3 496 324.1 495.1 277.2V256.3c0-149.2-133.9-265.632-287.3-235.57zM239.1 304.3c-26.47.0-48-21.56-48-48.05s21.53-48.05 48-48.05 48 21.56 48 48.05-20.6 48.05-48 48.05z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://github.com/zz-jason target=_blank aria-label=Github rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512"><path fill="currentcolor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6.0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6.0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3.0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1.0-6.2-.3-40.4-.3-61.4.0.0-70 15-84.7-29.8.0.0-11.4-29.1-27.8-36.6.0.0-22.9-15.7 1.6-15.4.0.0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5.0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9.0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4.0 33.7-.3 75.4-.3 83.6.0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6.0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9.0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://linkedin.com/in/zhangjian1012 target=_blank aria-label=Linkedin rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path fill="currentcolor" d="M416 32H31.9C14.3 32 0 46.5.0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6.0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3zM135.4 416H69V202.2h66.5V416zm-33.2-243c-21.3.0-38.5-17.3-38.5-38.5S80.9 96 102.2 96c21.2.0 38.5 17.3 38.5 38.5.0 21.3-17.2 38.5-38.5 38.5zm282.1 243h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6.0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2.0 79.7 44.3 79.7 101.9V416z"/></svg></span></span></a><a class="px-1 hover:text-primary-700 dark:hover:text-primary-400" href=https://twitter.com/zhangjian1012 target=_blank aria-label=Twitter rel="me noopener noreferrer"><span class="inline-block align-text-bottom"><span class="relative block icon"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path fill="currentcolor" d="M459.37 151.716c.325 4.548.325 9.097.325 13.645.0 138.72-105.583 298.558-298.558 298.558-59.452.0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055.0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421.0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391.0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04.0-57.828 46.782-104.934 104.934-104.934 30.213.0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg></span></span></a></div></div></div></div><div class=pt-8><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class="flex justify-between pt-3"><span><a class="flex group mr-3" href=/posts/sigmod-2024-amazon-memory-db/><span class="mr-3 text-neutral-700 group-hover:text-primary-600 ltr:inline rtl:hidden dark:text-neutral dark:group-hover:text-primary-400">&larr;</span>
<span class="ml-3 text-neutral-700 group-hover:text-primary-600 ltr:hidden rtl:inline dark:text-neutral dark:group-hover:text-primary-400">&rarr;</span>
<span class="flex flex-col"><span class="mt-[0.1rem] leading-6 group-hover:underline group-hover:decoration-primary-500">SIGMOD 2024 | Amazon MemoryDB: A Fast and Durable Memory-First Cloud Database</span>
<span class="mt-[0.1rem] text-xs text-neutral-500 dark:text-neutral-400"><time datetime="2024-05-04 00:00:00 +0000 UTC">4 May 2024</time></span></span></a></span>
<span></span></div></div><div class=pt-3><hr class="border-dotted border-neutral-300 dark:border-neutral-600"><div class=pt-3><script src=https://utteranc.es/client.js repo=zz-jason/zz-jason.github.io issue-term=pathname theme=github-dark crossorigin=anonymous async></script></div></div></footer></article><div id=top-scroller class="pointer-events-none absolute top-[110vh] bottom-0 w-12 ltr:right-0 rtl:left-0"><a href=#the-top class="pointer-events-auto sticky top-[calc(100vh-5.5rem)] flex h-12 w-12 mb-16 items-center justify-center rounded-full bg-neutral/50 text-xl text-neutral-700 backdrop-blur hover:text-primary-600 dark:bg-neutral-800/50 dark:text-neutral dark:hover:text-primary-400" aria-label="Scroll to top" title="Scroll to top">&uarr;</a></div></main><footer id=site-footer class="py-10 print:hidden"><div class="flex items-center justify-between"><p class="text-sm text-neutral-500 dark:text-neutral-400">@2023 Jian Zhang</p><p class="text-xs text-neutral-500 dark:text-neutral-400">Powered by <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://gohugo.io/ target=_blank rel="noopener noreferrer">Hugo</a> & <a class="hover:underline hover:decoration-primary-400 hover:text-primary-500" href=https://blowfish.page/ target=_blank rel="noopener noreferrer">Blowfish</a></p></div><script>mediumZoom(document.querySelectorAll("img:not(.nozoom)"),{margin:24,background:"rgba(0,0,0,0.5)",scrollOffset:0})</script><script type=text/javascript src=/js/process.min.35c1113bcc16c5a59bf031082f9e63822aa95280423881a7847a7ff33a16e6299ce6a840d9ef4e10d947e030a18f3f20359afb2ec0f35967484b9a9360ac3145.js integrity="sha512-NcERO8wWxaWb8DEIL55jgiqpUoBCOIGnhHp/8zoW5imc5qhA2e9OENlH4DChjz8gNZr7LsDzWWdIS5qTYKwxRQ=="></script></footer></div></body></html>