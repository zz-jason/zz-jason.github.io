<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines | Jian Zhang</title><meta name=keywords content><meta name=description content="简介 这篇论文提出了一种能够避免 long-running OLAP query 影响 OLTP 事务、在多核 CPU 上 scale、支持 out-of-memory workload 的 MVCC 实现机制。 长时间运行的 OLAP 查询会严重影响 OLTP 事务延迟。例如上图"><meta name=author content="Jian"><link rel=canonical href=https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-si/><link href=/assets/css/stylesheet.min.770d8a0aa405dcf691d11ff1ae1a3ef7f461eed00172c15620f8b0f1e5c77e8f.css integrity="sha256-dw2KCqQF3PaR0R/xrho+9/Rh7tABcsFWIPiw8eXHfo8=" rel="preload stylesheet" as=style><link rel=icon href=https://zz-jason.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zz-jason.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zz-jason.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zz-jason.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zz-jason.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.110.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-67872953-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines"><meta property="og:description" content="简介 这篇论文提出了一种能够避免 long-running OLAP query 影响 OLTP 事务、在多核 CPU 上 scale、支持 out-of-memory workload 的 MVCC 实现机制。 长时间运行的 OLAP 查询会严重影响 OLTP 事务延迟。例如上图"><meta property="og:type" content="article"><meta property="og:url" content="https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-si/"><meta property="article:published_time" content="2023-05-24T00:00:00+00:00"><meta property="article:modified_time" content="2023-05-24T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines"><meta name=twitter:description content="简介 这篇论文提出了一种能够避免 long-running OLAP query 影响 OLTP 事务、在多核 CPU 上 scale、支持 out-of-memory workload 的 MVCC 实现机制。 长时间运行的 OLAP 查询会严重影响 OLTP 事务延迟。例如上图"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zz-jason.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines","item":"https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-si/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines","name":"[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines","description":"简介 这篇论文提出了一种能够避免 long-running OLAP query 影响 OLTP 事务、在多核 CPU 上 scale、支持 out-of-memory workload 的 MVCC 实现机制。 长时间运行的 OLAP 查询会严重影响 OLTP 事务延迟。例如上图","keywords":[],"articleBody":"简介 这篇论文提出了一种能够避免 long-running OLAP query 影响 OLTP 事务、在多核 CPU 上 scale、支持 out-of-memory workload 的 MVCC 实现机制。\n长时间运行的 OLAP 查询会严重影响 OLTP 事务延迟。例如上图，在 TPC-C 开始 10 秒后执行一个 sleep 语句，之后 WiredTiger 和 PostgreSQL 的 TPC-C 的性能都出现了大幅下跌，下跌幅度取决于 GC 激进和精确程度。\n作者分析了 OLTP 性能下降的原因。长时间运行的 OLAP 查询会阻止 tombstone GC，从而导致 OLTP 事务执行 index range scan 时需要额外检查每个 index key 的 version chain，进行额外的 tombstone skipping， 影响事务性能。OLTP 事务执行过程中的 delete 和 update 会使这种情况变得更加糟糕。作者以 TPC-C 的 neworder 表为例，该表模拟了一个订单队列，各个事务会不断插入和删除订单，如果不能及时 GC tombstone，tombstone 也会越积越多。此外，对 index key 的更新也会遇到类似的问题，因为更新 index key 会先删除原来的 key 再插入新的 key。\n而另一方面，当 OLTP 事务频繁更新某些数据时，长时间运行的 OLAP 查询延迟也会受到影响。因为采用了 N2O 的 version 顺序，新版本在 version chain 的头部，OLAP 查询需要跳过 version chain 前面的新版本才能获取到它需要的老版本数据。\n有一个性质可以用来解决这个问题，那就是在任何时刻，每个 tuple 最多需要保留的版本数不会超过当前正在进行的事务总数。但没有任何 out-of-memory 系统实现了这种激进和精确的 GC 策略。\n作者还详细分析了内存数据库和传统关系型数据库各自 MVCC 系统的实现原理，感兴趣的朋友可以参考论文的 “2.4 SI Commit Protocols” 小节。下面我们重点来看作者在论文中提出的 MVCC 系统实现。\n系统设计概览 上图是作者提出的 MVCC 系统设计概览，总的来说可以分为如下几个方面：\nCommit Protocol：基于 Commit Log 实现了 LCB（Last Committed Before）接口，基于 LCB 接口实现了可见性检查，进而实现了 OSIC（Ordered Snapshot Instant Commit）。 Garbage Collection：基于 oldest_oltp 和 oldest_tx 这两个 watermark 避免了长时间运行的 OLAP 查询导致的版本堆积使得 OLTP 事务延迟增大的问题。通过 Tombstone Index 记录所有被 OLTP 事务删除的数据，通过 Graveyard Index 存储所有不被任何 OLTP 事务使用的 tombstone，避免了标记删除的 tombstone 增加 OLTP 事务延迟，和双 watermark 机制共同解决了频繁插入删除导致 tombstone 堆积，事务延迟增加的问题。 Version Storage：基于 off-row 的 Delta Index 和 in-row 的 FatTuple 这两种 version chain 存储格式之间的自适应转换实现了高效的数据读写和精确 GC。 Commit Protocol 一些系统假设：采用全局逻辑时钟，每个事务开始和提交时分别获取 start 和 commit ts。采用 first writer wins 的原则解决写写冲突。每个工作线程内的事务串行执行，多个工作线程之间的事务并发执行。\nOSIC 是 Ordered Snapshot Instant Commit 的简称，它实现了 SI 的事务隔离级别，通过 start ts 确定可见性，决定 snapshot，确定事务顺序等。和其他 out-of-memory 系统一样，事务 commit 时也不用再次修改 write set（比如将 write set 中的 start ts 修改为 commit ts）。\nLCB 和 Snapshot Cache LCB 是 Last Committed Before 的简称，LCB(w, ts) 表示工作线程 w 在 ts 这个时间戳之前最后一条已提交事务的 commit ts。要想根据 start ts 确定事务的 snapshot，只需要知道所有工作线程 w 上的 LCB(w, start ts) 即可。\n为了减少获取 LCB 的线程间同步开销，LCB 需要做到按需获取，且仅获取一次，理论上事务执行过程中最多获取 #W 次 LCB（#W 表示所有 worker 的数量）：\n仅在遇到其他 worker 写的数据版本（数据的版本信息中包含 worker id）时才去向对应的 worker 获取 LCB，避免获取无用的 LCB。 把获取到 LCB(wi, start ts) 缓存在 thread-local 的 snapshot cache 中，后面再遇到 worker wi 写的其他数据版本时就不再重复向该 worker 获取 LCB，而是直接读取 snapshot cache。 上图是根据 start ts、snapshot cache、LCB 进行可见性检查的伪代码，其中：\ntuple_ts_start 表示该 tuple 的版本号，tuple_w_i 表示写入该版本的 worker id。 isVisible() 里第 1 个 if 语句先检查这个数据版本是否是当前 worker 写入的，是的话那肯定可见 第 2 个 if 语句根据具体的事务隔离级别调整可见性检查使用的事务 start ts，如果是 RC 隔离级别，就更新当前事务的 start ts 为最新 ts，尽量读到所有 worker 上最新的 commit 第 3 个 if 语句接着查本地的 snapshot cache，也就是伪代码中的 sc。sc 存储了每个 worker 上最近一次 LCB(wi, start ts) 对应的 commit ts。如果该 tuple 的版本号比上次 LCB 查到的 commit ts 还小，该 tuple 一定对当前事务可见。 第 4 个 if 语句检查 snapshot cache 是否需要更新。缓存的内容实际是 tuple_w_i 这个 worker 在 tx_ts_start 这个时间戳对应的 last_commit_ts。如果缓存的 cache_ts_start 比 tx_ts_start 更小（不会发生更大的情况），这时候就需要更新 LCB 缓存 如果前面的检查都 fail 了，到这里我们已经拿到了准确的 LCB(tuple_w_i, tx_ts_start) 的结果并将其缓存在了 sc[tuple_w_i] 中，最后只需要根据准确的 last_commit_ts 做一次比较即可判断该版本是否可见。 Commit Log 为了计算每个工作线程 w 上的 LCB(w, ts)，需要为每个工作线程维护一个 Commit Log 队列，用来存储 start ts 和对应 commit ts 的映射关系，处理 LCB 请求时只需要查找 Commit Log 返回对应的 commit ts 即可。\nCommit Log 会被并发读写，因此也需要合适的线程间同步机制。每个 Commit Log 都有一把锁，事务 commit 时需要获得写锁，将 commit ts 和对应的 Commit Log 写入后才完成事务提交，释放锁，之后其他拥有更大 start ts 的事务就能看见提交后的最新版本了。\n对每个 worker 上的 Commit Log 队列来说，因为只需要为每个活跃事务保留一个 Commit Log，因此总共只需要保留 #W （worker 总数）条 Commit Log。每个 worker 开启新事务时都会检查 Commit Log 队列是否已满，如果队列已满，该 worker 需要收集所有活跃事务的 start ts，剔除那些活跃事务不可见的 Commit Log，限制 Commit Log 的总大小。\nGarbage Collection 为了解决因长时间运行的 OLAP 查询导致 MVCC 版本堆积而拖慢 OLTP 事务的问题，作者通过优化器将查询分为 OLAP 和 OLTP，并维护了 oldest_tx 和 oldest_oltp 这两个 watermark。它们分别表示最老的 OLAP 事务和最老的 OLTP 事务的 start ts。这样就可以对 newest_olap 到 oldest_oltp 之间的版本进行 Cooperative GC（每个 worker 回收自己创建的位于 LCB(w, oldest_tx or oldest_oltp) 之间的 MVCC 版本），从而避免长时间运行的 OLAP 事务对 OLTP 事务的影响。\n使用 start ts 的最高有效位来区分 OLTP 和 OLAP。事务结束后，start ts 将以 1/#W（#W 表示 worker 总数）的概率更新到全局共享数组中，从而可以通过分析该数组中的 OLTP 和 OLAP start ts 来获取这两个 watermark。\nTombstone 删除数据会产生 tombstone，而因为 long-running OLAP query 的存在，这些 tombstone 会在 main index 中一直累积，增加 OLTP 事务延迟：因为索引中没有数据的 MVCC 信息，要想知道一个数据是否删除了，需要先读索引再去 main index 拿到具体的 tuple，最后根据 tuple 上的 MVCC 信息判断数据是否已删除。\nGraveyard Index 因为被 tombstone 标记删除的数据只对那些 long-running OLAP query 可见，对那些 start ts 更大的 OLTP 事务不可见。因此作者引入了一个额外的 graveyard index 数据结构来存储这样的 tombstone，只要 tombstone 的 start ts 小于 oldest_oltp 就将其从 main index 移到 Graveyard Index。\nOLAP query 需要同时查询 main index 和 Graveyard Index 来获取所有可见的数据版本，而 OLTP query 只需要查询 main index 即可，牺牲少量 OLAP 查询性能保障了 OLTP 事务延迟不受 long-running OLAP query 的影响。\nTombstone Index 为了知道有哪些 tombstone 并及时对它们进行 GC，作者以去中心化的方式在每个 worker 上维护了一个 Tombstone Index。Tombstone Index 是一个 append-optimized B+ tree，key 由 start ts、command id 组成，value 为该 tombstone 的 tuple key。根据 oldest_tx 以及 oldest_oltp 这两个 watermark 确定 Tombstone Index 中的 tombstone 是应该直接删掉还是从 main index 转移到 graveyard index。\nVersion Storage 同时采用了 Delta Index 和 FatTuple 两种版本存储方式，默认采用 Delta Index。\nDelta Index 如上图所示，默认情况下所有老版本数据都存储在每个 worker 线程的 Delta Index 中，main index 只存储最新版本的 tuple 和其 version chain 的一些 metadata，这些 metadata 包括：\nWorkerID 和 TSstart：共同决定了该 tuple 最新版本的可见性 WorkerID、TSstart 和 CommandID 共同构成了 Delta Index 的 key，同时也指向了 version chain 中上一个更老版本的数据。CommandID 主要用来区分同一个事物的不同操作，在 Delta Index 中可以省略，使用（WorkerID、TSstart、IndexID、Key）作为 Delta Index 的 key。 #updates 和 since_oldest_tx：用来估算 version chain 的长度。因为精确的维护 version chain 的长度代价太大，作者采用了启发式的估算方式。#updates 表示在 oldest_tx 这个 watermark 不变的情况下该 tuple 最多更新了多少次，反应了受 long-running OLAP query 影响的情况下累积的版本数。since_oldest_tx 存储的是 oldest_tx 的低 16 位（2 字节）。每当事务更行时都会检查当前 oldest_tx 的低 16 位是否和 since_oldest_tx 相等，是的话就把 #update 加 1，否的话就重置 #update 计数器。当 #update 超过 worker 总数时就将其转成 FatTuple 的存储方式。 FatTuple 当 tuple 被频繁更新时会被自动转换为 FatTuple，把所有的数据版本 delta 都 inline 的存储在 main index 的 FatTuple 中。因为每个 FatTuple 只需要存储 #W 个数据版本，作者在 FatTuple 的基础上实现了 on-demand precise garbage collection（OPGC），OPGC 模式下会首先将 newest_olap 到 oldest_oltp 这个 dead zone 内的 delta GC 掉，如果还没有足够的槽位再去收集所有正在运行事务的 start ts，确定所有的 dead zone 后 GC 所有不需要的 delta。关于 precise garbage collection 可以参考《Long-lived Transactions Made Less Harmful》。\n另外，因为 GC 时只会扫描 Tombstone Index 和 Delta Index，为了保证 FatTuple 中的旧版本也能被 GC，在 page eviction 时需要将 FatTuple 转成 Delta Index。\nDurability and Recovery 因为所有事务操作都有 WAL，所以前面提到的数据结构比如 Commit Log、Graveyard Index、Tombstone Index、Delta Index 都没有 durability 要求。\n恢复也比较简单，在 analysis 阶段确定 winning transactions，在 redo 阶段 delete 操作直接将 main index 上的 tombstone 删掉。每个 worker 的 Commit Log 初始化为最后一个 winning transaction 的 commit ts。\n事务 abort 比较特殊。worker 的 Commit Log 中只存储了成功提交的事务，事务 abort 时需要将它所有的改动回滚（可能分散在 Delta Index、Tombstone Index 中），将它的 start ts 也回收，用于该 worker 的下一个事务的 start ts。\nEvaluation 从作者后面的实验来看，Graveyard Index 能够显著降低 long-running OLAP query 对 TPC-C 性能的影响：\n而 FatTuple 的设计也使得 OLAP scan 的性能不受影响： 后面作者还做了关于 “TPC-C + Scan: Scalability”、“Out-of-Memory Breakdown”、“Bulk Loading”、“Out-of-Memory Key/Value”、“Deterministic Execution Under Contention” 等实验，感兴趣的朋友可以详细阅读 “4 EVALUATION” 这一节，从实验结果来看整体效果不错。\n","wordCount":"3923","inLanguage":"en","datePublished":"2023-05-24T00:00:00Z","dateModified":"2023-05-24T00:00:00Z","author":{"@type":"Person","name":"Jian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-si/"},"publisher":{"@type":"Organization","name":"Jian Zhang","logo":{"@type":"ImageObject","url":"https://zz-jason.github.io/favicon.ico"}}}</script></head><body id=top><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://zz-jason.github.io/ accesskey=h title="Jian Zhang (Alt + H)">Jian Zhang</a>
<span class=logo-switches></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://zz-jason.github.io/ title=Home><span>Home</span></a></li><li><a href=https://zz-jason.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://zz-jason.github.io/posts/ title=Archives><span>Archives</span></a></li><li><a href=https://zz-jason.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines</h1><div class=post-meta>May 24, 2023&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Jian</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#%e7%ae%80%e4%bb%8b aria-label=简介>简介</a></li><li><a href=#%e7%b3%bb%e7%bb%9f%e8%ae%be%e8%ae%a1%e6%a6%82%e8%a7%88 aria-label=系统设计概览>系统设计概览</a></li><li><a href=#commit-protocol aria-label="Commit Protocol">Commit Protocol</a><ul><li><a href=#lcb-%e5%92%8c-snapshot-cache aria-label="LCB 和 Snapshot Cache">LCB 和 Snapshot Cache</a></li><li><a href=#commit-log aria-label="Commit Log">Commit Log</a></li></ul></li><li><a href=#garbage-collection aria-label="Garbage Collection">Garbage Collection</a><ul><li><a href=#tombstone aria-label=Tombstone>Tombstone</a></li><li><a href=#graveyard-index aria-label="Graveyard Index">Graveyard Index</a></li><li><a href=#tombstone-index aria-label="Tombstone Index">Tombstone Index</a></li></ul></li><li><a href=#version-storage aria-label="Version Storage">Version Storage</a><ul><li><a href=#delta-index aria-label="Delta Index">Delta Index</a></li><li><a href=#fattuple aria-label=FatTuple>FatTuple</a></li></ul></li><li><a href=#durability-and-recovery aria-label="Durability and Recovery">Durability and Recovery</a></li><li><a href=#evaluation aria-label=Evaluation>Evaluation</a></li></ul></div></details></div><div class=post-content><h2 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h2><p>这篇论文提出了一种能够避免 long-running OLAP query 影响 OLTP 事务、在多核 CPU 上 scale、支持 out-of-memory workload 的 MVCC 实现机制。</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304110017200.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304110017200.png></p><p>长时间运行的 OLAP 查询会严重影响 OLTP 事务延迟。例如上图，在 TPC-C 开始 10 秒后执行一个 sleep 语句，之后 WiredTiger 和 PostgreSQL 的 TPC-C 的性能都出现了大幅下跌，下跌幅度取决于 GC 激进和精确程度。</p><p>作者分析了 OLTP 性能下降的原因。长时间运行的 OLAP 查询会阻止 tombstone GC，从而导致 OLTP 事务执行 index range scan 时需要额外检查每个 index key 的 version chain，进行额外的 tombstone skipping， 影响事务性能。OLTP 事务执行过程中的 delete 和 update 会使这种情况变得更加糟糕。作者以 TPC-C 的 neworder 表为例，该表模拟了一个订单队列，各个事务会不断插入和删除订单，如果不能及时 GC tombstone，tombstone 也会越积越多。此外，对 index key 的更新也会遇到类似的问题，因为更新 index key 会先删除原来的 key 再插入新的 key。</p><p>而另一方面，当 OLTP 事务频繁更新某些数据时，长时间运行的 OLAP 查询延迟也会受到影响。因为采用了 N2O 的 version 顺序，新版本在 version chain 的头部，OLAP 查询需要跳过 version chain 前面的新版本才能获取到它需要的老版本数据。</p><p>有一个性质可以用来解决这个问题，那就是在任何时刻，每个 tuple 最多需要保留的版本数不会超过当前正在进行的事务总数。但没有任何 out-of-memory 系统实现了这种激进和精确的 GC 策略。</p><p>作者还详细分析了内存数据库和传统关系型数据库各自 MVCC 系统的实现原理，感兴趣的朋友可以参考论文的 &ldquo;2.4 SI Commit Protocols&rdquo; 小节。下面我们重点来看作者在论文中提出的 MVCC 系统实现。</p><h2 id=系统设计概览>系统设计概览<a hidden class=anchor aria-hidden=true href=#系统设计概览>#</a></h2><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304110954551.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304110954551.png></p><p>上图是作者提出的 MVCC 系统设计概览，总的来说可以分为如下几个方面：</p><ol><li><strong>Commit Protocol</strong>：基于 Commit Log 实现了 LCB（Last Committed Before）接口，基于 LCB 接口实现了可见性检查，进而实现了 OSIC（Ordered Snapshot Instant Commit）。</li><li><strong>Garbage Collection</strong>：基于 oldest_oltp 和 oldest_tx 这两个 watermark 避免了长时间运行的 OLAP 查询导致的版本堆积使得 OLTP 事务延迟增大的问题。通过 Tombstone Index 记录所有被 OLTP 事务删除的数据，通过 Graveyard Index 存储所有不被任何 OLTP 事务使用的 tombstone，避免了标记删除的 tombstone 增加 OLTP 事务延迟，和双 watermark 机制共同解决了频繁插入删除导致 tombstone 堆积，事务延迟增加的问题。</li><li><strong>Version Storage</strong>：基于 off-row 的 Delta Index 和 in-row 的 FatTuple 这两种 version chain 存储格式之间的自适应转换实现了高效的数据读写和精确 GC。</li></ol><h2 id=commit-protocol>Commit Protocol<a hidden class=anchor aria-hidden=true href=#commit-protocol>#</a></h2><p><strong>一些系统假设</strong>：采用全局逻辑时钟，每个事务开始和提交时分别获取 start 和 commit ts。采用 first writer wins 的原则解决写写冲突。每个工作线程内的事务串行执行，多个工作线程之间的事务并发执行。</p><p>OSIC 是 Ordered Snapshot Instant Commit 的简称，它实现了 SI 的事务隔离级别，通过 start ts 确定可见性，决定 snapshot，确定事务顺序等。和其他 out-of-memory 系统一样，事务 commit 时也不用再次修改 write set（比如将 write set 中的 start ts 修改为 commit ts）。</p><h3 id=lcb-和-snapshot-cache>LCB 和 Snapshot Cache<a hidden class=anchor aria-hidden=true href=#lcb-和-snapshot-cache>#</a></h3><p>LCB 是 Last Committed Before 的简称，LCB(w, ts) 表示工作线程 w 在 ts 这个时间戳之前最后一条已提交事务的 commit ts。要想根据 start ts 确定事务的 snapshot，只需要知道所有工作线程 w 上的 LCB(w, start ts) 即可。</p><p>为了减少获取 LCB 的线程间同步开销，LCB 需要做到按需获取，且仅获取一次，理论上事务执行过程中最多获取 #W 次 LCB（#W 表示所有 worker 的数量）：</p><ol><li>仅在遇到其他 worker 写的数据版本（数据的版本信息中包含 worker id）时才去向对应的 worker 获取 LCB，避免获取无用的 LCB。</li><li>把获取到 LCB(wi, start ts) 缓存在 thread-local 的 snapshot cache 中，后面再遇到 worker wi 写的其他数据版本时就不再重复向该 worker 获取 LCB，而是直接读取 snapshot cache。</li></ol><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212027428.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212027428.png>
上图是根据 start ts、snapshot cache、LCB 进行可见性检查的伪代码，其中：</p><ul><li>tuple_ts_start 表示该 tuple 的版本号，tuple_w_i 表示写入该版本的 worker id。</li></ul><ul><li>isVisible() 里第 1 个 if 语句先检查这个数据版本是否是当前 worker 写入的，是的话那肯定可见</li><li>第 2 个 if 语句根据具体的事务隔离级别调整可见性检查使用的事务 start ts，如果是 RC 隔离级别，就更新当前事务的 start ts 为最新 ts，尽量读到所有 worker 上最新的 commit</li><li>第 3 个 if 语句接着查本地的 snapshot cache，也就是伪代码中的 sc。sc 存储了每个 worker 上最近一次 LCB(wi, start ts) 对应的 commit ts。如果该 tuple 的版本号比上次 LCB 查到的 commit ts 还小，该 tuple 一定对当前事务可见。</li><li>第 4 个 if 语句检查 snapshot cache 是否需要更新。缓存的内容实际是 tuple_w_i 这个 worker 在 tx_ts_start 这个时间戳对应的 last_commit_ts。如果缓存的 cache_ts_start 比 tx_ts_start 更小（不会发生更大的情况），这时候就需要更新 LCB 缓存</li><li>如果前面的检查都 fail 了，到这里我们已经拿到了准确的 LCB(tuple_w_i, tx_ts_start) 的结果并将其缓存在了 sc[tuple_w_i] 中，最后只需要根据准确的 last_commit_ts 做一次比较即可判断该版本是否可见。</li></ul><h3 id=commit-log>Commit Log<a hidden class=anchor aria-hidden=true href=#commit-log>#</a></h3><p>为了计算每个工作线程 w 上的 LCB(w, ts)，需要为每个工作线程维护一个 Commit Log 队列，用来存储 start ts 和对应 commit ts 的映射关系，处理 LCB 请求时只需要查找 Commit Log 返回对应的 commit ts 即可。</p><p>Commit Log 会被并发读写，因此也需要合适的线程间同步机制。每个 Commit Log 都有一把锁，事务 commit 时需要获得写锁，将 commit ts 和对应的 Commit Log 写入后才完成事务提交，释放锁，之后其他拥有更大 start ts 的事务就能看见提交后的最新版本了。</p><p>对每个 worker 上的 Commit Log 队列来说，因为只需要为每个活跃事务保留一个 Commit Log，因此总共只需要保留 #W （worker 总数）条 Commit Log。每个 worker 开启新事务时都会检查 Commit Log 队列是否已满，如果队列已满，该 worker 需要收集所有活跃事务的 start ts，剔除那些活跃事务不可见的 Commit Log，限制 Commit Log 的总大小。</p><h2 id=garbage-collection>Garbage Collection<a hidden class=anchor aria-hidden=true href=#garbage-collection>#</a></h2><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212109238.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212109238.png></p><p>为了解决因长时间运行的 OLAP 查询导致 MVCC 版本堆积而拖慢 OLTP 事务的问题，作者通过优化器将查询分为 OLAP 和 OLTP，并维护了 oldest_tx 和 oldest_oltp 这两个 watermark。它们分别表示最老的 OLAP 事务和最老的 OLTP 事务的 start ts。这样就可以对 newest_olap 到 oldest_oltp 之间的版本进行 Cooperative GC（每个 worker 回收自己创建的位于 LCB(w, oldest_tx or oldest_oltp) 之间的 MVCC 版本），从而避免长时间运行的 OLAP 事务对 OLTP 事务的影响。</p><p>使用 start ts 的最高有效位来区分 OLTP 和 OLAP。事务结束后，start ts 将以 1/#W（#W 表示 worker 总数）的概率更新到全局共享数组中，从而可以通过分析该数组中的 OLTP 和 OLAP start ts 来获取这两个 watermark。</p><h3 id=tombstone>Tombstone<a hidden class=anchor aria-hidden=true href=#tombstone>#</a></h3><p>删除数据会产生 tombstone，而因为 long-running OLAP query 的存在，这些 tombstone 会在 main index 中一直累积，增加 OLTP 事务延迟：因为索引中没有数据的 MVCC 信息，要想知道一个数据是否删除了，需要先读索引再去 main index 拿到具体的 tuple，最后根据 tuple 上的 MVCC 信息判断数据是否已删除。</p><h3 id=graveyard-index>Graveyard Index<a hidden class=anchor aria-hidden=true href=#graveyard-index>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212341883.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212341883.png></p><p>因为被 tombstone 标记删除的数据只对那些 long-running OLAP query 可见，对那些 start ts 更大的 OLTP 事务不可见。因此作者引入了一个额外的 graveyard index 数据结构来存储这样的 tombstone，只要 tombstone 的 start ts 小于 oldest_oltp 就将其从 main index 移到 Graveyard Index。</p><p>OLAP query 需要同时查询 main index 和 Graveyard Index 来获取所有可见的数据版本，而 OLTP query 只需要查询 main index 即可，牺牲少量 OLAP 查询性能保障了 OLTP 事务延迟不受 long-running OLAP query 的影响。</p><h3 id=tombstone-index>Tombstone Index<a hidden class=anchor aria-hidden=true href=#tombstone-index>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212342342.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304212342342.png></p><p>为了知道有哪些 tombstone 并及时对它们进行 GC，作者以去中心化的方式在每个 worker 上维护了一个 Tombstone Index。Tombstone Index 是一个 append-optimized B+ tree，key 由 start ts、command id 组成，value 为该 tombstone 的 tuple key。根据 oldest_tx 以及 oldest_oltp 这两个 watermark 确定 Tombstone Index 中的 tombstone 是应该直接删掉还是从 main index 转移到 graveyard index。</p><h2 id=version-storage>Version Storage<a hidden class=anchor aria-hidden=true href=#version-storage>#</a></h2><p>同时采用了 Delta Index 和 FatTuple 两种版本存储方式，默认采用 Delta Index。</p><h3 id=delta-index>Delta Index<a hidden class=anchor aria-hidden=true href=#delta-index>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304220931386.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304220931386.png></p><p>如上图所示，默认情况下所有老版本数据都存储在每个 worker 线程的 Delta Index 中，main index 只存储最新版本的 tuple 和其 version chain 的一些 metadata，这些 metadata 包括：</p><ol><li>WorkerID 和 TSstart：共同决定了该 tuple 最新版本的可见性</li><li>WorkerID、TSstart 和 CommandID 共同构成了 Delta Index 的 key，同时也指向了 version chain 中上一个更老版本的数据。CommandID 主要用来区分同一个事物的不同操作，在 Delta Index 中可以省略，使用（WorkerID、TSstart、IndexID、Key）作为 Delta Index 的 key。</li><li>#updates 和 since_oldest_tx：用来估算 version chain 的长度。因为精确的维护 version chain 的长度代价太大，作者采用了启发式的估算方式。#updates 表示在 oldest_tx 这个 watermark 不变的情况下该 tuple 最多更新了多少次，反应了受 long-running OLAP query 影响的情况下累积的版本数。since_oldest_tx 存储的是 oldest_tx 的低 16 位（2 字节）。每当事务更行时都会检查当前 oldest_tx 的低 16 位是否和 since_oldest_tx 相等，是的话就把 #update 加 1，否的话就重置 #update 计数器。当 #update 超过 worker 总数时就将其转成 FatTuple 的存储方式。</li></ol><h3 id=fattuple>FatTuple<a hidden class=anchor aria-hidden=true href=#fattuple>#</a></h3><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304220951672.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304220951672.png></p><p>当 tuple 被频繁更新时会被自动转换为 FatTuple，把所有的数据版本 delta 都 inline 的存储在 main index 的 FatTuple 中。因为每个 FatTuple 只需要存储 #W 个数据版本，作者在 FatTuple 的基础上实现了 on-demand precise garbage collection（OPGC），OPGC 模式下会首先将 newest_olap 到 oldest_oltp 这个 dead zone 内的 delta GC 掉，如果还没有足够的槽位再去收集所有正在运行事务的 start ts，确定所有的 dead zone 后 GC 所有不需要的 delta。关于 precise garbage collection 可以参考《Long-lived Transactions Made Less Harmful》。</p><p>另外，因为 GC 时只会扫描 Tombstone Index 和 Delta Index，为了保证 FatTuple 中的旧版本也能被 GC，在 page eviction 时需要将 FatTuple 转成 Delta Index。</p><h2 id=durability-and-recovery>Durability and Recovery<a hidden class=anchor aria-hidden=true href=#durability-and-recovery>#</a></h2><p>因为所有事务操作都有 WAL，所以前面提到的数据结构比如 Commit Log、Graveyard Index、Tombstone Index、Delta Index 都没有 durability 要求。</p><p>恢复也比较简单，在 analysis 阶段确定 winning transactions，在 redo 阶段 delete 操作直接将 main index 上的 tombstone 删掉。每个 worker 的 Commit Log 初始化为最后一个 winning transaction 的 commit ts。</p><p>事务 abort 比较特殊。worker 的 Commit Log 中只存储了成功提交的事务，事务 abort 时需要将它所有的改动回滚（可能分散在 Delta Index、Tombstone Index 中），将它的 start ts 也回收，用于该 worker 的下一个事务的 start ts。</p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>从作者后面的实验来看，Graveyard Index 能够显著降低 long-running OLAP query 对 TPC-C 性能的影响：</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202305232344307.png alt="Figure 9: Graveyard technique stabilizes TPC-C with a long- running transaction"></p><p>而 FatTuple 的设计也使得 OLAP scan 的性能不受影响：
<img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202305232348801.png alt="Figure 10: Scan performance"></p><p>后面作者还做了关于 “TPC-C + Scan: Scalability”、“Out-of-Memory Breakdown”、“Bulk Loading”、“Out-of-Memory Key/Value”、“Deterministic Execution Under Contention” 等实验，感兴趣的朋友可以详细阅读 “4 EVALUATION” 这一节，从实验结果来看整体效果不错。</p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://zz-jason.github.io/posts/vldb-2006-analysis-of-two-existing-and-one-new-dynamic-programming-algorithm-for-the-generation-of-optimal-bushy-join-trees-without-cross-products/><span class=title>« Prev Page</span><br><span>[VLDB 2006] Analysis of Two Existing and One New Dynamic Programming Algorithm for the Generation of Optimal Bushy Join Trees without Cross Products</span></a>
<a class=next href=https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-latches/><span class=title>Next Page »</span><br><span>[VLDB 2023] Scalable and Robust Latches for Database Systems</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines on twitter" href="https://twitter.com/intent/tweet/?text=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Snapshot%20Isolation%20for%20High-Performance%20Storage%20Engines&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-si%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-si%2f&title=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Snapshot%20Isolation%20for%20High-Performance%20Storage%20Engines&summary=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Snapshot%20Isolation%20for%20High-Performance%20Storage%20Engines&source=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-si%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-si%2f&title=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Snapshot%20Isolation%20for%20High-Performance%20Storage%20Engines"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-si%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines on whatsapp" href="https://api.whatsapp.com/send?text=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Snapshot%20Isolation%20for%20High-Performance%20Storage%20Engines%20-%20https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-si%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines on telegram" href="https://telegram.me/share/url?text=%5bVLDB%202023%5d%20Scalable%20and%20Robust%20Snapshot%20Isolation%20for%20High-Performance%20Storage%20Engines&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fvldb-2023-scalable-and-robust-si%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=zz-jason/zz-jason.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://zz-jason.github.io/>Jian Zhang</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}</script></body></html>