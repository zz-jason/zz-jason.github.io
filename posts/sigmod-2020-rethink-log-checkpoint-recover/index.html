<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines | Jian Zhang</title><meta name=keywords content><meta name=description content="简介 基于磁盘的 DBMS 通常采用 ARIES 风格的日志恢复机制，它可以处理超过内存的数据和事务，可以在多次崩溃的情况下快速恢复，支持 fuzzy checkpoint 等。然而，ARIES 的"><meta name=author content="Jian"><link rel=canonical href=https://zz-jason.github.io/posts/sigmod-2020-rethink-log-checkpoint-recover/><link href=/assets/css/stylesheet.min.770d8a0aa405dcf691d11ff1ae1a3ef7f461eed00172c15620f8b0f1e5c77e8f.css integrity="sha256-dw2KCqQF3PaR0R/xrho+9/Rh7tABcsFWIPiw8eXHfo8=" rel="preload stylesheet" as=style><link rel=icon href=https://zz-jason.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://zz-jason.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://zz-jason.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://zz-jason.github.io/apple-touch-icon.png><link rel=mask-icon href=https://zz-jason.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta name=generator content="Hugo 0.110.0"><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-67872953-1","auto"),ga("send","pageview"))</script><script async src=https://www.google-analytics.com/analytics.js></script><meta property="og:title" content="[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines"><meta property="og:description" content="简介 基于磁盘的 DBMS 通常采用 ARIES 风格的日志恢复机制，它可以处理超过内存的数据和事务，可以在多次崩溃的情况下快速恢复，支持 fuzzy checkpoint 等。然而，ARIES 的"><meta property="og:type" content="article"><meta property="og:url" content="https://zz-jason.github.io/posts/sigmod-2020-rethink-log-checkpoint-recover/"><meta property="article:published_time" content="2023-04-09T00:00:00+00:00"><meta property="article:modified_time" content="2023-04-09T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines"><meta name=twitter:description content="简介 基于磁盘的 DBMS 通常采用 ARIES 风格的日志恢复机制，它可以处理超过内存的数据和事务，可以在多次崩溃的情况下快速恢复，支持 fuzzy checkpoint 等。然而，ARIES 的"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://zz-jason.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines","item":"https://zz-jason.github.io/posts/sigmod-2020-rethink-log-checkpoint-recover/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines","name":"[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines","description":"简介 基于磁盘的 DBMS 通常采用 ARIES 风格的日志恢复机制，它可以处理超过内存的数据和事务，可以在多次崩溃的情况下快速恢复，支持 fuzzy checkpoint 等。然而，ARIES 的","keywords":[],"articleBody":"简介 基于磁盘的 DBMS 通常采用 ARIES 风格的日志恢复机制，它可以处理超过内存的数据和事务，可以在多次崩溃的情况下快速恢复，支持 fuzzy checkpoint 等。然而，ARIES 的中心化日志模块开销很高，不能在现代多核 CPU 上扩展。这篇论文提出适用于多核 CPU 和高性能存储的日志恢复算法。作者扩展了《Scalable Logging through Emerging Non-Volatile Memory》中的 scalable logging 机制，实现了 continuous checkpoint、高效的页面分配和跨日志文件的 commit 优化。其性能与内存数据库相当。\nScalable Logging 《Scalable Logging through Emerging Non-Volatile Memory》中介绍的 Scalable Logging 采用多个日志文件来解决并发事务之间的全局锁竞争问题。每个日志文件供一个或多个工作线程独立使用。\nGSN（global sequence number）：类比分布式时钟，事务和 page 都可以看作是分布式系统中的进程，而 WAL record 则是需要排序的事件。当事务访问 page 时，它的 txnGSN 会被设置为 max(txnGSN, pageGSN)，而后续产生的 WAL record 的 GSN 则为 txnGSN+1。这种 GSN 机制可以在不同日志文件的 WAL record 之间建立类似于分布式时钟的偏序关系。当两个事务访问不同的页面时，它们的 GSN 不同步，后 commit 的事务甚至可以有较小的 GSN（如图 1b 所示）。这种机制也保证了每个日志内部的 WAL record 按 GSN 排序。在恢复过程中，page 的日志记录需要从所有的日志中收集，按 GSN 排序后再使用。\npassive group commit：在事务提交时，首先会将自己的日志 flush，然后加入到 group commit queue 中等待。等队列中小于该事务 GSN 的所有其他事务 commit 后，当前事务才会 commit，从而维护正确的 WAL record 偏序关系。\n这篇论文的方案类似于 scalable logging：每个工作线程都有自己的本地日志文件，WAL record 包含 log type、page ID、transaction ID 以及 before-after image 等信息。在事务提交前，先将该事务的 WAL record 写入 PMEM 缓存中，然后根据作者提出的 Remote Flush Avoidance 机制（简称 RFA）检查是否有其他事务和它修改了共同的 page 产生了依赖关系，并根据需要提交到 group commit 队列中。采用 continuous checkpointing 来平滑 IO，避免突发写入造成事务延迟抖动。\nTwo-Stage Distributed Logging 每个工作线程都有自己的本地日志文件，每个事务由一个工作线程完成，WAL record 也写入该线程所属的日志文件中。每个工作线程的日志由 3 个阶段构成：\nPMEM 的 log chunk 组成的循环链表。工作线程不断取出 free chunk 写入当前事务的 WAL。当事务的所有 WAL 在此阶段写入完成时，即可向外部返回提交成功。 SSD。每个工作线程都有一个 WAL writer 线程，它将 full chunk 写入 SSD，为第 1 阶段提供可用的 free chunk。 Low-Latency Commit Through RFA 为保证 ACID 中的 C，提交当前事务时需要确保 GSN 小于它的所有事务都已经执行完并且对应的 WAL 已经被 flush。例如：Txn1 在 page a 上删除了一条数据，并在其本地日志 L1 上写下 GSN 为 12 的 WAL；Txn2 在 page a 上插入了一条数据，并在其本地日志 L2 上写下 GSN 为 13 的 WAL。Txn2 的提交需要等待 Txn1 的提交结束，也就是 GSN 13 之前的 GSN 12 也需要被 flush 到磁盘后才能对外返回执行成功：\nTimeline\nTxn1 (Log=L1)\nTxn2 (Log=L2)\n1\nDELETE(Pa, a1), WAL(L1, GSN=12)\n2\nINSERT(Pa, a2), WAL(L2, GSN=13)\n3\nCOMMIT\n3\nCOMMIT\n这种机制对事务的吞吐量和延迟产生了很大的影响。Scalable logging 采用了 passive group commit 来缓解这个问题。事务的 WAL 在 flush 到本地日志后立即进入全局 commit queue，由 group commit 后台线程周期性地检查所有日志文件中已 flush 的 GSN，以此判断哪些事务可以提交。\n大多数事务是互相独立的，因此即使 GSN 存在大小关系，也可以并行提交。为避免不必要的 group commit，作者提出了一种 RFA（Remote Flush Avoidance）机制来识别互相独立的事务。\nRFA 原理很简单：如果该页面上的修改对应的 WAL 已经 flush，或者该页面没有被其他线程并发修改过，则可以跳过 group commit：\n每个事务开始时，记录所有日志文件的 flush GSN 中的最小值，记为 GSN_flushed。所有进行中的事务写入的 WAL GSN 都一定大于 GSN_flushed。如果 page 的 GSN 不超过 GSN_flushed，则表示该 page 上的修改对应的 WAL 已经 flush 到了各个日志文件中，该事务不依赖于任何未提交的事务。 每个 page 记录了上一次的修改者的日志文件，记为 L_last。每次事务修改 page 时，page 的 L_last 都会更新为修改者的日志文件。如果 page 的 GSN 超过了该事务的 GSN_flushed，则说明该事务依赖正在进行的另一个事务（可能是它自己）。需要检查 page 的 L_last 是否与该事务的日志文件相同，以判断是否有其他人修改了该 page。 每个事务都会维护 needsRemoteFlush 标志。如果其他事务修改了页面，则该标志设置为 true，该事务需要进入 group commit 队列。如果该事务的所有读写操作结束后，needsRemoteFlush 仍然为 false，则可以跳过 group commit 队列，避免 remote flush。 下图形象的对比了传统 ARIES、普通 group commit，以及经过 RFA 优化的 group commit 在处理互相独立事务时的差异：\nContinuous Checkpointing continuous checkpoint 希望做到只有新增的 WAL 超过某个阈值时才触发 checkpoint。为了避免扫整个 buffer pool，buffer pool 被分成了 S 个 buffer shard，每次增量 checkpoint 时采用 round-robin 的方式选择一个 buffer shard 进行全量 checkpoint。continuous checkpoint 的伪代码如下：\n为了计算 checkpoint 结束后哪些 WAL 日志可以删除，需要如下两个信息：\nbuffer pool 的 checkpointed GSN：checkpointer 维护每个 buffer shard 的 checkpointed GSN（也就是上面伪代码中的 maxChkptedInShard[shard]），表示该 buffer shard 上所有 GSN 小于该值的 WAL 都已经被 flush 到了各个日志文件中。整个 buffer pool 的 checkpointed GSN 是所有 buffer shard 的 checkpointed GSN 的最小值。 活跃事务的最小 GSN：未提交的活跃事务也会影响 WAL 日志清除，因为在事务 abort 时需要靠这些日志产生对应的 compensation log。因此需要维护所有活跃事务的最小 GSN。 利用 checkpointed GSN 和活跃事务的最小 GSN 即可得出可以被安全清理的 WAL GSN，各个日志文件可以根据它来清理不需要的 WAL 日志。\n上图展示了一个增量 checkpoint 的例子。buffer pool 被分成了 3 个 shard，每个 shard 有 2 个 page：\n最近一次增量 checkpoint 发生在 shard b1 上，从中间 checkpointer 维护的 checkpointed GSN 表来看，shard b1 的 checkpointed GSN 是 34。 下一次增量 checkpoint 发生在 shard b2，shard b2 此时的 checkpointed GSN 是 8，也是整个 buffer pool 中最小的 checkpointed GSN，而所有活跃事务的最小 GSN 是 27，意味着目前只有 GSN 小于 8 的 WAL 才能被清理掉。 对 shard b2 做增量 checkpoint，b2 里只有一个 dirty page G，将 G 写入磁盘后，b2 的 checkpointed GSN 就被更新成了 checkpoint 前所有日志文件最小的 GSN 46。buffer pool 的 checkpointed GSN 也从一开始的 8 变成了 24（shard 3 的 checkpointed GSN）。 Page Provisioning 除了 checkpoint，buffer manager 在进行缓存替换时也会将 dirty page 写入磁盘。LeanStore 的 buffer pool 存在 hot、cool 以及 free 三个区域。为了提升工作线程的事务性能，作者使用了一个 page provider 线程来负责 page 相关的 housekeeping 工作，确保工作线程有足够的 free page 可用：\n挑选 hot page 放入 cool 区域，以及将 cool 区域中的 page 按需移回 hot。 驱逐 cool 区域的 cool page（如果是 dirty page 就将其落盘）将 buffer frame 放入 free 区域供工作线程复用。 工作线程发现 buffer pool 偏离它的状态时（比如 hot area 满了）会唤醒 page provider。page provider 多轮运行后，使得 cool 和 free 区域的 page 数量恢复到预设的状态，在每轮运行中 page provider 首先 unswizzle 固定数目的 hot page，比如 256 个，将它们插入 cool 区域的 FIFO queue 的队首，然后从 FIFO queue 的队尾驱逐 cool page。如果是 clean page 就直接出队，清理内存后放回 free list 中。如果是 dirty page 就先保留在 queue 中，先将其标记为 writeBack，拷贝到本地的 writeBack buffer 中。因为仍然在 cool 区域的 FIFO 队列中，在 writeBack 状态的 page 仍然可以被修改，也可以在 swizzled 和 unswizzled 状态之间转换。当 writeBack buffer 填满后，它们才被一次性写入磁盘，更新对应的 GSN 和 dirty 标志。在下一轮运行过程中，page provider 仍然会从 FIFO queue 的队尾取出一个 cool page，这时候如果还是上一次遇到的 dirty page，因为已经写入磁盘且没经过其他修改，所以也不再 dirty，可以直接当做 clean page 处理。\nTransaction Abort 作者和 ARIES 一样采用了 STEAL 的策略，允许未提交的更改写到磁盘上。事务 abort 也需要写 compensation WAL。\nRecovery 和 ARIES 一样，LeanStore 的 recovery 也分成了 3 个阶段。\nlog analysis：每个线程都会扫描包括 PMEM 在内的所有 WAL 日志，分析出执行成功和失败的事务。对执行成功的事务，将对应的 WAL 按照 page ID 进行 partition，对执行失败的事务将他们放到 undo list 中。 redo：每个线程都分配了一个 page ID 范围，将对应的 WAL 按照 (page ID, GSN) 排序，接着逐个 page 进行 redo。 undo：遍历需要 undo 的 WAL，进行逻辑上的 undo。 总结 以上就是这篇论文提出的日志和恢复机制的核心部分了，论文后面作者也提供了详细的测试结果，感兴趣的朋友可以阅读原文再详细了解下。\n","wordCount":"3337","inLanguage":"en","datePublished":"2023-04-09T00:00:00Z","dateModified":"2023-04-09T00:00:00Z","author":{"@type":"Person","name":"Jian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://zz-jason.github.io/posts/sigmod-2020-rethink-log-checkpoint-recover/"},"publisher":{"@type":"Organization","name":"Jian Zhang","logo":{"@type":"ImageObject","url":"https://zz-jason.github.io/favicon.ico"}}}</script></head><body id=top><noscript><style type=text/css>#theme-toggle,.top-link{display:none}</style></noscript><header class=header><nav class=nav><div class=logo><a href=https://zz-jason.github.io/ accesskey=h title="Jian Zhang (Alt + H)">Jian Zhang</a>
<span class=logo-switches></span></div><ul id=menu onscroll=menu_on_scroll()><li><a href=https://zz-jason.github.io/ title=Home><span>Home</span></a></li><li><a href=https://zz-jason.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://zz-jason.github.io/posts/ title=Archives><span>Archives</span></a></li><li><a href=https://zz-jason.github.io/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines</h1><div class=post-meta>April 9, 2023&nbsp;·&nbsp;7 min&nbsp;·&nbsp;Jian</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><div class=details>Table of Contents</div></summary><div class=inner><ul><li><a href=#%e7%ae%80%e4%bb%8b aria-label=简介>简介</a></li><li><a href=#scalable-logging aria-label="Scalable Logging">Scalable Logging</a></li><li><a href=#two-stage-distributed-logging aria-label="Two-Stage Distributed Logging">Two-Stage Distributed Logging</a><ul><li><a href=#low-latency-commit-through-rfa aria-label="Low-Latency Commit Through RFA">Low-Latency Commit Through RFA</a></li></ul></li><li><a href=#continuous-checkpointing aria-label="Continuous Checkpointing">Continuous Checkpointing</a></li><li><a href=#page-provisioning aria-label="Page Provisioning">Page Provisioning</a></li><li><a href=#transaction-abort aria-label="Transaction Abort">Transaction Abort</a></li><li><a href=#recovery aria-label=Recovery>Recovery</a></li><li><a href=#%e6%80%bb%e7%bb%93 aria-label=总结>总结</a></li></ul></div></details></div><div class=post-content><h2 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h2><p>基于磁盘的 DBMS 通常采用 ARIES 风格的日志恢复机制，它可以处理超过内存的数据和事务，可以在多次崩溃的情况下快速恢复，支持 fuzzy checkpoint 等。然而，ARIES 的中心化日志模块开销很高，不能在现代多核 CPU 上扩展。这篇论文提出适用于多核 CPU 和高性能存储的日志恢复算法。作者扩展了《<a href=http://www.vldb.org/pvldb/vol7/p865-wang.pdf>Scalable Logging through Emerging Non-Volatile Memory</a>》中的 scalable logging 机制，实现了 continuous checkpoint、高效的页面分配和跨日志文件的 commit 优化。其性能与内存数据库相当。</p><h2 id=scalable-logging>Scalable Logging<a hidden class=anchor aria-hidden=true href=#scalable-logging>#</a></h2><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304091631305.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304091631305.png></p><p>《<a href=http://www.vldb.org/pvldb/vol7/p865-wang.pdf>Scalable Logging through Emerging Non-Volatile Memory</a>》中介绍的 Scalable Logging 采用多个日志文件来解决并发事务之间的全局锁竞争问题。每个日志文件供一个或多个工作线程独立使用。</p><p><strong>GSN（global sequence number）</strong>：类比分布式时钟，事务和 page 都可以看作是分布式系统中的进程，而 WAL record 则是需要排序的事件。当事务访问 page 时，它的 txnGSN 会被设置为 max(txnGSN, pageGSN)，而后续产生的 WAL record 的 GSN 则为 txnGSN+1。这种 GSN 机制可以在不同日志文件的 WAL record 之间建立类似于分布式时钟的偏序关系。当两个事务访问不同的页面时，它们的 GSN 不同步，后 commit 的事务甚至可以有较小的 GSN（如图 1b 所示）。这种机制也保证了每个日志内部的 WAL record 按 GSN 排序。在恢复过程中，page 的日志记录需要从所有的日志中收集，按 GSN 排序后再使用。</p><p><strong>passive group commit</strong>：在事务提交时，首先会将自己的日志 flush，然后加入到 group commit queue 中等待。等队列中小于该事务 GSN 的所有其他事务 commit 后，当前事务才会 commit，从而维护正确的 WAL record 偏序关系。</p><p>这篇论文的方案类似于 scalable logging：每个工作线程都有自己的本地日志文件，WAL record 包含 log type、page ID、transaction ID 以及 before-after image 等信息。在事务提交前，先将该事务的 WAL record 写入 PMEM 缓存中，然后根据作者提出的 Remote Flush Avoidance 机制（简称 RFA）检查是否有其他事务和它修改了共同的 page 产生了依赖关系，并根据需要提交到 group commit 队列中。采用 continuous checkpointing 来平滑 IO，避免突发写入造成事务延迟抖动。</p><h2 id=two-stage-distributed-logging>Two-Stage Distributed Logging<a hidden class=anchor aria-hidden=true href=#two-stage-distributed-logging>#</a></h2><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304091743166.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304091743166.png></p><p>每个工作线程都有自己的本地日志文件，每个事务由一个工作线程完成，WAL record 也写入该线程所属的日志文件中。每个工作线程的日志由 3 个阶段构成：</p><ol><li>PMEM 的 log chunk 组成的循环链表。工作线程不断取出 free chunk 写入当前事务的 WAL。当事务的所有 WAL 在此阶段写入完成时，即可向外部返回提交成功。</li><li>SSD。每个工作线程都有一个 WAL writer 线程，它将 full chunk 写入 SSD，为第 1 阶段提供可用的 free chunk。</li></ol><h3 id=low-latency-commit-through-rfa>Low-Latency Commit Through RFA<a hidden class=anchor aria-hidden=true href=#low-latency-commit-through-rfa>#</a></h3><p>为保证 ACID 中的 C，提交当前事务时需要确保 GSN 小于它的所有事务都已经执行完并且对应的 WAL 已经被 flush。例如：Txn1 在 page a 上删除了一条数据，并在其本地日志 L1 上写下 GSN 为 12 的 WAL；Txn2 在 page a 上插入了一条数据，并在其本地日志 L2 上写下 GSN 为 13 的 WAL。Txn2 的提交需要等待 Txn1 的提交结束，也就是 GSN 13 之前的 GSN 12 也需要被 flush 到磁盘后才能对外返回执行成功：</p><p>Timeline</p><p>Txn1 (Log=L1)</p><p>Txn2 (Log=L2)</p><p>1</p><p>DELETE(Pa, a1), WAL(L1, GSN=12)</p><p>2</p><p>INSERT(Pa, a2), WAL(L2, GSN=13)</p><p>3</p><p>COMMIT</p><p>3</p><p>COMMIT</p><p>这种机制对事务的吞吐量和延迟产生了很大的影响。Scalable logging 采用了 passive group commit 来缓解这个问题。事务的 WAL 在 flush 到本地日志后立即进入全局 commit queue，由 group commit 后台线程周期性地检查所有日志文件中已 flush 的 GSN，以此判断哪些事务可以提交。</p><p>大多数事务是互相独立的，因此即使 GSN 存在大小关系，也可以并行提交。为避免不必要的 group commit，作者提出了一种 RFA（Remote Flush Avoidance）机制来识别互相独立的事务。</p><p>RFA 原理很简单：如果该页面上的修改对应的 WAL 已经 flush，或者该页面没有被其他线程并发修改过，则可以跳过 group commit：</p><ol><li>每个事务开始时，记录所有日志文件的 flush GSN 中的最小值，记为 GSN_flushed。所有进行中的事务写入的 WAL GSN 都一定大于 GSN_flushed。如果 page 的 GSN 不超过 GSN_flushed，则表示该 page 上的修改对应的 WAL 已经 flush 到了各个日志文件中，该事务不依赖于任何未提交的事务。</li><li>每个 page 记录了上一次的修改者的日志文件，记为 L_last。每次事务修改 page 时，page 的 L_last 都会更新为修改者的日志文件。如果 page 的 GSN 超过了该事务的 GSN_flushed，则说明该事务依赖正在进行的另一个事务（可能是它自己）。需要检查 page 的 L_last 是否与该事务的日志文件相同，以判断是否有其他人修改了该 page。</li><li>每个事务都会维护 needsRemoteFlush 标志。如果其他事务修改了页面，则该标志设置为 true，该事务需要进入 group commit 队列。如果该事务的所有读写操作结束后，needsRemoteFlush 仍然为 false，则可以跳过 group commit 队列，避免 remote flush。</li></ol><p>下图形象的对比了传统 ARIES、普通 group commit，以及经过 RFA 优化的 group commit 在处理互相独立事务时的差异：</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304091943779.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304091943779.png></p><h2 id=continuous-checkpointing>Continuous Checkpointing<a hidden class=anchor aria-hidden=true href=#continuous-checkpointing>#</a></h2><p>continuous checkpoint 希望做到只有新增的 WAL 超过某个阈值时才触发 checkpoint。为了避免扫整个 buffer pool，buffer pool 被分成了 S 个 buffer shard，每次增量 checkpoint 时采用 round-robin 的方式选择一个 buffer shard 进行全量 checkpoint。continuous checkpoint 的伪代码如下：</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151404821.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151404821.png></p><p>为了计算 checkpoint 结束后哪些 WAL 日志可以删除，需要如下两个信息：</p><ol><li><strong>buffer pool 的 checkpointed GSN</strong>：checkpointer 维护每个 buffer shard 的 checkpointed GSN（也就是上面伪代码中的 maxChkptedInShard[shard]），表示该 buffer shard 上所有 GSN 小于该值的 WAL 都已经被 flush 到了各个日志文件中。整个 buffer pool 的 checkpointed GSN 是所有 buffer shard 的 checkpointed GSN 的最小值。</li><li><strong>活跃事务的最小 GSN</strong>：未提交的活跃事务也会影响 WAL 日志清除，因为在事务 abort 时需要靠这些日志产生对应的 compensation log。因此需要维护所有活跃事务的最小 GSN。</li></ol><p>利用 checkpointed GSN 和活跃事务的最小 GSN 即可得出可以被安全清理的 WAL GSN，各个日志文件可以根据它来清理不需要的 WAL 日志。</p><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151614862.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151614862.png></p><p>上图展示了一个增量 checkpoint 的例子。buffer pool 被分成了 3 个 shard，每个 shard 有 2 个 page：</p><ol><li>最近一次增量 checkpoint 发生在 shard b1 上，从中间 checkpointer 维护的 checkpointed GSN 表来看，shard b1 的 checkpointed GSN 是 34。</li><li>下一次增量 checkpoint 发生在 shard b2，shard b2 此时的 checkpointed GSN 是 8，也是整个 buffer pool 中最小的 checkpointed GSN，而所有活跃事务的最小 GSN 是 27，意味着目前只有 GSN 小于 8 的 WAL 才能被清理掉。</li><li>对 shard b2 做增量 checkpoint，b2 里只有一个 dirty page G，将 G 写入磁盘后，b2 的 checkpointed GSN 就被更新成了 checkpoint 前所有日志文件最小的 GSN 46。buffer pool 的 checkpointed GSN 也从一开始的 8 变成了 24（shard 3 的 checkpointed GSN）。</li></ol><h2 id=page-provisioning>Page Provisioning<a hidden class=anchor aria-hidden=true href=#page-provisioning>#</a></h2><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151634946.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151634946.png></p><p>除了 checkpoint，buffer manager 在进行缓存替换时也会将 dirty page 写入磁盘。LeanStore 的 buffer pool 存在 hot、cool 以及 free 三个区域。为了提升工作线程的事务性能，作者使用了一个 page provider 线程来负责 page 相关的 housekeeping 工作，确保工作线程有足够的 free page 可用：</p><ol><li>挑选 hot page 放入 cool 区域，以及将 cool 区域中的 page 按需移回 hot。</li><li>驱逐 cool 区域的 cool page（如果是 dirty page 就将其落盘）将 buffer frame 放入 free 区域供工作线程复用。</li></ol><p>工作线程发现 buffer pool 偏离它的状态时（比如 hot area 满了）会唤醒 page provider。page provider 多轮运行后，使得 cool 和 free 区域的 page 数量恢复到预设的状态，在每轮运行中 page provider 首先 unswizzle 固定数目的 hot page，比如 256 个，将它们插入 cool 区域的 FIFO queue 的队首，然后从 FIFO queue 的队尾驱逐 cool page。如果是 clean page 就直接出队，清理内存后放回 free list 中。如果是 dirty page 就先保留在 queue 中，先将其标记为 writeBack，拷贝到本地的 writeBack buffer 中。因为仍然在 cool 区域的 FIFO 队列中，在 writeBack 状态的 page 仍然可以被修改，也可以在 swizzled 和 unswizzled 状态之间转换。当 writeBack buffer 填满后，它们才被一次性写入磁盘，更新对应的 GSN 和 dirty 标志。在下一轮运行过程中，page provider 仍然会从 FIFO queue 的队尾取出一个 cool page，这时候如果还是上一次遇到的 dirty page，因为已经写入磁盘且没经过其他修改，所以也不再 dirty，可以直接当做 clean page 处理。</p><h2 id=transaction-abort>Transaction Abort<a hidden class=anchor aria-hidden=true href=#transaction-abort>#</a></h2><p>作者和 ARIES 一样采用了 STEAL 的策略，允许未提交的更改写到磁盘上。事务 abort 也需要写 compensation WAL。</p><h2 id=recovery>Recovery<a hidden class=anchor aria-hidden=true href=#recovery>#</a></h2><p><img src=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151721637.png alt=https://raw.githubusercontent.com/zz-jason/blog-images/master/images/202304151721637.png></p><p>和 ARIES 一样，LeanStore 的 recovery 也分成了 3 个阶段。</p><ol><li>log analysis：每个线程都会扫描包括 PMEM 在内的所有 WAL 日志，分析出执行成功和失败的事务。对执行成功的事务，将对应的 WAL 按照 page ID 进行 partition，对执行失败的事务将他们放到 undo list 中。</li><li>redo：每个线程都分配了一个 page ID 范围，将对应的 WAL 按照 (page ID, GSN) 排序，接着逐个 page 进行 redo。</li><li>undo：遍历需要 undo 的 WAL，进行逻辑上的 undo。</li></ol><h2 id=总结>总结<a hidden class=anchor aria-hidden=true href=#总结>#</a></h2><p>以上就是这篇论文提出的日志和恢复机制的核心部分了，论文后面作者也提供了详细的测试结果，感兴趣的朋友可以阅读原文再详细了解下。</p></div><footer class=post-footer><nav class=paginav><a class=prev href=https://zz-jason.github.io/posts/vldb-2023-scalable-and-robust-latches/><span class=title>« Prev Page</span><br><span>[VLDB 2023] Scalable and Robust Latches for Database Systems</span></a>
<a class=next href=https://zz-jason.github.io/posts/vldb-2022-memory-opotimized-mvcc/><span class=title>Next Page »</span><br><span>[VLDB 2022] Memory-Optimized Multi-Version Concurrency Control for Disk-Based Database Systems</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines on twitter" href="https://twitter.com/intent/tweet/?text=%5bSIGMOD%202020%5d%20Rethinking%20Logging%2c%20Checkpoints%2c%20and%20Recovery%20for%20High-Performance%20Storage%20Engines&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fsigmod-2020-rethink-log-checkpoint-recover%2f&hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fsigmod-2020-rethink-log-checkpoint-recover%2f&title=%5bSIGMOD%202020%5d%20Rethinking%20Logging%2c%20Checkpoints%2c%20and%20Recovery%20for%20High-Performance%20Storage%20Engines&summary=%5bSIGMOD%202020%5d%20Rethinking%20Logging%2c%20Checkpoints%2c%20and%20Recovery%20for%20High-Performance%20Storage%20Engines&source=https%3a%2f%2fzz-jason.github.io%2fposts%2fsigmod-2020-rethink-log-checkpoint-recover%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fzz-jason.github.io%2fposts%2fsigmod-2020-rethink-log-checkpoint-recover%2f&title=%5bSIGMOD%202020%5d%20Rethinking%20Logging%2c%20Checkpoints%2c%20and%20Recovery%20for%20High-Performance%20Storage%20Engines"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fzz-jason.github.io%2fposts%2fsigmod-2020-rethink-log-checkpoint-recover%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines on whatsapp" href="https://api.whatsapp.com/send?text=%5bSIGMOD%202020%5d%20Rethinking%20Logging%2c%20Checkpoints%2c%20and%20Recovery%20for%20High-Performance%20Storage%20Engines%20-%20https%3a%2f%2fzz-jason.github.io%2fposts%2fsigmod-2020-rethink-log-checkpoint-recover%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines on telegram" href="https://telegram.me/share/url?text=%5bSIGMOD%202020%5d%20Rethinking%20Logging%2c%20Checkpoints%2c%20and%20Recovery%20for%20High-Performance%20Storage%20Engines&url=https%3a%2f%2fzz-jason.github.io%2fposts%2fsigmod-2020-rethink-log-checkpoint-recover%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer><script src=https://utteranc.es/client.js repo=zz-jason/zz-jason.github.io issue-term=pathname theme=github-light crossorigin=anonymous async></script></article></main><footer class=footer><span>&copy; 2023 <a href=https://zz-jason.github.io/>Jian Zhang</a></span>
<span>&#183;</span>
<span>Powered by <a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span>
<span>&#183;</span>
<span>Theme <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)"><button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></button></a>
<script defer src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<script>window.onload=function(){localStorage.getItem("menu-scroll-position")&&(document.getElementById("menu").scrollLeft=localStorage.getItem("menu-scroll-position"))},document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})});var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")};function menu_on_scroll(){localStorage.setItem("menu-scroll-position",document.getElementById("menu").scrollLeft)}</script></body></html>