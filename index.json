[{"content":" Manang, Nepal, 2024\nIntroduction # 闪存的性能越来越强，价格也越来越便宜：一块 PCIe 4.0 SSD 有超过 1M 的随机读 IOPS，7GB/s 的总带宽，而新一代 PCIe 5.0 SSD 拥有 12GB/s 的总带宽，已经很接近内存的带宽了。价格方面，企业级 SSD 的价格差不多 $200/TB，比内存便宜了 10-50 倍。作者相信，随着 Optane 的商业失败，闪存在数据存储方面会发挥越来越大的作用，成为 cost-efficient 的唯一选择。\n如下表所示，假设总共有 $15000 的预算，一半用来配置 64 核 CPU、512GB 内存，剩下的预算可以用来加 2TB 内存，或者加 8 块 4TB 的 PCIe 4.0 NVMe SSD 以实现更大的存储容量：\n虽然可以配置多块 SSD 实现更大存储容量，但现有存储引擎设计却并不能完全发挥它们的总 IOPS 和带宽。每块 SSD 的 4KB 随机读能达到 1.5M IOPS ，8 块 SSD 可以达到 12M 的总 IOPS。如下图所示，作者测试了 5 个系统的随机读性能，总数据量 100GB，buffer pool 10GB，即使表现最好的 LeanStore 距离理论上限也还有 3.5 倍的差距：\n对一组 NVMe SSD 来说，怎样才能发挥它们性能上限？应该使用什么 IO API，pread/pwrite、libaio、io_uring 还是 SPDK？既要减少 IO 放大又要提高性能，应该使用多大的 page size？如何并发控制几百万的 IOPS？应该使用什么样的线程模型，是专门的 IO 线程池还是在每个工作线程中进行 IO？\n以上这些问题就是这篇论文希望解决的。作者讨论了 NVMe 闪存的硬件特点，针对这些特点重新设计 LeanStore 使其能够充分发挥多块 SSD 的总 IOPS，最大化系统的读写吞吐。\nWhat Modern NVMe Storage Can Do # 作者通过各种 micro benchmark 揭示了 SSD 的硬件特性和如何充分发挥它的性能。所有实验都基于 64 核 AMD zen4 CPU + 8 块三星 PM1733 SSD。\nDrive Scalability # 上图是 4KB 随机读和读写混合的测试结果，揭示了不同读写比例的总 IOPS 上限。完全随机写的情况下 8 块 SSD 的总 IOPS 为 4.7M，90% 随机读的总 IOPS 提升到了 8.9M，完全随机读的总 IOPS 可以达到 12.5M。\nThe Case for 4KB Pages # 对数据库来说 page size 的选择很重要，许多数据库使用了比 4KB 更大的 page size，比如 PG、SQL Server 为 8KB，MySQL 为 16KB，WiredTiger 为 32KB。更大的 page size 对 in-memory 工作负载更有利（比如减少 B+ 树的高度），也能减少 buffer pool 的 page 数量降低缓存维护负担。但 page size 过大的会导致严重的 IO 放大，可以说 page size 的选择是各种 trade off 后的结果。\n上图是不同 page size 的随机读性能测试结果。结果表明 4KB 的 page size 能最大化 IOPS，最小化 IO 延迟。虽然在吞吐方面还不是最优，比如 16KB page size 才能最大化读带宽，但是为了充分发挥磁盘 IOPS 提高存储引擎性能，4KB page size 是最优选择。\nSSD Parallelism # 光调整 page size 到 4KB 还不够。SSD 是个内部高度并行化的设备，提供了多个可以同时读写的数据通道。它的随机读延迟在 100us 级别，一次一个 page 的同步 IO 只能获得 10K IOPS，也就是 40MB/s 的带宽，距离单盘 1.5M IOPS 的上限相去甚远。\n上图测试了 8 块 SSD 在不同 IO depth 下的 IOPS。当 IO depth 为 3000 时才能达到极限的 12.5M IOPS。换言之，从一次一个 page 的同步 IO 视角来看，需要 3000 并发这样的同步 IO 才能耗尽所有 SSD 的 IOPS，而在单机数据库里实现和管理这 3000 并发是个非常挑战的事情。\nI/O Interfaces # 作者讨论了 4 个 Linux 上常用的 IO 库：POSIX pread/pwrite、libaio、io_uring 以及 SPDK。不管使用哪个库，最终都需要将用户的 IO 请求发给 SSD 的 submission queue，SSD 处理完后会将 completion event 发送到 completion queue，通知应用 IO 完成。\nPOSIX pread/pwrite 是一种同步接口，每次处理一个 IO 请求，IO 未完成会被阻塞，每次请求都会产生 context switch 和系统调用。\nlibaio 是一种异步接口，通过一次 io_submit() 系统调用提交多个 IO 请求，IO 处理不阻塞用户程序，通过 get_events() 获取 completion events 来判断之前提交的 IO 请求是否已经完成。libaio 降低了系统调用和 context switch，单线程即可同时处理多个 IO 请求。\nio_uring 是 libaio 的继任者，也是一种异步接口。通过用户态和内核态共享的 submission/completion queue（图 C 蓝色部分）来提交和收割 IO 请求。用户通过 io_uring_enter() 提交请求，内核的处理过程和其他接口一样各个 layer 都要走一遍，直到最后把 IO 请求提交到 SSD 的 submission queue 中。io_uring 有一个 SQPOLL 模式，开启后会在内核中启动后台线程 kernel-worker 拉取和处理用户 submission queue（图 C 蓝色部分）中的 IO 请求。SQPOLL 模式下不需要任何系统调用。io_uring 还有个 IOPOLL 模式用于通知用户 IO 处理完成。之前所有介绍的 IO 库都是通过硬件中断来通知 IO 完成的，在 io_uring 的 IOPOLL + SQPOLL 模式下，可以完全省去系统调用和硬件中断，理论上比 libaio 性能更好，开销更低。\nSPDK 全称是 Intel Storage Performance Development Kit，SPDK NVMe driver 是 NVMe SSD 的用户态驱动，用户可以通过 SPDK bypass 内核直接和 SSD 的 submission/completion queue 交互，也不会有硬件中断，理论上拥有最好的性能和最低的开销。\nA Tight CPU Budget # 要打满 12M IOPS 对 CPU 的消耗也很高，按照作者使用的 AMD 2.5GHz 64 核 CPU 来算，平均每 13K（2.5G*64/12M）个时钟周期就需要完成一个 IO 请求。\n上图测试了不同 IO 库实现相同 IOPS 时的 CPU 开销。一些结论：\nlibaio 和普通 io_uring 在使用 32 线程后 IOPS 就上不去了，完全不能打满 12M IOPS，大多时间都消耗在了无法省去的内核处理上。 SQPOLL + IOPOLL 模式的 io_uring，禁掉一些内核功能比如文件系统，RAID，OS page cache 后性能会好很多，使用 32 线程可以打满 IOPS。 SPDK 的性能最好，CPU 开销最低，3 线程就可以打满 IOPS。 这里基本就宣告只能使用 SPDK 了，因为即使 io_uring 的 SQPOLL + IOPOLL 模式也需要 32 线程才能耗尽 IOPS，而剩下的 CPU，也就是 6.5K 个时钟周期需要用来进行查询处理、索引遍历、并发控制、缓存替换、日志记录等，肯定是不够用的。\nImplications for High-Performance Storage Engines # LeanStore 虽然专门面向 NVMe SSD 优化，但还是不能充分发挥多块 SSD 的 IOPS，上面的分析也给出了差距来源和优化思路。\nLeanStore 将工作线程和 page provider 分开。工作线程负责处理用户事务，当需要的 page 不在 buffer pool 时通过 pread 从磁盘读取。page provider 负责缓存替换，将 buffer pool 中不常使用的 page 写回磁盘。\n工作线程的 pread 导致每个 page IO 都有一次系统调用，需要同步等待内核返回结果。按之前的分析，这种模式要想打满 12M IOPS 需要上千个工作线程，而真有这么多工作线程时，又会有更多 CPU 用于 context switch 和线程调度，既低效又不鲁棒。\nHow to Exploit NVMe Storage # 作者基于 LeanStore 继续优化，使其完全发挥所有 SSD 的总 IOPS，在只有 64 核 128 线程的情况下，使系统能处理上千并发的 IO request。\nDesign Overview and Outline # 上图是 IO request 视角的系统设计概览，优化后的 LeanStore 使用协程调度了上千并发的 IO request，同时设计了高效的缓存替换和异步脏页回写使系统提供足够的空闲 buffer frame，由 IO backend 通过 SPDK 读写 SSD。\nDBMS-Managed Multitasking # 如上图所示，系统在每个 CPU core 上启动一个工作线程，工作线程执行一批由 boost coroutine 实现的 user task，coroutine 调度由工作线程的 scheduler 负责，coroutine 调度开销远低于 context switch，只需 ~20 CPU 时钟周期。\ncoroutine 切换发生在 page fault（此时需要根据 page id 从磁盘读取对应的 page 上来），或者缺乏空闲 buffer frame（此时需要将内存中某些 page 写回磁盘以得到可用的空闲 buffer frame），或者锁等待（为了避免 coroutine 因为等锁而阻塞，作者修改了所有 latch 实现，使当前 coroutine 能够被切换走），或者 user task 结束。\n系统采用非阻塞 IO。例如当前 user task 发生 page fault 后会将 IO 请求提交到 IO backend，之后就暂停执行，由 scheduler 调度执行下一个 user task，等 IO 请求完成后才再次调度执行该 user task。\nBackground Work Through System Tasks # 系统中除了执行用户 query 的 user task 以外还有运行系统任务的 system task。比如 page eviction，如果不够快，user task 的 IO request 会因为等待空闲 buffer frame 而停顿。\n老版本的 LeanStore 使用后台线程 page provider 进行缓存替换，要满足百万级别的 page eviction 和 IOPS 就需要多个 page provider 线程，但需要多少 page provider 很难提前预知，尤其是工作负载发生变化的时候。\n新版本中，像 page provider 这样的后台任务和 user task 一样采用 boost coroutine 实现为 system task，由 scheduler 在工作线程上调度执行。scheduler 每次调度 task 时都会检查 free buffer frame 是否充足，并按需调度 page eviction task，如果瓶颈在缓存替换，user task 得不到调度，page eviction 就自然得到了更多 CPU 时间。\n以上图为例，scheduler 按照 runTask、submitIO、eviction、pollIO 的顺序调度用户任务和系统任务：\n调度执行 user task 1 执行其中的 B+ Tree lookup，在遍历时遇到了 page fault 将对应的 page read request 提交给 IO backend，将 task 状态置为 waitIO suspend 当前 user task，由 scheduler 继续调度下个任务 下个任务是调用 IO backend 的 submit 接口，提交 IO backend 中所有累积的 IO request 到 SSD，接着执行 eviction 任务，根据需要进行缓存替换 接着进入下一个 task 调度循环 这次执行的事 user task 2，它没有 page fault、执行过程中也没有锁等待，所有过程都在 CPU 中完成，user task 2 执行结束后，scheduler 继续调度后面的系统任务 在 pollIO 时发现上个 user task 1 的 IO request 已经执行结束，调用 callback 将其任务状态置为 IOdone 下轮调度开始，scheduler 重新调度到 user task 1，发现 IO 已经完成，于是 resume user task 1 并从上次 suspend 的地方继续执行。 Managing I/O # IO backend 是一个 async IO 封装，支持包括 libaio、io_uring 和 SPDK 在内的所有主流异步 IO 库，对所有 SSD 做了个类似 RAID 0 的封装使其看起来像是一块 SSD。没用 Linux 的 RAID 0 一方面是为了更好的性能，一方面是为了 SPDK。\n需要有专门的线程负责 IO 吗？需要每个线程负责一块 SSD 还是所有线程可以读写所有 SSD？上图展示了 3 种可能的 IO 线程模型：\nDedicated IO threads 模型和 io_uring 的 SQPOLL 模式类似，工作线程需要和 IO 线程交互，每个 IO request 都会有额外的消息传递开销。Dedicated IO threads 的问题在之前的实验中也揭示过，一方面不好确定线程池大小，另一方面这些线程也不能完全发挥 SSD IOPS。\nSSD Assignment 模型中，每个工作线程负责一个 SSD，当需要向其他工作线程所负责的 SSD 读写数据时仍然避免不了消息传递开销。IO benchmark 时通常采用这种模型，它的 cache locality 最好，polling 调用最少。但数据库负载和 IO benchmark 负载不一样，在数据库负载中可能需要读写其他线程上的 SSD 的数据，这就引入了线程同步开销，需要实验来看能不能被它带来的收益抵消。\nAll-to-All 模型中，每个工作线程都可以读写所有 SSD，为每个 SSD 都准备了独立的 submission/completion queue，作者实现了一个 IO channel 的接口用于封装底层不同的 IO 库。每个 worker 都有通过 IO channel 读写底下所有 SSD。不需要在 worker 之间消息传递和线程同步。没有任何特殊角色的线程，这也意味着系统只需 1 个 cpu core 就可以具备完整功能。\n上图测试对比了 SSD assignment 和 all-to-all 两种模型，二者性能几乎一致，由于 all-to-all 模型的各种优点，最终 LeanStore 采用了该线程模型。\nCPU Optimizations and Scalability # 当所有 IO 优化实现后，作者发现系统的 CPU 成为瓶颈了，IOPS 仍旧没有打满。于是还有不少 CPU 相关的优化需要做。\n在多核 scalability 方面锁竞争是最大的瓶颈。早期 LeanStore 设计中，采用了全局大锁保护所有正在进行的 IO 操作，但在多块高性能 SSD 的场景下，这把大锁很快就成为性能瓶颈了。作者将 inflight IO 操作按照 page id 进行分区，每个分区使用各自的小锁，通过减小锁粒度减少了锁竞争。\n另一个显著的开销是 findParent。LeanStore 采用了 pointer swizzling，在 page eviction 时需要在当前 page 的父节点中更新其状态，由于节点中没有存储 parent pointer，于是需要不断调用 findParent 从根节点开始遍历。当 IOPS 很高 page eviction 频繁时，findParent 的 CPU 开销就凸显出来了。LeanStore 采用了一种称为 optimistic parent pointer 的优化，主要思路是在当前节点缓存 parent pointer，避免重复的 findParent 开销。\nEVALUATION # 还是一开始的实验环境：AMD EPYC 7713 CPU（64 核 128 线程），512GB 内存，8 块三星 PM1733 SSD，每块 3.84TB，每块 SSD 有 1.5M 4KB 随机读 IOPS。在测试所有 SSD 时需要设置内核参数 amd_iommu=off 才能发挥所有性能。每次测试前都擦盘清零。\n测试中关闭了 wal，使用了最低可用的事务隔离级别，主要关注整系统的 IO 吞吐。所有系统都配置成 4KB page size，采用 DIRECT IO，8 byte key + 120 byte value。\nSystem Comparison # 1000 warehouse，大约 160GB，buffer pool 10GB，90% 的数据在磁盘上，LeanStore 采用 SPDK 作为 IO backend，使用 64 线程 TPS 1.07M。在 random lookup benchmark 中，key 均匀分布，WiredTiger、RocksDB 和 LeanStore 的 QPS 分别是 1.8M、2.8M 以及 13.2M。\nIncremental Performance Improvements # 这里展示了不同优化策略对 TPC-C 和 random lookup 的性能影响。\nOut-Of-Memory Scalability # buffer pool 固定在 400GB，不断增加 TPC-C 数据量， 测试了 LeanStore 在 out-of-memory 下的性能表现。当数据完全在内存中时 64 线程可实现 2M 多 TPS，当 25K warehouse，总数据 4TB 十倍于 buffer pool 时，TPC-C 仍旧有 1.1M，100K warehouse 总数据 16TB 四十倍于 buffer pool 时，TPC-C 仍旧有 400K。\nI/O Interfaces # CPU Usage # Latency # 总结 # ","date":"28 May 2024","permalink":"/posts/vldb-2023-high-perf-nvme-io/","section":"Posts","summary":"Manang, Nepal, 2024","title":"[VLDB 2023] What Modern NVMe Storage Can Do, And How To Exploit It: High-Performance I/O for High-Performance Storage Engines"},{"content":"","date":"28 May 2024","permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"28 May 2024","permalink":"/","section":"Jian Zhang","summary":"","title":"Jian Zhang"},{"content":"","date":"28 May 2024","permalink":"/categories/paper-reading/","section":"Categories","summary":"","title":"Paper Reading"},{"content":"","date":"28 May 2024","permalink":"/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"","date":"28 May 2024","permalink":"/categories/storage/","section":"Categories","summary":"","title":"Storage"},{"content":"","date":"4 May 2024","permalink":"/categories/cloud/","section":"Categories","summary":"","title":"Cloud"},{"content":"","date":"4 May 2024","permalink":"/categories/redis/","section":"Categories","summary":"","title":"Redis"},{"content":" Annapurna, 2024\nIntroduction # 这篇论文讲述了 Amazon MemoryDB for Redis 的架构设计。主要特点是基于跨 AZ 的 distributed transaction log service 和 S3 解耦 Redis 的 durability 和 availability，将 durability 和 consistency 交给共享存储，利用 transaction log 完成 leader 选举，slot 迁移等，解决了 Redis Cluster 在 durability、consistency、availability、scalability 等各方面的问题，架构也更加简约。\n论文的 “Introduction” 部分介绍了一个缓存 + 数据库的经典案例，“Background and Motivation” 部分介绍了 Redis Cluster 的架构及其各种问题，熟悉 Redis 的朋友应该非常了解了，本文不再详细介绍。\nDurability and Consistency # Decoupling Durability # 和 Aurora 类似，MemoryDB 将数据持久性（durability）和服务可用性（availability）解耦，把 Redis 作为 in-memory 的计算和存储引擎，把复制流重定向到 transaction log 用数据复制和 leader 选举，避免大量修改 Redis 代码，最大限度的兼容所有 Redis API。\nMemoryDB 采用 passive replication，数据从 primary shard 写入，经过 transaction log 同步给 replica shard。具体实现上，MemoryDB 拦截了 Redis 复制流，切割成 record 后写入 transaction log，replica shard 顺序读取和重放这些 record，最终和 primary replica 保持一致。\ntransaction log service 是一个 AWS 内部服务，具有强一致、跨 AZ 高可用、低延迟的特点。高可用方面，只有当数据同步到多个 AZ 后才对外返回写入成功，保证 11 个 9 的可用性。论文没有描述 transaction log service 的实现细节，推测是基于 EBS 做的一个 log 服务？\n数据持久性由 transaction log service 保障后，服务可用性就可按需独自扩缩容，控制成本。transaction log 大小主要取决于写入带宽，snapshot 后可以清理过期 log，资源开销很低。MemoryDB 主要成本来自 shard 节点的内存占用，客户通常只会设置 1 个 primary，或者 1 primary + 1 replica，既节省成本，也能获得 11 个 9 的高可用和持久化能力。\nMaintaining Consistency # Redis 使用异步复制机制，新 leader 不一定有最新数据，故障恢复后可能丢数据。MemoryDB 通过同步复制避免了这个问题：只有数据写入 transaction log 后才返回成功。\nMemoryDB 复用了 Redis 在操作结束后生成复制信息的机制，将复制信息作为 WBL（write-behind log）写入 transaction log。把 Redis 命令的结果而不是命令本身写入 transaction log 可以使非确定性命令比如 SPOP（随机删除 set 中的一个元素）的结果在各个副本之间保持一致。\n日志复制的一个问题是，primary 内存中修改完成，但是 transaction log 写入失败，此时不能对用户返回成功，也不能对其他并发请求可见。其他数据库系统会使用 MVCC 来解决，但 Redis 不支持 MVCC，于是 MemoryDB 添加了一个 client blocking layer 来解决这个问题。MemoryDB 会追踪每个 key 的读写请求，收到 client 的写请求后，写请求的 reply 先被缓存在 tracker 中，transaction log 写入成功后才将 reply 返回给 client。并发读按以前流程正常处理，但结束时需要访问 tracker 来检查是否需要等某个 log 写入后才能返回结果。MemoryDB 以 key 为粒度检测是否需要 block。\n通过上面的机制，读写请求在 primary shard 上满足了 strong consistency。client 也可以向 replica shard 发起 read-only 请求，replica 的数据根据 transaction log 同步而来，代表了某个 point-in-time 的 snapshot。单个 replica 上的读操作满足 sequential consistency，通过 proxy 在多个 shard replica 之上的读操作只能满足 eventual consistency。\nAvailability, Recovery and Resilience # Leader Election # Redis Cluster 中，primary shard 通过 cluster bus 广播心跳给其他 replica，当多数 replica 没有收到心跳后就认为 primary shard 遭遇不测开始重新选主。通过某种 ranking 算法选举出 most up-to-date 的新 leader。这种方式有脑裂、丢数据的风险。\nMemoryDB 通过 lease 机制确保只有一个 leader，利用 transaction log 确保只有数据完全 up-to-date 的 replica 能够选主成功，保证数据的强一致性。不再使用原来的 quorum 机制。\n选主时需要利用 transaction log service 的 conditional append API。在 transaction log 中每条记录都有唯一 ID，conditional append 要求指定需要使用的 ID 作为前置条件。\n各个 replica 竞选 leader 时利用 conditional append 写入一条特殊日志，写入成功既竞选成功，之后 leader 在 transaction log 中定期写入心跳，维持 lease，通过 Redis cluster bus 将拓扑信息广播出去。其他 replica 竞选 leader 失败后会读取 transaction log，读到 leader 的心跳记录后退出选举。\n因为 conditional append 的特殊性，只有数据完全 update-to-date 的 replica 才可能竞选成功，多个副本同时竞选也只会有一个成功，老副本加入后也只有追上数据后才能选举成功，简化了选主过程，也避免了数据一致性问题。\nRecovery # MemoryDB 通过外部的 monitoring service 和 Redis 内部 gossip 的拓扑信息进行故障检测。\n故障恢复时，先恢复到 S3 存储的最新 snapshot，然后重放增量的 transaction log。不需要和任何现存副本交互，可以并发同时恢复多个副本，也避免了影响健康节点的工作负载。\nMemoryDB 采用了下图所示的 off-box snapshot 机制在后台启动用户不可见的 off-box cluster，off-box replica 和普通副本一样从 S3 和 transaction log 中创建，之后一边消费 transaction log 一边按需创建 snapshot，对用户 cluster 无任何影响。避免了使用 Redis fork 机制的一系列问题：\n虽然 off-box cluster 增加了集群成本，但考虑到 snapshot 过程的计算和 IO 开销，使用独立的 off-box cluster 可以尽量减少对服务稳定性、可用性的影响。\nMemoryDB 根据 snapshot freshness 来判断是否产生新的 snapshot。snapshot freshness 可以认为是自 snapshot 创建以来增量 transaction log 的长度，受 write throughput 和 data set 影响。write throughput 越大 transaction log 就涨的越快，data set 越大 snapshot 的时间就越长，累积的 transaction log 也越多。MemoryDB 不间断的采样 write throughout 和 data set size，计算 snapshot freshness，在 snapshot stale 时创建新的 snapshot。\nManagement Operations # 这一小节介绍了 control plane 的功能和实现。control plane 负责客户集群的所有运维操作，比如安装、升级、扩缩容、re-sharding、故障恢复等。其中变更副本数比较简单，变更实例类型（scale-up）和变更实例版本一样采用 N+1 rolling update 的方式，变更 shard 数量（scale-out），也就是 re-sharding 采用了 2PC 的方式，会稍微复杂点。\n变更 shard 数量，也就是 re-sharding，需要在 shard 之间迁移 slot，操作结束后还需要为新增（或减少）的 shard 创建（或销毁）对应的 transaction log。slot 迁移分为 data movement 和 ownership transfer 2 个步骤：\ndata movement：需要将属于该 slot 的 key-value 和 transaction log 中的数据迁移到 target primary shard 上，迁移过程中 source primary shard 会继续提供该 slot 的读写服务。在进行 ownership transfer 之前，source primary 会阻塞该 slot 上所有新增的写请求，等待所有正在执行的写请求结束，检查 source 和 target primary 上的数据完整性，如果失败则中断本次 slot 迁移，清理数据，否则进入 slot ownership transfer 阶段。\nownership transfer：slot ownership 记录在 transaction log 中，ownership transfer 需要更新 2 个 shard 的 transaction log，算是一种分布式事务，通过 2PC 完成。所有 2PC 的状态都记录在 source 和 target 的 transaction log 中，source 或 target 故障后新的 primary shard 仍旧可以通过之前 2PC 的状态继续或 abort ownership transfer。\nPerformance Benchmark # 下图展示了 Redis Cluster 和 MemoryDB 的性能对比，左边是 read-only，右边是 write-only。MemoryDB read-only 性能好的主要原因是采用了 Enhanced IO Multiplexing 功能将多个客户端连接聚合成 1 个，提升了性能和吞吐。write-only 性能更差的原因是 MemoryDB 需要跨 AZ 的写 transaction log，在预期之中：\n下图从左到右分别展示了 read-only、write-only、read-write 三个工作负载下的延迟对比，read-only 延迟都差不多，MemoryDB write-only 延迟更高也在预期内：\n下图展示了 MemoryDB off-box snapshot 期间 QPS 和延迟的变化情况，也是如预期一般没有任何影响，证明在隔离的环境中读共享的 transaction log 对主服务的影响确实可以忽略不计，用户也不用像以前 Redis 那样为主服务集群保留额外的内存以应对 fork 的影响了：\n一些思考 # MemoryDB 和社区上 RocksDB + Raft + Redis 的方案不太一样，前者（MemoryDB）每个 shard 仍旧是 in-memory 的，后者可以利用 RocksDB 将数据落盘，牺牲性能来降低成本。不过 MemoryDB 的每个 shard 应该也可以采用类似的落盘方案进一步降低成本。\ntransaction log service 的抽象为上层服务解决了很多问题，许多系统架构应该都可以基于这一层抽象来简化，减少复杂度，提升鲁棒性。\n","date":"4 May 2024","permalink":"/posts/sigmod-2024-amazon-memory-db/","section":"Posts","summary":"Annapurna, 2024","title":"SIGMOD 2024 | Amazon MemoryDB: A Fast and Durable Memory-First Cloud Database"},{"content":"","date":"2 May 2024","permalink":"/categories/duckdb/","section":"Categories","summary":"","title":"DuckDB"},{"content":" Glacier, Tilicho Lake, Annapurna, 2024\n简介 # 本文以目前 DuckDB main 分支最新代码（commit: 670cd34）为例，介绍 DuckDB 查询优化的整体过程和部分细节。\n和大多数优化器一样，DuckDB 查询优化也可以分为逻辑优化和物理优化两个阶段。如下图所示，整个查询优化由 ClientContext::CreatePreparedStatement() 驱动完成，其中的关键步骤和组件为：\nPlanner：负责将 AST 中的 SQLStatement 转换为初始未经优化的 logical plan，对应图中第 1 个红框部分的代码 Optimizer：负责 logical optimize，对初始 logical plan 做一系列等价变换的优化后得到最终 logical plan，对应图中第 2 个红框部分的代码 PhysicalPlanGenerator：负责 physical optimize，将优化后的 logical plan 转换成最终用于执行的 physical plan，对应图中第 3 个红框部分的代码 参考代码：src/main/client_context.cpp\n从上面的代码片段可以看到，DuckDB 将逻辑优化和物理优化分成两个独立的阶段，没有额外其他优化机制，其中逻辑优化（Optimizer）可以选择性的打开或关闭，整体来说比较简单。接下来我们分别看看每个阶段的大致实现原理。\nPlanner and binder # 和大多优化器一样，DuckDB 的逻辑执行计划也是树形结构。每个节点由 LogicalOperator 表示，根节点代表了整个逻辑执行计划。每个 LogicalOperator 主要由这些部分组成：\nchildren：所有孩子节点，代表了以孩子节点为根的所有 subplan types：该算子输出的 output schema，也就是计算结果中每列的数据类型 estimated_cardinality：估算的 cardinality 参考代码：src/include/duckdb/planner/logical_operator.hpp\n将 SQLStatement 转换为这样的逻辑执行计划由 Planner::CreatePlan() 完成：\n参考代码：src/planner/planner.cpp\n实际执行由 Binder 完成，它会根据 SQLStatement 的具体类型先创建出 BoundStatement，然后根据 BoundStatement 创建由 LogicalOperator 表示的逻辑执行计划。这个过程会遍历该 SQLStatement 的每个 SQL 子句，有非常多细节需要处理，相关代码在 src/planner/binder/statement/ 目录中，感兴趣的朋友可以自行查阅，本文不再展开介绍：\n参考代码：src/planner/binder.cpp\nLogical plan optimizer # Optimizer::Optimize() 负责所有逻辑优化，这里每个启发式优化规则都是有收益的，优化规则之间的先后顺序经过精细调整固化在了 Optimizer 代码中。逻辑优化就是 Optimizer 顺序应用所有优化规则，这些优化规则按照应用的先后顺序分别是：\nEXPRESSION_REWRITER：不改变 logical plan 的算子结构，仅优化每个算子的表达式树。所有 rewrite rule 在 Optimizer 初始化时确定。expression rewrite 会不停迭代直到没有任何 rule match FILTER_PULLUP FILTER_PUSHDOWN REGEX_RANGE IN_CLAUSE DELIMINATOR JOIN_ORDER UNNEST_REWRITER UNUSED_COLUMNS DUPLICATE_GROUPS COMMON_SUBEXPRESSIONS TOP_N COLUMN_LIFETIME STATISTICS_PROPAGATION COMMON_AGGREGATE COLUMN_LIFETIME REORDER_FILTER EXTENSION 末尾的 EXTENSION 是用户自定义优化规则，在所有 built-in 优化规则执行完后，DuckDB 会枚举 config 中设置的 optimizer extensions，依次应用这些自定义优化规则：\n参考代码：src/optimizer/optimizer.cpp\n可以认为 EXTENSION 是 DuckDB 的优化器插件，允许开发者按需实现自定义的优化规则。插件接口也不复杂，下图是所有接口定义，只需要依次实现 OptimizerExtensionInfo、OptimizerExtension 以及 optimize_function_t 函数即可，可以参考 DuckDB 源码中的 demo(test/extension/loadable_extension_optimizer_demo.cpp)：\n参考代码：src/include/duckdb/optimizer/optimizer_extension.hpp\nDuckDB 支持的优化规则比较多，有些优化规则比较复杂，这里就不一一介绍了，之后有机会再单独详细分析。\nPhysical plan generator # 物理执行计划由 PhysicalOperator 表示，通过 PhysicalPlanGenerator 完成物理优化，由 PhysicalPlanGenerator::CreatePlan() 完成。和其他优化器不一样，DuckDB 的物理优化是个启发式优化过程，没有利用 cardinality 和 physical property 进行 cost-based optimization（CBO）：\n参考代码：src/execution/physical_plan_generator.cpp\n以 LogicalComparisonJoin 的物理优化过程为例，CreatePlan() 采用 bottom-up 的方式先得到子树的 estimated cardinality 和物理执行计划，最后再启发式的为当前算子构造 PhysicalOperator：\n参考代码：src/execution/physical_plan/plan_comparison_join.cpp\n构造物理执行计划时利用 estimated cardinality 选择适合的物理算子：\n参考代码：src/execution/physical_plan/plan_comparison_join.cpp\n其他算子的物理优化也采用类似的物理优化过程，这里不再详细展开。\n一些思考 # 关于 logical optimize，DuckDB 采用了手动维护顺序的启发式优化规则，好处是优化器的结果和耗时可预测，问题比较好排查。缺点也很明显，添加优化规则的心智负担大，需要考虑放在哪些依赖的优化规则之后，以及添加后对其他优化规则有什么影响，也会有 query 不能完全优化的问题。也许类似 volcano/cascades 或者 DuckDB ExpressionRewriter 那样通过 rule matching 不断迭代和优化直到没有任何 rule match 为止在灵活性和优化效果上会好点，不过这样原本的可预测性优点就没了，出问题后也比较难调查和调试，需要通过更复杂的机制（比如 planner trace）来解决。\n关于 physical optimize，DuckDB 没有采用 cost-based optimization，物理算子的物理属性也无法充分利用上，可能会存在 sub-optimal plan 的问题，比如下面这个例子中冗余的 order by 目前 DuckDB 无法消除。不过对目前的 DuckDB 来说应该不是个问题，它没有 index scan、merge aggregate、merge join 等能够提供某种顺序的物理算子，只有 order by 能够保证顺序，虽然建表时支持唯一索引、二级索引，但即使 order by 索引列也并不能利用索引来提速，哪怕是 select * from t order by a limit 1 这样的简单语句也需要扫全表后再求 TopN。这样其实只需要在逻辑优化阶段特殊优化 order by 就好了，比如下面的例子可以添加和应用一个 order by 消除的规则：\nselect * from ( select * from t order by a) t left join ( select * from s order by a) s on t.a=s.a order by t.a; DuckDB 不支持创建按照某些属性进行数据分布的表，虽然执行引擎采用了 pushed based execution model（详见： [DuckDB] Push-Based Execution Model），每个 pipeline 并发执行，但也没有引入 shuffle/exchange 算子，pipeline 的 source 和 sink 都是读写全局数据，整个 query 执行过程不会产生特定的数据分布，因此物理优化过程也不需要考虑 data distribution 这样的物理属性。\n不需要考虑 order 和 distribution 这两个物理属性，物理优化过程就可以极大的简化了，像 DuckDB 这样采用启发式方式就足够使用了，等哪天支持了 index scan/lookup、hash/range partition table，应该才有必要在优化器和执行器上利用这些 physical property 进一步优化。\n","date":"2 May 2024","permalink":"/posts/duckdb-optimizer/","section":"Posts","summary":"Glacier, Tilicho Lake, Annapurna, 2024","title":"DuckDB | Query Optimizer"},{"content":"","date":"2 May 2024","permalink":"/categories/query-optimization/","section":"Categories","summary":"","title":"Query Optimization"},{"content":" 那玛峰，2023\n简介 # 通常商用数据库都有 reversion-based plan correction（RBPC）机制来避免 plan regression，比如 SQL Server APC、Oracle SPM 等：从执行过的 plan 中挑出 execution cost 最低的，强制优化器使用该 plan 以避免 regression。\n论文中提出的 Plan Stitch 可以看做是 RBPC 的增强：将搜索空间限制在历史上执行过的 physical plan 上，利用算子级的 execution cost 和类似 System R 的搜索方法，“缝合”出 execution cost 最低的 stitched plan。\n相比 RBPC，即使某个历史 plan 因为某个表发生了 schema change 导致整体不可用，但只要它的某个 subplan 是有效的，这样的 subplan 也可以被 plan stitch 利用上。\n相比基于 query feedback 的查询优化，plan stitch 将搜索空间限制在历史执行过的 physical plan 上，不会产生新的、未执行过的 subplan，这个特点使它比 feedback 更适合那些对 plan 稳定性要求高的场景。\nplan stitch 开销很低，即使没有 plan regression 也可以在日常使用 plan stitch 辅助优化器基于历史生成更优的执行计划。\n作者在 SQL Server 上利用 TPC-DS 和三个真实负载对 Plan Stitch 进行了充分的测试验证，从测试结果来看 plan stitch 在广度上能比 RBPC 优化更多的 query，在深度上能大幅减少 query 的 execution cost。经过 RBPC 优化后的查询中，83% 都能经过 plan stitch 进一步优化，减少至少 10% 的 execution cost，甚至有些 query 能减少 2 个数量级的 execution cost。\n问题描述 # 通常来说，我们希望优化器能够利用当前的索引和统计信息，稳定的生成高效的执行计划。但因为各种原因，有时同一个 SQL 生成的执行计划可能比之前更差，也就是发生了 plan regression。\n这个问题在 SQL Server 中可以通过 Automatic Plan Correction (APC) 来解决，其他数据库也有类似的机制，比如 Oracle 的 SQL Plan Management (SPM)。APC 是一种保守的机制，主要思路是统计 SQL 的历史执行计划和执行时间，强制优化器采用历史上执行效率最高的 plan，避免新 plan 带来的 regression 风险。\nAPC 这样的 reversion-based plan correction（RBPC）机制风险低，大部分情况下非常有效，但也有一定的缺陷：以 plan 粒度的修正方式限制了发现更优 plan 的可能性。\n以上图 A、B、C、D 四表 join 为例。它的历史执行计划 p1 如图 a，全部采用 hash join，后来用户为表 B 和 D 创建了能被 nested loop join 使用的索引，优化器产生了新的执行计划 p2 如图 b。执行后发现 p2 出现了性能回退，实际上 p1 的 execution cost 比 p2 更低。按照 RBPC 的策略该查询的 plan 回退到了 p1。\n但从 p2 的 execution cost statistics 可以看出，对 D 表执行 index seek + nested loop join 的 execution cost 比 p1 的 scan + hash join 更低。如果像 p1 那样对 A、B、C 采用 hash join，像 p2 那样对 D 表采用 nested loop join，就能得到一个比 p1 更优的 plan，也就是图 c 所示的 p3。\n这就是 plan stitch 的目标：根据历史上执行过的 physical plan 和算子级的 execution cost 信息，缝合出 execution cost 最低的 stitched plan。和 APC 一样，Plan Stitch 也是一种保守的机制，仅在历史上执行过的 physical plan 所限制的搜索空间中根据 execution cost 组合最优 plan，发生 regression 的风险很低。\nPlan Stitch 整体架构 # Plan Stitch 整体架构如上图所示，plan stitch 实现为优化器的扩展组件，可以是外部服务，也可以是后台线程。输入为用户的 query、存储在 execution data repo 中（sql server 的 query store）的历史 plan、算子级的 execution cost 等信息，以及存储在 metadata 中的 schema、index 等信息。输出为加上 hint 后的 query，也就是论文所说的 “stitched plan”。\n任何时候，只要 execution data repo 中发现该 query 拥有多个历史 plan 和对应的 execution statistics 就可以为其计算 stitched plan，然后将 stitched plan 交给优化器，通过现有的 RBPC 机制（比如 sql server 的 APC、oracle 的 SPM）来决定是使用当前优化器实时优化出来的 plan 还是这个 stitched plan。\nplan stitch 依赖 execution data 计算 execution cost，为了确保代价估算的准确度，这些历史上的 execution data 需要保持一定的新鲜度。和直方图、ndv 这些统计信息一样，在表的数据量、数据分布发生变化后也需要淘汰过期的 execution plan 和他们的 execution data，和统计信息的更新机制一样有很多启发式机制可以参考。\n对于参数化的 sql 或者 prepare 语句，plan stitch 和 RBPC 一样存储和使用所有 plan 的 avg execution cost，也可以根据需要采用带权重的 cost 计算方式。\n构造搜索空间 # plan stitch 需要被限制在所有历史 plan 的 subplan 组成的搜索空间中，要根据一堆历史 physical plan 逆向构造出这个搜索空间有两个难点：一个是如何判断出两个 subplan 相等，一个是如何 encode 这个搜索空间。\n判断 subplan 相等采用的方法和物化视图、公共子表达式优化等类似的启发式方法。然后采用了类似 volcano/cascades planner 的方式把整个搜索空间 encode 成了论文中提出的 AND-OR graph。对照 volcano/cascades planner 来看的话，整个搜索空间其实就是由 expression group 和其中的 expression 构成。文中每个 or node 代表一个 logical plan + required physical property，每个 child and node 代表能满足该 required physical property 的 physical plan。不难看出，每个 or node 的 child 一定是 and node，而 and node 的 child 也一定是 or node。\n构造这个 AND-OR graph 的方法比较简单直接：对每个 physical plan（图中的 AND node），从所有历史 plan 中找出和它等价的 subplan（也是 physical plan，图中的 AND node），然后为所有这些 AND node 构造一个 OR node，代表这组相同的 physical plan 对应的 logical plan 和 physical property。\nplan 的 leaf node 代表了某个表的 access path，如果这个 access path 使用了被 drop 的索引，那么在构造这个 AND-OR graph 时就不能把这个 leaf node 加入进去，确保基于 AND-OR graph 搜索出来的 stitched plan 的有效性。一个 AND-OR graph 的例子如下图所示： Bottom-up 的搜索方式 # Plan Stitch 和 System R 一样采用 bottom up 的方式搜索 stitched plan：自底向上为每个 or node 计算出 execution cost 最低的 stitched subplan，直到最后计算出根节点 or node 的 stitched plan。整个搜索算法的伪代码如下所示：\n搜索时需要估算 stitched subplan 的 cost，文中采用的方法如下，stitchedSubUnitCost 是用来为 op（physical subplan）估算 cost 的函数，它依赖的输入有 4 个：\nopCost：observed execution cost，根据历史 plan 中该算子的 execution cost 计算而来 execCount：该算子在原 plan 中被执行的次数，一般来说都是 1，除非它是原 plan 中 Nested Loop Join 或 Apply（Hyper 中也叫 Dependent Join，用来做计算子查询的算子）的 inner 端 childSubUnitCost：child stitched subplan 的 cost childExecCount：child subplan 被执行的次数 整体来看和普通优化器的 cost model 差不太多，只不过 cost 换成了 execution cost 而不是根据直方图、ndv 等估算，每个算子应该都需要根据自己的计算逻辑和这些输入信息计算最终的 cost。\n实验结果 # 上图是 plan stitch 相比 RBPC 带来的性能提升和性能 regression。作者根据 TPC-DS 和 3 个 customer workload 设计了测试用例。图 4-7 展示了 plan stitch 相比 RPBC 减少的 cpu time 百分比，计算方式为 100*(cpu_time_stitched-cpu_time_rbpc)/cpu_time_rbpc。\n图 8 展示了因为估算误差导致 stitched plan 相比 RBPC 出现性能回退的 query 占比，作者只提了回退至少 10% 的 query 的占比低于 2.7%，但是没有提这 2.7% 的 query 里面最差的回退有多少，如果要实际使用 stitched plan 还是得多测试，除了关注回退数量也关注下最差的回退情况。\n原文第 5 结 “EXPERIMENT” 从各个方面对 plan stitch 进行测试评估，这篇文章就不再详细介绍了，感兴趣的朋友可以自行阅读：\nPlan Quality (Section 5.2). How much improvement in plan execution cost does Plan Stitch bring compared to RBPC? How much is the risk of plan regression using Plan Stitch? Cost Estimation (Section 5.3). How accurate is the stitched plan’s estimated execution cost compared to true execution cost? Coverage (Section 5.4). How many queries and plans can Plan Stitch improve? Overhead (Section 5.5). What is the overhead of Plan Stitch? Stitched Plan Analysis (Section 5.6). How different is the stitched plan compared to the optimizer’s plan? How many previously-executed plans are used for the stitched plan? Why does the optimizer miss the cheaper stitched plan in its optimization? Parameterized Queries (Section 5.7). How much does Plan Stitch improve in aggregated execution cost of query instances? Data Changes (Section 5.8). How much does cost estimation in Plan Stitch degrade when data changes? 总结 # 总的来说，在保证 plan 稳定性的前提下尽可能提升执行性能，plan stitch 这种基于历史同时限制搜索空间的方法是个非常值得借鉴的思路。\n","date":"22 April 2024","permalink":"/posts/vldb-2018-plan-stitch/","section":"Posts","summary":"那玛峰，2023","title":"[VLDB 2018] Plan Stitch: Harnessing the Best of Many Plans"},{"content":" 乌孙古道，2023\nIntroduction # 这篇论文中，作者利用 C++ 20 引入的 coroutine 特性将 thread-to-transaction 的执行模型改为 2-level coroutine-to-transaction，并在此基础上实现了基于协程的 software prefetch 机制，减少了后续计算的 cache miss，提升了事务的整体执行性能。相比已有的 group prefetching、software pipelined prefetching 以及 asynchronous memory access chaining (AMAC) 这三种 software prefetch 来说，基于协程的 software prefetch 不需要过多的修改数据结构的接口和内部实现，在代码实现和性能收益之间做了非常不错的平衡，对数据库或高并发系统性能优化有一定的参考价值。\nPS：其实这篇论文很早就关注了，一直想抽时间看看，结果一拖就是 2 年多。。\n关于 CPU prefetch，我最早是通过 《 Relaxed Operator Fusion for In-Memory Databases: Making Compilation, Vectorization, and Prefetching Work Together At Last》了解到的，后面又通过《 Improving Hash Join Performance through Prefetching》以及《 Asynchronous Memory Access Chaining》了解了 group prefetching、software pipelined prefetching 以及 asynchronous memory access chaining (AMAC) 的细节，感兴趣的朋友也可以顺便读一下这几篇论文。\n在介绍 CoroBase 前，我们先了解下基于协程的 software prefetch 的基本思路和一些已有研究结果。\nSoftware Prefetching via Coroutines # 基于协程的 software prefetch 思路比较简单直接：当协程函数 t1 要通过指针访问数据时，它可以向 CPU 发起 prefetch 指令让 CPU 把这块数据从内存加载到 cache 中，因为加载数据需要时间，在这个协程发完 prefetch 指令去捞数据的时，可以将当前协程挂起，让 CPU 继续执行其他数据已经在 cache 中的协程的计算任务，这样 IO（CPU prefetch）和计算完全并行起来，cache miss 减少，整体的执行效率也提升了。\n简单来说，基于协程的 software prefetch 只需要在指针解引用之前加上 prefetch 和 suspend 就可以了，极大简化了开发负担。\n之前也有人研究过的基于协程的 software prefetch，比如采用 thread-to-transaction 的处理模型，在事务内采用多个协程并发读写一批数据，实现 software prefetch。这种实现方式对应用层有一定的侵入性，如下图所示，用户需要从 (a) 中的 get 接口改成 (b) 中的 multi_get 接口才能享受到这种事务内部多个协程批量处理多条数据的 software prefetch 方式所带来优化收益：\n另外，之前的研究大多是针对具体的计算场景比如 hash join，index lookup 等进行 software prefetch 优化，缺乏整个事务全部采用 software prefetch 优化的研究和实验。\nC++20 Stackless Coroutine # 基于协程的 software prefetch 虽然减少了 cache miss，但引入了协程切换开销，要想有收益就需要使协程切换开销低于最后一级 cache miss 的开销。\n第三方协程库，比如微软和 boost 协程库等，都是有栈协程（stackfull coroutine），因为 runtime stack 的原因导致有栈协程的切换开销比 L3 cache miss 高许多，利用它们实现本文的 software prefetch 方案不会有直接收益。\n作者采用了 C++ 20 引入的无栈协程（stackless coroutine），它的协程上下文保存在堆内存的 coroutine frame 中，切换开销非常小。\n关于有栈协程和无栈协程的实现原理和各自的优缺点这里不过多介绍，我了解的也比较有限，大家可以查阅维基百科，或者查阅这份关于 C++ coroutine 的 Draft Technical Specification 详细了解。\nCoroBase Design # 作者在内存数据库 ERMIA 的基础上实现了 CoroBase，代码开源在 https://github.com/sfu-dis/corobase。CoroBase 采用了 coroutine-to-transaction 的处理模型，每个工作线程运行多个协程，每个协程处理一个事务，每个线程上的一批事务共同开始，共同结束。当事务 t1 需要 prefetch 数据时，它所在的协程发起 prefetch 后挂起，其工作线程继续执行事务 t2 的协程。\nOverview # CoroBase 采用了 MVCC，每个 record 通过 RID 来唯一定位，RID 和 record 的版本链存储在 indirection array 中，版本链按照从新到旧的顺序存储所有的数据版本。为了加速数据读取，CoroBase 为每个表维护了一个索引，存储了索引 key 到 RID 的映射关系，一次 index lookup 的过程涉及非常多的指针跳转。CoroBase 的整体架构以及它的 coroutine-to-transaction 执行模式如下图所示：\n每个工作线程都运行了一个 round-robin 的协程调度器，每次处理一批协程，不断轮训每个协程的状态，唤醒和执行没有完成的协程直到所有协程都执行完为止。batch_size 和 CPU 支持的 outstanding memory access 相关，CoroBase 使用 8 作为 batch_size：\n将一个 thread-to-transaction 的内存数据库改造成 coroutine-to-transaction 的所有改动如下，durability 方面不需要任何修改，最主要的挑战来自协程切换开销、并发的线程和事务之间的资源管理和同步等，作者分成了三个部分逐一介绍：\nTwo-Level Coroutine-to-Transaction # 要支持所有类型的数据库操作（insert/read/update/scan/delete），最直接的办法就是根据经验和 profile 结果将所有可能导致 cache miss 的地方都转成无栈协程进行 software prefetch，利用协程改写所有需要进行 prefetch 的函数，这种方式下可能产生非常深的协程调用链，发生非常多的协程切换。\n为了解决这个问题，CoroBase 采用了 2-level coroutine-to-transaction 的执行方式。第一级协程作为事务执行的驱动函数，串联事务处理的所有过程，调用其他函数或协程完成事务的最终执行。第二级协程用来优化某个步骤中所有可能发生 cache miss 的地方，所有需要改成 coroutine 的函数都通过手动 inline 或编译器 inline 的方式拍平在这个 coroutine 中。将整个 coroutine 调用链深度限制在了 2 层，以此减少协程切换开销。\n关于第二级 coroutine 的编码成本：大多数代码手动 inline 的开销并不大（确实见过许多为了不必要的抽象和非常短小的代码），另外也可以借助编译器工具使得用户只需要写模块化的代码，由编译器来完成这样的 source-to-source 的代码转换。\n这样的 code flattening 也不完全没有坏处，因为相同的代码片段被内嵌到不同的协程中，比如 update 和 read 时采用的树遍历算法，会导致 CPU 的 instruction cache miss 上升，coroutine 越大 instruction cache miss 可能越多，不过从作者后面的测试来看整体上仍然是有收益的，看来这部分影响还是比较小。\nResource Management # 资源管理通常和事务的执行模型是强相关的。在 thread-to-transaction 的执行模式下，线程本地的所有资源都可以认为都属于该事务，而在 coroutine-to-transaction 的执行模式下，这些资源可能会被并发执行的多个事务共同修改。作者列举了两个比较常见的问题和他们的修改方式，一个是基于 epoch 的内存回收，一个是 thread-local storage。\n许多内存数据库都采用了无锁数据结构降低锁竞争提升多核 scalability，对于一块内存来说，为了在不影响其他事务的情况下将其安全的删除，可以采用基于 epoch 的内存回收策略（比如之前分享过的 leanstore、umbra 系列论文中也有类似的机制用于回收不再使用的内存 btree page）。大概思路是每个线程周期性推进自己的 epoch，根据 epoch 判断数据的可见性，当数据对所有线程/事务不可见时即可安全回收。\nCoroBase 采用了一种简单不易出错的解决办法：在所有事务的协程处理之前推进一次 epoch，在所有事物结束之后再推进一次 epoch，等这一批事务都执行完后这些不需要的数据就能够回收了。这一部分对应在上面的伪代码中的 enter_epoch 和 exit_epoch。\nthread-local storage 的问题解决起来比较简单，基本上所有 thread-local 变量都可以看做是 transaction-local 的，因此 CoroBase 将每个 thread-local 变量转换成大小为 batch_size 的 thread-local 数组，分配给相应的事务独立使用。因为 batch_size 很小，所以这部分内存增长也还能够接受。\nConcurrency Control and Synchronization # 如果仅实现 snapshot isolation，从 thread-transaction 模型改成 coroutine-transaction 模型不需要做任何修改，因为并发事务需要访问的数据结构都是通过锁或原子变量保护起来的。\nCoroBase 采用了 serial safety net（SSN）的方式提供 Serializability，它会追踪所有事务的依赖关系，abort 可能违反序列化隔离级别的事务。要做的修改和 thread-local storage 一样，将每个 thread-local 的 bitmap 改成 transaction-local。不过因为能够同时执行的事务变多了，可能事务冲突和 abort 也会变多，这个优化就留给将来了。\n在数据更新时，对索引、版本链这样的 structural modification operation（SMO）过程中，因为性能瓶颈不在 cache miss（作者测下来占总体开销的 6% 以内），作者没有为这类 SMO 操作进行 prefetch 优化。\nEvaluation # 作者采用 8 字节的 key 和 value 分别测试了 YCSB、TPC-C、TPC-CR 和 TPC-CH 等 benchmark，可以重点关注一下 OLTP 的性能测试和对 TPC-CH 的影响\n从最基础的测试开始，作者对比了不采用 software prefetch 以及各种方式实现 software prefetch 后的性能对比如下，可以看到不采用任何 software prefetch 的 Navie/ERMIA 性能最差，利用 AMAC 手动改写数据结构后的 AMAC-MK 效果最好，相比 Navie 方式有 2.3× 到 2.96× 的性能提升，将所有 coroutine 拍平以及利用 multi key 接口（最小化 coroutine 切换开销换取最佳 prefetch 收益）的优化方案 CORO-MK 相比完全将所有嵌套函数改成嵌套 coroutine 的方案也有 17% 的性能提升，验证了 two level coroutine to transaction 机制的有效性：\n从 TPC-C read-only 负载的测试结果来看，采用 2-level coroutine-to-transaction 机制的CoroBase 性能相比采用 1-level coroutine-to-transaction 和 mutli-key 接口的实现方式来看性能接近，而 CoroBase 因为不需要采用 multi-key 接口，对应用层更加友好，进一步验证了 two level coroutine to transaction 机制的有效性：\n除了读以外，作者也测试了 update 的场景，因为 update 需要先读再修改再写回去，cache locality 天然会更好一些，不过 corobase 依然相比不使用 prefetch 的 navie 方式有 1.45× 到 1.77× 的性能提升：\n作者还进行了许多其他方面的测试，比如 TPC-CR、TPC-CH 等，感兴趣的朋友们可以阅读原论文 EVALUATION 这一章节获取更详细的测试结果。\nSummary # 总的来说 CoroBase 这篇论文提出的基于协程的 software prefetch 方案效果是非常不错的，虽然相比 AMAC 这种手写 hash table 等数据结构的方式在协程切换上有一定的性能开销，并且手动拍平需要嵌套调用的 coroutine 成 2 阶段 coroutine 调用也有一定的实现成本，但它在性能上相比不进行任何 prefetch 的方式有非常明显的收益，且在代码实现方面相比 AMAC 又要简单许多，在性能收益和代码实现方面取得了不错的平衡。不过毕竟它要大动整个数据库的执行框架，要在现有系统中落地实现也有不小的风险。\n","date":"22 July 2023","permalink":"/posts/vldb-2020-corobase/","section":"Posts","summary":"乌孙古道，2023","title":"[VLDB 2020] CoroBase: Coroutine-Oriented Main-Memory Database Engine"},{"content":"","date":"22 July 2023","permalink":"/categories/cpu-prefetch/","section":"Categories","summary":"","title":"CPU Prefetch"},{"content":"","date":"22 July 2023","permalink":"/categories/memory-database/","section":"Categories","summary":"","title":"Memory Database"},{"content":" 乌孙古道，2023\n简介 # MySQL 查询优化是一个非常复杂的过程，本文以 TPC-H Q4 为例，通过 optimizer trace 尽量系统性的总结 MySQL 8.0.31 查询优化过程，方便将来了解 MySQL 查询优化细节能够按图索骥。\nTPC-H Q4 及其执行计划 # TPC-H Q4 如下，它包含了分组聚合和子查询，是个稍微有点复杂的 SQL，在过滤 orders 表时使用了一个 exists 子查询：\nSELECT O_ORDERPRIORITY, COUNT(*) AS ORDER_COUNT FROM ORDERS WHERE O_ORDERDATE \u0026gt;= DATE \u0026#39;1993-12-01\u0026#39; AND O_ORDERDATE \u0026lt; DATE \u0026#39;1993-12-01\u0026#39; + INTERVAL \u0026#39;3\u0026#39; MONTH AND EXISTS ( SELECT * FROM LINEITEM WHERE L_ORDERKEY = O_ORDERKEY AND L_COMMITDATE \u0026lt; L_RECEIPTDATE ) GROUP BY O_ORDERPRIORITY ORDER BY O_ORDERPRIORITY; 通过 explain format=tree 可以看到如下的执行计划。总结来说是先让 orders 表和 lineitem 表进行 nested loop semi join，然后对 join 结果进行分组聚合，最后对聚合结果进行排序，输出结果给客户端：\n-\u0026gt; Sort: ORDERS.O_ORDERPRIORITY -\u0026gt; Table scan on \u0026lt;temporary\u0026gt; -\u0026gt; Aggregate using temporary table -\u0026gt; Nested loop semijoin (cost=391601.89 rows=216090) -\u0026gt; Filter: ((ORDERS.O_ORDERDATE \u0026gt;= DATE\u0026#39;1993-12-01\u0026#39;) and (ORDERS.O_ORDERDATE \u0026lt; \u0026lt;cache\u0026gt;((DATE\u0026#39;1993-12-01\u0026#39; + interval \u0026#39;3\u0026#39; month)))) (cost=160543.89 rows=165020) -\u0026gt; Table scan on ORDERS (cost=160543.89 rows=1485477) -\u0026gt; Filter: (LINEITEM.L_COMMITDATE \u0026lt; LINEITEM.L_RECEIPTDATE) (cost=1.32 rows=1) -\u0026gt; Index lookup on LINEITEM using PRIMARY (L_ORDERKEY=ORDERS.O_ORDERKEY) (cost=1.32 rows=4) 关于上面的执行计划我们重点关注 2 个问题：\nMySQL 是在查询优化的哪个步骤将 exists 子查询改写成 semi join 的？ MySQL 代价估算是在什么时候完成的？ Optimizer Trace 用法和原理 # MySQL Optimizer Trace 的详细用户文档见《 The Optimizer Trace》。为了 trace Q4 的优化过程，我们需要在执行 SQL 前把当前 session 的 optimizer_trace 打开，执行 SQL 后在 information_schema.optimizer_trace 表中获取 MySQL 为其生成的 trace 信息：\nset @@optimizer_trace=\u0026#34;enabled=on\u0026#34;; source q4.sql select * from information_schema.optimizer_trace\\G MySQL Trace 的原理非常简单，就是在代码关键路径中埋点，将需要 trace 的信息通过诸如 Opt_trace_array 的形式添加到 optimizer trace 结果的 json 对象中，比如：\nOpt_trace_context *const trace = \u0026amp;thd-\u0026gt;opt_trace; Opt_trace_object trace_wrapper(trace); Opt_trace_object trace_optimize(trace, \u0026#34;join_optimization\u0026#34;); trace_optimize.add_select_number(query_block-\u0026gt;select_number); Opt_trace_array trace_steps(trace, \u0026#34;steps\u0026#34;); Optimizer Trace 结果简介 # Q4 的 Optimizer trace 结果 trace.json 是一个很大的 json 文本，可以使用能够折叠 json 对象的编辑器或在线网站来辅助分析这个 json 文本。从 trace 结果来看，这条 SQL 在 MySQL 中先后经历了下面 3 个阶段。\n阶段 1：join_preparation。位于 sql_resolver.cc 的 Query_block::prepare() 函数中，主要功能是解析 AST 上的各个 SQL 子句，同时也完成子查询相关的转换和优化，比如转换成 semi join，推导 table information，常量消除，冗余表达式消除等，干的事情比较杂。\n阶段 2：join_optimization。位于 sql_optimizer.cc 的 JOIN::optimize() 函数中，该函数包含了查询优化的主要逻辑，通过一系列逻辑等价的 query rewrite，cost based join optimization，rule-based access path selection 等优化步骤将 Query_block 优化成 query execution plan（QEP）。\n阶段 3：join_execution。位于 sql_union.cc 的 Query_expression::ExecuteIteratorQuery() 函数中，主要负责查询执行。\n接下来我们简单看看各个阶段的主要优化步骤，尝试回答一开始提出的几个问题。\n阶段 1：join_preparation # join_preparation 的入口是 Query_block::prepare()，它驱动了所有子过程。\nsetup_conds # 第 1 个 step 就是 exists 子查询所在的 Query_block 对应的 \u0026ldquo;join_preparation\u0026rdquo;，从 gdb 可以看到完整的调用路径：\nQuery_block::prepare sql_resolver.cc:300 -\u0026gt; Query_block::setup_conds sql_resolver.cc:1690 -\u0026gt; Item_cond::fix_fields item_cmpfunc.cc:5505 -\u0026gt; Item_subselect::fix_fields item_subselect.cc:547 -\u0026gt; SubqueryWithResult::prepare item_subselect.cc:2971 -\u0026gt; Query_expression::prepare sql_union.cc:753 -\u0026gt; Query_block::prepare sql_resolver.cc:438 -\u0026gt; opt_trace_print_expanded_query opt_trace2server.cc:271 Q4 的 exists 子查询位于 WHERE 子句中，它的 Query_block 是在外层 Query_block 的 setup_conds 阶段被 prepare 的，调用入口是：\n// Set up join conditions and WHERE clause if (setup_conds(thd)) return true; opt_trace_print_expanded_query # Query_block::prepare sql_resolver.cc:438 -\u0026gt; opt_trace_print_expanded_query opt_trace2server.cc:271 处理完 WHERE 字句后，会依次处理其他 Query_block 的各个部分。当处理完当前 Query_block 的所有子查询后，通过 opt_trace_print_expanded_query() 将 expanded query 打印到 optimizer trace 中，即第 2 步的 \u0026ldquo;expanded_query\u0026rdquo; 中所看到的 SQL。从结果来看，exists 子查询仍然没有被改写，里面的关联表达式 LINEITEM`.`L_ORDERKEY` = `ORDERS`.`O_ORDERKEY 也仍然存在：\n/* select#1 */ select `ORDERS`.`O_ORDERPRIORITY` AS `O_ORDERPRIORITY`,count(0) AS `ORDER_COUNT` from `ORDERS` where ((`ORDERS`.`O_ORDERDATE` \u0026gt;= DATE\u0026#39;1993-12-01\u0026#39;) and (`ORDERS`.`O_ORDERDATE` \u0026lt; (DATE\u0026#39;1993-12-01\u0026#39; + interval \u0026#39;3\u0026#39; month)) and exists(/* select#2 */ select 1 from `LINEITEM` where ((`LINEITEM`.`L_ORDERKEY` = `ORDERS`.`O_ORDERKEY`) and (`LINEITEM`.`L_COMMITDATE` \u0026lt; `LINEITEM`.`L_RECEIPTDATE`)))) group by `ORDERS`.`O_ORDERPRIORITY` order by `ORDERS`.`O_ORDERPRIORITY` flatten_subqueries # Query_block::prepare sql_resolver.cc:548 -\u0026gt; Query_block::flatten_subqueries sql_resolver.cc:3951 -\u0026gt; Query_block::convert_subquery_to_semijoin sql_resolver.cc:2968 Query_block::prepare() 中进行 semi join 改写的接口调用如上所示，exists 子查询在之后的 flatten_subqueries() 中被改写成了 semi join，其对应的 optimizer trace 记录在了 \u0026quot;transformation_to_semi_join\u0026quot; 这个 json object 中。本文不详细分析子查询的改写过程，知道这个入口就行。\nif (has_sj_candidates() \u0026amp;\u0026amp; flatten_subqueries(thd)) return true; apply_local_transforms # Query_block::prepare sql_resolver.cc:592 -\u0026gt; Query_block::apply_local_transforms sql_resolver.cc:767 -\u0026gt; Query_block::simplify_joins sql_resolver.cc:2168 apply_local_transforms() 由最外层的 Query_block 发起，然后递归的对内层 Query_block 调用 apply_local_transforms() 完成所有 Query_block 的本地优化，是一个 top-down 的优化过程，包含了一些常见的 top-down 的优化，比如 column pruning、partition pruning、outer join 转 inner join、ONLY_FULL_GROUPBY validation、predicate pushdown 等。\nsimplify_joins() 主要的功能是 outer join 转成 inner join，为后续 join reorder 提供更多可能性，它总共包含了 4 种可能的 transformation：OUTER_JOIN_TO_INNER、JOIN_COND_TO_WHERE、PAREN_REMOVAL、SEMIJOIN，optimizer trace 中看到的第 4 个 step 就对应了 SEMIJOIN，如果还有其他类型的转化发生，也会一并记录在 \u0026quot;transformations\u0026quot; 这个数组里。\n阶段 2：join_optimize # join_optimize 的入口是 JOIN::optimize()，它驱动了所有子过程。接下来我们将看到 Q4 经历的各个 join_optimize 子过程。\noptimize_cond # JOIN::optimize sql_optimizer.cc:472 -\u0026gt; optimize_cond sql_optimizer.cc:10250 optimize_cond() 主要优化查询的 where 和 having 条件，对应 optimizer trace 中的 \u0026ldquo;condition_processing\u0026rdquo; 部分。在 optimize_cond() 中，主要完成以下优化：\nequality propagation，利用 x=y, y=z 推导出 x=y=z，将普通的二元等式转换为多元等式 (x, y, z, \u0026hellip;) constant propagation，在 equality propagation 完成后，进行常量传播，例如根据 x=42 推导出所有多元等式 (x, y, z, \u0026hellip;) 中的字段都等于 42 trivial condition removal，在常量传播后，消除所有始终为真或始终为假的 predicate substitute_gc # JOIN::optimize sql_optimizer.cc:599 -\u0026gt; substitute_gc sql_optimizer.cc:1192 substitute_gc 主要检查 query 的 where 条件、order by 等，将其中的表达式替换为匹配的 generated column。尽管 optimizer trace 中提到了这一信息，但由于 Q4 中涉及的 LINEITEM 和 ORDERS 表都没有 generated column，这个优化并未生效。\nmake_join_plan # JOIN::optimize sql_optimizer.cc:694 -\u0026gt; JOIN::make_join_plan sql_optimizer.cc:5284 -\u0026gt; trace_table_dependencies sql_optimizer.cc:6246 MySQL 8.0 新增的 hypergraph join order 算法默认关闭。下面这些信息在 optimizer trace 中属于老的 join order 算法：\ntable_dependencies ref_optimizer_key_uses pulled_out_semijoin_tables rows_estimation execution_plan_for_potential_materialization considered_execution_plans MySQL 首先会在 \u0026ldquo;rows_estimation\u0026rdquo; 中估算每个基表和 JOIN 的基数，并在后续的优化过程中持续计算每个候选执行计划的 cost。在 \u0026ldquo;considered_execution_plans\u0026rdquo; 中，我们可以看到所有候选执行计划及其 cost。\noptimizer trace 中有许多由 make_join_plan 暴露的详细信息，对于我们进一步了解 join order、access path 以及 cost estimation 非常有帮助。由于篇幅所限，我们在此不再详细展开，将来有机会的话再单独介绍这个优化过程。\nmake_join_query_block # JOIN::optimize sql_optimizer.cc:779 -\u0026gt; make_join_query_block sql_optimizer.cc:9577 make_join_query_block 是 MySQL 在完成 join order 后的一个优化过程，其主要目的是尽早过滤掉不需要的中间结果，将包括 join 的 on condition 在内的所有 predicate 尽可能地下推。optimizer trace 中的 \u0026ldquo;attaching_conditions_to_tables\u0026rdquo; 反映了这一过程中下推到各个表上的 predicate。\noptimize_distinct_group_order # JOIN::optimize sql_optimizer.cc:794 -\u0026gt; JOIN::optimize_distinct_group_order sql_optimizer.cc:1464 完成 join 相关的优化后，MySQL 在 optimize_distinct_group_order() 中继续对 distinct、group by 和 order by 进行优化，例如将 distinct 转换为 group by，消除不必要的 trivial order by 等，这里面的细节也非常多，对应了 optimizer trace 中的 \u0026ldquo;optimizing_distinct_group_by_order_by\u0026rdquo; 部分。\nfinalize_table_conditions # JOIN::optimize sql_optimizer.cc:1015 -\u0026gt; JOIN::finalize_table_conditions sql_optimizer.cc:9108 进行最后一轮的 condition 优化，这一步主要是去除冗余的 filter，将缓存表达式中的常量，避免每一行数据都重新计算等，对应了 optimizer trace 中的 \u0026ldquo;finalizing_table_conditions\u0026rdquo; 部分。\nmake_join_readinfo # JOIN::optimize sql_optimizer.cc:1018 -\u0026gt; make_join_readinfo sql_select.cc:3112 做执行前的 plan 调整比如分配 join buffer，里面的内容比较杂，对应了 optimizer trace 中的 \u0026ldquo;refine_plan\u0026rdquo; 部分。\nmake_tmp_tables_info # JOIN::optimize sql_optimizer.cc:1021 -\u0026gt; JOIN::make_tmp_tables_info sql_select.cc:4219 这是 MySQL 查询优化的最后一步，为执行计划中各个 SQL 算子按需分配 tmp table，对应 optimizer trace 的 \u0026ldquo;considering_tmp_tables\u0026rdquo; 部分。\n阶段 3：join_execution # 比较有意思的是 MySQL optimizer trace 中还记录了查询执行过程的一些信息，例如 \u0026ldquo;temp_table_aggregate\u0026rdquo;，它的调用链路如下，从这可以看到 MySQL 查询执行的驱动过程，这部分内容我们不再展开：\nSql_cmd_dml::execute_inner sql_select.cc:799 -\u0026gt; Query_expression::execute sql_union.cc:1823 -\u0026gt; Query_expression::ExecuteIteratorQuery sql_union.cc:1763 -\u0026gt; SortingIterator::Init sorting_iterator.cc:444 -\u0026gt; SortingIterator::DoSort sorting_iterator.cc:531 -\u0026gt; filesort filesort.cc:408 -\u0026gt; TemptableAggregateIterator\u0026lt;DummyIteratorProfiler\u0026gt;::Init composite_iterators.cc:1680 总结 # MySQL 8.0.31 在 optimizer trace 中新增了查询优化和执行的信息，特别是 make_join_plan 的细节。结合 MySQL 代码，optimizer trace 对我们理解执行计划生成过程非常有帮助。基于 MySQL 二次开发也可以扩展 optimizer trace 提升系统的可观测性。\n","date":"1 July 2023","permalink":"/posts/mysql-optimizer-trace/","section":"Posts","summary":"乌孙古道，2023","title":"[MySQL 8.0] 通过 Optimizer Trace 概览查询优化"},{"content":"","date":"1 July 2023","permalink":"/categories/mysql/","section":"Categories","summary":"","title":"MySQL"},{"content":" 乌孙古道，2023\n《Dynamic Programming Strikes Back》这篇论文提出了非常著名的 DPhyp 算法，能处理复杂的 join predicate，也能处理复杂的 join 算子，比如 outer join、semi join、anti join、dependent join 等。MySQL 8.0 实验性地支持了 DPhyp，我们最近的项目中也采用了 DPhyp，它有很高的实用价值，非常值得学习。\nINTRODUCTION # join reorder 算法主要是两类，一类是像 DPsize、DPsub、DPccp 这样 bottom-up 的动态规划算法，一类是像 top-down partition search 这样的记忆化搜索算法，它们目前都没有被大规模生产使用，主要原因：\n它们都没有考虑包含多个表，或者非等值的 join predicate 它们都没有考虑 outer join，anti join，semi join，dependent join 等不能随意 reorder 的 join operator 已有的一些解决方案都不是很完美，于是作者提出了 DPhyp，希望能够同时解决上面两个问题，使其能够被用于实际生产环境中。\nHYPERGRAPHS # DPhyp 的思想和 DPccp 是类似的，都是按照一定的顺序不重复的枚举所有的 csg-cmp-pair 来完成 join reorder 的动态规划过程。\n不一样的地方是，为了处理像 R1+R2+R3=R4+R5+R6 这样的多表复杂 join predicate，需要将原来的 query 构造成 hyper graph，在 hyper graph 中按照一定顺序不重复的枚举所有的 csg-cmp-pair 来完成基于 hyper graph 的 DPhyp 算法。在介绍 DPhyp 之前我们先来了解一些关于 hypergraph 的概念。\nDefinitions # hypergraph 由 hypernode 和 hyperedge 构成：\nhypernode：普通节点的集合。例如上图中的 hypernode {R1} 和 {R1, R2, R3} hyperedge：连接两个 hypernode V 和 U 的边，其中 V∩U=∅ 这样，所有的 join predicate 都可以看做是一个 hyperedge，比如 R1.a + R2.b + R3.c = R4.d + R5.e + R6.f 这个 predicate 代表了一个 hyperedge，连接了两个 hypernode {R1, R2, R3} 和 {R4, R5, R6}。\nCsg-cmp-pair # 同样的，hypergraph 也有自己的 csg 和 cmp 的定义，只是把普通节点换成了 hypernode，把普通边换成了 hyperedge。例如，上图中：\n{{R4}, {R6}} 不是一个 csg，因为它们之间没有直接相连的 hyperedge，不联通。 {{R1}, {R2}} 是一个 csg，{{R4}, {R5}} 是它的一个联通 cmp，但 {{R4}, {R6}} 不是。 和 DPccp 一样，csg-cmp-pair 指有 hyperedge 相连的 csg 和 cmp。用 min(S) 表示 hypernode 中最小的节点，DPhyp 仅枚举满足 min(S1) \u0026lt; min(S2) 的 csg-cmp-pair，根据最优子结构求解最佳 join order。\nNeighborhood # 和 DPccp 的一样，所有的 csg 和 csg-cmp-pair 都是从一个节点出发，然后根据它的 neighborhood 不断扩展来搜索到的。\nhypergraph 的情况稍微复杂。\nnon-subsumed hyperedge：所有不被其他 hyperedge 所包含的 hyperedge 所组成的边集。例如下图中有三个超边 ({R1}, {R2}), ({R1}, {R2, R3}), ({R1, R2}, {R3, R4})，那么：\n({R1}, {R2}) 被 ({R1}, {R2, R3}) 包含； ({R1}, {R2, R3}) 是一个非包含的超边，因为它没有被其他任何超边包含； ({R1, R2}, {R3, R4}) 也是一个非包含的超边。 non-subsumed hypernode：不被其他 hypernode 包含的 hyper node，比如 hypernode {R1} 和 {R2} 被 {R1, R2} 包含，{R1} 和 {R2} 都是 non-subsumed hypernode，而 {R1, R2} 不是 non-subsumed hypernode。\ninteresting hypernode：寻找 interesting hypernode 分为两步：\n找到所有潜在的 interesting hypernode 集合 E↓′(S, X)，然后最小化 E↓′(S, X)，消除其中被包含的 hypernode，得到最终的 interesting hypernode 集合 E↓(S, X)。 就这些定义，给定一个 hypernode S 和一个 excluded 节点集 X，它的 neighborhood 点集 N(S, X) 为：\n对于一开始图 2 中的 hypergraph 来说，当 S 和 X 都为 {R1, R2, R3} 时，N(S, X) 为 {R4}。\nTHE ALGORITHM # 主要的挑战：\n如何高效的遍历 Hyperedge，使得我们可以遍历出所有相邻的 Hypernode。 Hyperedge 指向的 Hypernode 可能包含多个节点，使得递归遍历和增长 Hypergraph 子图变的非常复杂。 为了方便的保存算法执行过程的上下文信息，论文将 Hypergraph Join Order 算法需要使用的中间数据保存在 DPhyp 这个 class 中，将算法所涉及到的 5 个子过程实现为 DPhyp 的成员函数。\n算法的入口是 Solve() 函数，用来初始化动态规划的 DP 表，以及所有子状态（也就是单表的最佳 Join Order）的初始值。接着调用 EmitCsg() 和 EnumerateCsgRec()。\nEnumerateCsgRec() 负责递归的枚举所有联通子图。\nSolve() # The algorithm calls EmitCsg({v}) for single nodes v ∈ V to generate all csg-cmp-pairs ({v}, S2) via calls to EnumerateCsgCmp and EmitCsgCmp. The calls to EnumerateCsgRec extend the initial set {v} to larger sets S1. EnumerateCsgRec() # EnumerateCsgRec 的主要作用通过 csg S1 的 neighborhood 将其扩展成更大的 csg。\nFor each of these subsets N, it checks whether S1 ∪ N is a connected component. This is done by a lookup into the dpTable. If this test succeeds, a new connected com- ponent has been found and is further processed by a call EmitCsg(S1 ∪ N ). Then, in a second step, for all these sub- sets N of the neighborhood, we call EnumerateCsgRec such that S1 ∪ N can be further extended recursively. Take a look at step 12. This call was generated by Solve on S1 = {R2 }. The neighborhood consists only of {R3 }, since R1 is in X (R4,R5,R6 are not in X either, but not reachable). EnumerateCsgRec first calls EmitCsg, which will create the joinable complement (step 13). It then tests {R2 , R3 } for connectedness. The according dpTable entry was generated in step 13. Hence, this test succeeds, and {R2,R3} is further processed by a recursive call to Enumer- ateCsgRec (step 14). Now the expansion stops, since the neighborhood of {R2, R3} is empty, because R1 ∈ X.\nEmitCsg() # EmitCsg 的主要作用是根据 csg S1 的 neighborhood 生成匹配的 csg-cmp-pair。\nB_min(S1): All nodes that have ordered before the smallest element in S1 (captured by the set B_min(S1)) are removed from the neighborhood to avoid duplicate enumerations. Since the neighborhood also contains min(v) for hyperedges (u, v) with |v| \u0026gt; 1, it is not guaranteed that S1 is connected to v. Take a look at step 20. The current set S1 is S1 = {R1, R2, R3}, and the neighborhood is N = {R4}. As there is no hyperedge connecting these two sets, there is no call to EmitCsgCmp. However, the set {R4} can be extended to a valid complement, namely {R4, R5, R6}. Properly extending the seeds of complements is the task of the call to EnumerateCmpRec in step 21.\nEnumerateCmpRec() # EnumerateCmpRec 的主要作用是基于初始的 cmp S2 和它的 neighborhood N(S2, X) 扩展更多的 cmp。\nTake a look at step 21 again. The parameters are S1 = {R1, R2, R3} and S2 = {R4}. The neighborhood consists of the single relation R5. The set {R4,R5} induces a connected subgraph. It was inserted into dpTable in step 6. However, there is no hyperedge connecting it to S1. Hence, there is no call to EmitCsgCmp. Next is the recursive call in step 22 with S2 changed to {R4,R5}. Its neighborhood is {R6}. The set {R4,R5,R6} induces a connected subgraph. The according test via a lookup into dpTable succeeds, since the according entry was generated in step 7. The second part of the test also succeeds, as our only true hyperedge connects this set with S1. Hence, the call to EmitCsgCmp in step 23 takes place and generates the plans containing all relations.\nEmitCsgCmp() # EmitCsgCmp 的主要作用是将 csg-cmp-pair (S1, S2) 的最佳 plan join 起来，计算 S1、S2 的 join predicate 和 join cost，更新 dpTable。\nThe task of EmitCsgCmp(S1,S2) is to join the optimal plans for S1 and S2, which must form a csg-cmp-pair. For this purpose, we must be able to calculate the proper join predicate and costs of the resulting joins.\nEvaluation # 目前没有太多关于 Hypergraph 的 Join Order Benchmark，所以论文自己尝试构造了几个 Benchmark：\nThe general design princi- ple of our hypergraphs used in the experiments is that we start with a simple graph and add one big hyperedge to it. Then, we successively split the hyperedge into two smaller ones until we reach simple edges. As starting points, we use those graphs that have proven useful for the join ordering of simple graphs.\n论文着重考虑 Cycle 和 Star Query，没有考虑 Chain 和 Clique Query：\nThe behavior of join ordering algorithms on chains and cycles does not differ much: the impact of one additional edge is minor. Hence, we decided to use cycles as one starting point.\nStar queries have also been proven to be very useful to illustrate different performance behaviors of join ordering algorithms. Moreover, star queries are common in data warehousing and thus deserve special attention. Hence, we also used star queries as a starting point.\nThe last potential candidate are clique queries. However, adding hyperedges to a clique query does not make much sense, as every subset of relations already induces a connected subgraph.\n论文给了 Split Hyperedge 得到更多 Hypergraph 的例子：\nFig. 4a shows a starting cycle-based query. It con- tains eight relations R0 , . . . , R7 . The simple edges are ({Ri},{Ri+1}) for 0 ≤ i ≤ 7 (with R7+1 = R0). We then added the hyperedge ({R0,\u0026hellip;,R3}, {R4,\u0026hellip;,R7}). Each of its hypernodes consists of exactly half of the re- lations. From this graph (call it G0), we derive hypergraphs G1 , . . . , G3 by successively splitting the hyperedge. This is done by splitting each hypernode into two hypernodes comprising half of the relations. That is, apart from the simple edges, G1 has the hyperedges ({R0 , R1 }, {R6 , R7 }) and ({R2, R3}, {R4, R5}). To derive G2, we split the first hyperedge into ({R0 }, {R6 }) and ({R1 }, {R7 }). G3 addi- tionally splits the second hyperedge into ({R2},{R4}) and ({R3 }, {R5 }).\n比较可惜的是，这篇论文仅关注了 DPhyp 生成 Join Order 的优化时间，没有关注生成的 Join Order 实际运行时间。在 Selectivity 估算、Cost Model 都理想的情况下，DPhyp 确实能产生不错的执行计划。不过考虑到 Selectivity 估算误差，所做出来的 Join Order 的实际运行时间是否也是最优的可能还需要更多的实验来验证。不过这个就属于 Join Order 问题的其他子问题了：\n如何准确、高效的估算 Join Cardinality 如何建立 Cost Model 以考虑不同的 Join 算子物理实现之间的代价 Cycle-Based Hypergraphs # Star-Based Hypergraphs # Queries with Regular Graphs # NON-REORDERABLE OPERATORS # Considered Binary Operators # Besides the fully reorderable join (B), we also consider the following operators with limited reorderability capabilities: full outer join (M), left outer join (P), left antijoin (I), left semijoin (G), and left nestjoin (T). Except for the nestjoin, these are standard operators.\nnestjoin 有一种变种称之为 d-join，d-join 可以和看做是 SQL Server 中的 Apply 算子，或者 Hyper 中的 DependentJoin 算子。在子查询优化中通常用来表示关联子查询，对子查询的解关联优化非常有帮助。\nReorderability # Existing Approaches # Rao et al 在《A practical approach to outerjoin and antijoin reordering》中提出了基于 extended eligibility list (EEL) 的方案，能够高效的处理普通 join、left join 和 anti join，但是不能处理 dependent join。另外是它需要在 EmitCsgCmp 中检查，\nNon-Commutative Operators # TRANSLATION OF JOIN PREDICATES # ","date":"4 June 2023","permalink":"/posts/sigmod-2008-dphyp/","section":"Posts","summary":"乌孙古道，2023","title":"[SIGMOD 2008] Dynamic Programming Strikes Back"},{"content":"","date":"4 June 2023","permalink":"/categories/join-reorder/","section":"Categories","summary":"","title":"Join Reorder"},{"content":" 天堂湖，乌孙古道，2023\nINTRODUCTION # 《 Analysis of Two Existing and One New Dynamic Programming Algorithm for the Generation of Optimal Bushy Join Trees without Cross Products》这篇论文先分析了 DPsize 和 DPsub 这两个常用的 join reorder DP 算法，发现它们在枚举 DP 使用的 csg-cmp-pair 时并不高效，实际时间复杂度远高于理论下界。作者提出了一种高效的 csg-cmp-pair 的枚举算法和 DPccp 算法，使得每个 csg-cmp-pair 仅被枚举一次，在各种类型的 join graph 和 join size 下都能表现出良好的 DP 性能。\nGuido Moerkotte 和 Thomas Neumann 后面基于 DPccp 和 hyper graph 提出了新的 DPhyp 算法，这篇论文对理解 DPhyp 算法也有一定帮助。\nALGORITHMS AND ANALYSIS # 在讲 DPccp 之前，先看看 DPsize 和 DPsub 这两个常用的 join reorder 算法，它们都能寻找到不包含 cross-product 的最优 bushy tree，只是时间复杂度不同。\nDPsize: Size-Driven Enumeration # 在著名的《 Access Path Selection in a Relational Database Management System》一文中，Selinger 提出了一种自下而上的 join reorder DP 算法。该算法按照连接节点数量从小到大的顺序为每个 interesting order 计算出了最佳的左深树 join order。虽然 Selinger 的 join reorder 算法只考虑了左深树，但是可以在这个算法思路的基础上扩展出能够枚举 bushy tree 的 DPsize 算法，其伪代码如下所示：\nDPsize 的思路简单直接：将 n 个表的 join order 问题分解成 k 和 n-k 个表的子问题。包含 1 个表的最佳 join order 就是该表本身，这是 DP 的初始解。然后从小大到大计算每个 join size 下的最佳 join order。在计算包含 s 个表的 join order 时，通过枚举所有 s1+s2=s 的 s1 和 s2 各自的最佳 join order 来确定包含 s 个表的最佳 join order。\n注意代码中的两个 counter。一个是 InnerCounter，它描述了该算法的时间复杂度，一个是 CsgCmpPairCounter，它描述了该算法枚举的所有联通的、有效的 csg-cmp-pair (S1, S2) 的数量。后面 DPsub 也统计了同样的 counter，便于对比分析。\n作者总结了 chain、cycle、star 和 clique 这 4 种 join graph 上的 InnerCounter 计算公式如下：\nDPsub: Subset-Driven Enumeration # DPsub 起源于 Vance 和 Maier 在《 Rapid Bushy Join-order Optimization with Cartesian Products》这篇论文提出的 join reorder 算法。原算法考虑了 cross-product，但因为 cross-product 极大的增加了 join order 的搜索空间，作者将其修改为不考虑 cross-product，得到了下面 DPsub 的伪代码：\nDPsub 是一种状态压缩 DP。它利用一个整数的各个比特位来表示有哪些表参与 join（第 i 位为 0 或 1 分别代表第 i 个表是否参与 join）。熟悉状态压缩 DP 和位运算的朋友们应该比较容易理解这个算法。\n每个整数 i 都代表了一个节点子集，也就是一个 DP 状态。状态 i 代表的点集用 S 表示，只对那些联通的点集 S 计算最佳 join order。在计算时依次枚举 S 的所有子集 S1 和对应的补集 S2=S\\S1，根据他们的最佳 join order 得到点集 S 的最佳 join order。\n从 1 到 2^n-1 递增的枚举所有可能的 DP 状态，这样的枚举方式可以保证在计算状态 i 的最优解时，i 的所有子状态已经在之前的枚举过程中计算出来了，状态 i 的最优解可以根据这些最优子状态计算出来。\n同样的，作者总结了 chain、cycle、star 和 clique 这 4 种 join graph 上的 InnerCounter 计算公式如下：\nAlgorithm-Independent Results # csg-cmp-pair：csg 是 connected subgraph 的缩写，cmp 是 complement 的缩写，如果 S1 和 S2 是两个不相交的联通子图，并且在 S1 中的某个节点 v1 和 S2 中的某个节点 v2 有一条边（也就是 join predicate），那么就说 (S1, S2) 是一个 csg-cmp-pair。另外子图和补图关系是相互的，(S2, S1) 也是一个 csg-cmp-pair。把 query graph 中 csg-cmp-pair 的个数用 #ccp 表示。#ccp 有一些非常重要的性质：\n它是 query graph 的固有属性，和具体的 join reorder 算法无关，query graph 决定了 #ccp 它是所有正确的 join reorder 算法中 CreateJoinTree 调用次数的下界，也就是 join reorder 算法的时间复杂度下界。 作者总结了 DPsize、DPsub 在 chain、cycle、star、clique 这 4 种 join graph 上的 #csg 和 #ccp 的计算公式，这些公式表明了 join reorder 算法的时间复杂度下界：\nSample Numbers # 作者抽样计算了几个 join size 下，DPsize、DPsub 算法中 InnerCounter 以及 #ccp 的数值，如下表所示：\n可以看到：DPsub 和 DPsize 在 chain、cycle、star、clique 类型的 query graph 中各有优劣，一个最大的特点是，不管 DPsize 还是 DPsub，它们的 InnerCounter 都远高于 #ccp，代表它们在枚举 csg-cmp-pair 时有许多失败尝试，距离 DP 算法的理论复杂度下界 #ccp 有好几个数量级的差距。因此作者就想设计一种能够达到 #ccp 这个理论时间复杂度下界的 DP 算法，也就是后面将要介绍的 DPccp。\nTHE NEW ALGORITHM DPCCP # 作者在这个章节详细介绍了能够快速且不重复的枚举所有 csg-cmp-pair 的 DPccp 算法，并利用图论和数学证明方法证明了这个枚举算法的正确性。\nProblem Statement # 因为要解决的是 DPsize 和 DPsub 这两个 join reorder 算法枚举了大量不联通的子图的问题，因此 DPccp 的目标就变成了如何高效的枚举所有联通的 csg-cmp-pair 一次且仅一次，以达到 DP 算法理论时间复杂度的下界 #ccp，同时也有一些限制条件：\ncsg-cmp-pair 的枚举顺序需要能够用来进行 DP，也就是当枚举到 (S1, S2) 这样的 csg-cmp-pair 时，S1 和 S2 各自所有的 csg-cmp-pair 都已经枚举过了。 另外是生成 csg-cmp-pair 的开销需要是常数级别，或者至少是线性级别，这样才能在时间复杂度上优于 DPsize 或 DPsub。 DPccp 的伪代码如下所示，通过枚举所有的 csg-cmp-pair，逐步求解最优 join order：\nEnumerating Connected Subsets # 要枚举 csg-cmp-pair，我们可以考虑如何枚举所有的 csg。先来看看如下定义：\nneighborhood N(v)：所有和节点 v 有一条边相连的点集 neighborhood N(S)：所有和点集 S 中的节点有一条边相连且不在 S 中的点集 一种利用 neighborhood 来枚举 csg 的方法是：从一个 csg S 出发，先寻找它的 neighborhood N(S)，然后枚举 N(S) 的子集并和 S 组成新的 csg S\u0026rsquo;，然后递归的调用这个函数来枚举所有从 S\u0026rsquo; 出发的 csg。不过这样的问题是一些 csg 可能被重复枚举。\n要解决重复枚举的问题，作者先用 BFS 方式遍历每个节点并给出一个递增的编号，然后按照节点编号从大到小逆序枚举所有的出发节点，并从这些出发节点开始枚举所有的 csg：\n从节点 vi 出发枚举 csg 时，不考虑那些编号比 i 小的节点 vj，因为这些节点会在后面被遍历到 从 csg S∪S\u0026rsquo; 出发枚举 csg 时，不再考虑 N(S)，因为这些节点也会在后面枚举的新的 S\u0026rsquo; 遍历到 枚举所有 csg 的伪代码如下所示，入口函数是 EnumerateCsg，辅助函数是 EnumerateCsgRec，Rec 应该是 recursive 的意思，隐含着该函数会不断被递归调用的信息：\n下面的例子展示了该算法的执行过程，这是一个 5 节点的 join graph，已经按照 BFS 的方式从 R0 出发对节点进行编号，跟着例子走一遍可以更深刻的体会这个枚举过程：\nEnumerating Complements of Connected Subgraphs # 上面的 EnumerateCsgRec 也可以用来继续枚举所有和 csg S1 联通的补图 S2：\nS2 一定包含 S1 的 neighborhood N(S1) 中的某些节点 仅枚举所有 min(S1) \u0026lt; min(S2) 的 csg-cmp-pair，其中 min(S) 指的是 S 中最小的节点编号 EnumerateCmp 的伪代码如下所示，其中 Bi(N) 指的是点集 N 中所有编号小于等于 i 的点集。为了枚举所有 min(S2) \u0026gt; min(S1)，且和 S1 没有任何交集的 csg S2，需要将所有编号小于等于 min(S1) 以及 S1 中的节点都排除掉，将这些排除掉的节点初始化 X 中。\n然后按照节点编号从大到小遍历 S1 的 neighborhood N(S1)\\X 后的点集 N，从 N 中每个节点出发构造和 S1 相连的 csg。同样的，为了避免重复枚举 S2，每次枚举的 min(S2) 也都确保比后续要枚举的 min(S2\u0026rsquo;) 要大，所以在调用 EnumerateCsgRec 时需要把 Bi(N) 也加到 X 中去。\n还是以这个 5 节点的 join graph 为例，如果 S1={R1}，那么初始化的 X={R0, R1}，它的 N={R4}，构造出的第一个 cmp 就是 {R4}，接着调用 EnumerateCsgRec(G, {R4}, {R0, R1}∪{R4}) 可以得到 {R4, R2}, {R4, R3}, {R4, R2, R3}，这 4 个 cmp 和 S1 共同构成了用于 DP 的 4 个 csg-cmp-pair：\nCorrectness Proof # 整个 DPccp 算法的正确性是由这个 csg-cmp-pair 枚举算法的正确性来保证的，作者在这一章节花了比较长的篇幅严谨的证明了 csg-cmp-pair 枚举算法的正确性，篇幅所限这里不再展开，感兴趣的朋友可以阅读原文。\nEVALUATION # 作者实现和测试了 DPsize、DPsub 以及新提出的 DPccp 这 3 种 DP 算法在 chain、cycle、star 以及 clique 这 4 种 join graph 下不同 join size 的时间开销，以 DPccp 的时间为基准和 DPsize、DPsub 的时间进行了对比：\n综合来看 DPccp 在各种 join graph 以及各种 join size 情况下性能表现都很不错，是个非常不错的 join order DP 算法。\n","date":"28 May 2023","permalink":"/posts/vldb-2006-dpccp/","section":"Posts","summary":"天堂湖，乌孙古道，2023","title":"[VLDB 2006] Analysis of Two Existing and One New Dynamic Programming Algorithm for the Generation of Optimal Bushy Join Trees without Cross Products"},{"content":" 夏特古道，2023\n简介 # 这篇论文提出了一种能够避免 long-running OLAP query 影响 OLTP 事务、在多核 CPU 上 scale、支持 out-of-memory workload 的 MVCC 实现机制。\n长时间运行的 OLAP 查询会严重影响 OLTP 事务延迟。例如上图，在 TPC-C 开始 10 秒后执行一个 sleep 语句，之后 WiredTiger 和 PostgreSQL 的 TPC-C 的性能都出现了大幅下跌，下跌幅度取决于 GC 激进和精确程度。\n作者分析了 OLTP 性能下降的原因。长时间运行的 OLAP 查询会阻止 tombstone GC，从而导致 OLTP 事务执行 index range scan 时需要额外检查每个 index key 的 version chain，进行额外的 tombstone skipping， 影响事务性能。OLTP 事务执行过程中的 delete 和 update 会使这种情况变得更加糟糕。作者以 TPC-C 的 neworder 表为例，该表模拟了一个订单队列，各个事务会不断插入和删除订单，如果不能及时 GC tombstone，tombstone 也会越积越多。此外，对 index key 的更新也会遇到类似的问题，因为更新 index key 会先删除原来的 key 再插入新的 key。\n而另一方面，当 OLTP 事务频繁更新某些数据时，长时间运行的 OLAP 查询延迟也会受到影响。因为采用了 N2O 的 version 顺序，新版本在 version chain 的头部，OLAP 查询需要跳过 version chain 前面的新版本才能获取到它需要的老版本数据。\n有一个性质可以用来解决这个问题，那就是在任何时刻，每个 tuple 最多需要保留的版本数不会超过当前正在进行的事务总数。但没有任何 out-of-memory 系统实现了这种激进和精确的 GC 策略。\n作者还详细分析了内存数据库和传统关系型数据库各自 MVCC 系统的实现原理，感兴趣的朋友可以参考论文的 \u0026ldquo;2.4 SI Commit Protocols\u0026rdquo; 小节。下面我们重点来看作者在论文中提出的 MVCC 系统实现。\n系统设计概览 # 上图是作者提出的 MVCC 系统设计概览，总的来说可以分为如下几个方面：\nCommit Protocol：基于 Commit Log 实现了 LCB（Last Committed Before）接口，基于 LCB 接口实现了可见性检查，进而实现了 OSIC（Ordered Snapshot Instant Commit）。 Garbage Collection：基于 oldest_oltp 和 oldest_tx 这两个 watermark 避免了长时间运行的 OLAP 查询导致的版本堆积使得 OLTP 事务延迟增大的问题。通过 Tombstone Index 记录所有被 OLTP 事务删除的数据，通过 Graveyard Index 存储所有不被任何 OLTP 事务使用的 tombstone，避免了标记删除的 tombstone 增加 OLTP 事务延迟，和双 watermark 机制共同解决了频繁插入删除导致 tombstone 堆积，事务延迟增加的问题。 Version Storage：基于 off-row 的 Delta Index 和 in-row 的 FatTuple 这两种 version chain 存储格式之间的自适应转换实现了高效的数据读写和精确 GC。 Commit Protocol # 一些系统假设：采用全局逻辑时钟，每个事务开始和提交时分别获取 start 和 commit ts。采用 first writer wins 的原则解决写写冲突。每个工作线程内的事务串行执行，多个工作线程之间的事务并发执行。\nOSIC 是 Ordered Snapshot Instant Commit 的简称，它实现了 SI 的事务隔离级别，通过 start ts 确定可见性，决定 snapshot，确定事务顺序等。和其他 out-of-memory 系统一样，事务 commit 时也不用再次修改 write set（比如将 write set 中的 start ts 修改为 commit ts）。\nLCB 和 Snapshot Cache # LCB 是 Last Committed Before 的简称，LCB(w, ts) 表示工作线程 w 在 ts 这个时间戳之前最后一条已提交事务的 commit ts。要想根据 start ts 确定事务的 snapshot，只需要知道所有工作线程 w 上的 LCB(w, start ts) 即可。\n为了减少获取 LCB 的线程间同步开销，LCB 需要做到按需获取，且仅获取一次，理论上事务执行过程中最多获取 #W 次 LCB（#W 表示所有 worker 的数量）：\n仅在遇到其他 worker 写的数据版本（数据的版本信息中包含 worker id）时才去向对应的 worker 获取 LCB，避免获取无用的 LCB。 把获取到 LCB(wi, start ts) 缓存在 thread-local 的 snapshot cache 中，后面再遇到 worker wi 写的其他数据版本时就不再重复向该 worker 获取 LCB，而是直接读取 snapshot cache。 上图是根据 start ts、snapshot cache、LCB 进行可见性检查的伪代码，其中：\ntuple_ts_start 表示该 tuple 的版本号，tuple_w_i 表示写入该版本的 worker id。 isVisible() 里第 1 个 if 语句先检查这个数据版本是否是当前 worker 写入的，是的话那肯定可见 第 2 个 if 语句根据具体的事务隔离级别调整可见性检查使用的事务 start ts，如果是 RC 隔离级别，就更新当前事务的 start ts 为最新 ts，尽量读到所有 worker 上最新的 commit 第 3 个 if 语句接着查本地的 snapshot cache，也就是伪代码中的 sc。sc 存储了每个 worker 上最近一次 LCB(wi, start ts) 对应的 commit ts。如果该 tuple 的版本号比上次 LCB 查到的 commit ts 还小，该 tuple 一定对当前事务可见。 第 4 个 if 语句检查 snapshot cache 是否需要更新。缓存的内容实际是 tuple_w_i 这个 worker 在 tx_ts_start 这个时间戳对应的 last_commit_ts。如果缓存的 cache_ts_start 比 tx_ts_start 更小（不会发生更大的情况），这时候就需要更新 LCB 缓存 如果前面的检查都 fail 了，到这里我们已经拿到了准确的 LCB(tuple_w_i, tx_ts_start) 的结果并将其缓存在了 sc[tuple_w_i] 中，最后只需要根据准确的 last_commit_ts 做一次比较即可判断该版本是否可见。 Commit Log # 为了计算每个工作线程 w 上的 LCB(w, ts)，需要为每个工作线程维护一个 Commit Log 队列，用来存储 start ts 和对应 commit ts 的映射关系，处理 LCB 请求时只需要查找 Commit Log 返回对应的 commit ts 即可。\nCommit Log 会被并发读写，因此也需要合适的线程间同步机制。每个 Commit Log 都有一把锁，事务 commit 时需要获得写锁，将 commit ts 和对应的 Commit Log 写入后才完成事务提交，释放锁，之后其他拥有更大 start ts 的事务就能看见提交后的最新版本了。\n对每个 worker 上的 Commit Log 队列来说，因为只需要为每个活跃事务保留一个 Commit Log，因此总共只需要保留 #W （worker 总数）条 Commit Log。每个 worker 开启新事务时都会检查 Commit Log 队列是否已满，如果队列已满，该 worker 需要收集所有活跃事务的 start ts，剔除那些活跃事务不可见的 Commit Log，限制 Commit Log 的总大小。\nGarbage Collection # 为了解决因长时间运行的 OLAP 查询导致 MVCC 版本堆积而拖慢 OLTP 事务的问题，作者通过优化器将查询分为 OLAP 和 OLTP，并维护了 oldest_tx 和 oldest_oltp 这两个 watermark。它们分别表示最老的 OLAP 事务和最老的 OLTP 事务的 start ts。这样就可以对 newest_olap 到 oldest_oltp 之间的版本进行 Cooperative GC（每个 worker 回收自己创建的位于 LCB(w, oldest_tx or oldest_oltp) 之间的 MVCC 版本），从而避免长时间运行的 OLAP 事务对 OLTP 事务的影响。\n使用 start ts 的最高有效位来区分 OLTP 和 OLAP。事务结束后，start ts 将以 1/#W（#W 表示 worker 总数）的概率更新到全局共享数组中，从而可以通过分析该数组中的 OLTP 和 OLAP start ts 来获取这两个 watermark。\nTombstone # 删除数据会产生 tombstone，而因为 long-running OLAP query 的存在，这些 tombstone 会在 main index 中一直累积，增加 OLTP 事务延迟：因为索引中没有数据的 MVCC 信息，要想知道一个数据是否删除了，需要先读索引再去 main index 拿到具体的 tuple，最后根据 tuple 上的 MVCC 信息判断数据是否已删除。\nGraveyard Index # 因为被 tombstone 标记删除的数据只对那些 long-running OLAP query 可见，对那些 start ts 更大的 OLTP 事务不可见。因此作者引入了一个额外的 graveyard index 数据结构来存储这样的 tombstone，只要 tombstone 的 start ts 小于 oldest_oltp 就将其从 main index 移到 Graveyard Index。\nOLAP query 需要同时查询 main index 和 Graveyard Index 来获取所有可见的数据版本，而 OLTP query 只需要查询 main index 即可，牺牲少量 OLAP 查询性能保障了 OLTP 事务延迟不受 long-running OLAP query 的影响。\nTombstone Index # 为了知道有哪些 tombstone 并及时对它们进行 GC，作者以去中心化的方式在每个 worker 上维护了一个 Tombstone Index。Tombstone Index 是一个 append-optimized B+ tree，key 由 start ts、command id 组成，value 为该 tombstone 的 tuple key。根据 oldest_tx 以及 oldest_oltp 这两个 watermark 确定 Tombstone Index 中的 tombstone 是应该直接删掉还是从 main index 转移到 graveyard index。\nVersion Storage # 同时采用了 Delta Index 和 FatTuple 两种版本存储方式，默认采用 Delta Index。\nDelta Index # 如上图所示，默认情况下所有老版本数据都存储在每个 worker 线程的 Delta Index 中，main index 只存储最新版本的 tuple 和其 version chain 的一些 metadata，这些 metadata 包括：\nWorkerID 和 TSstart：共同决定了该 tuple 最新版本的可见性 WorkerID、TSstart 和 CommandID 共同构成了 Delta Index 的 key，同时也指向了 version chain 中上一个更老版本的数据。CommandID 主要用来区分同一个事物的不同操作，在 Delta Index 中可以省略，使用（WorkerID、TSstart、IndexID、Key）作为 Delta Index 的 key。 #updates 和 since_oldest_tx：用来估算 version chain 的长度。因为精确的维护 version chain 的长度代价太大，作者采用了启发式的估算方式。#updates 表示在 oldest_tx 这个 watermark 不变的情况下该 tuple 最多更新了多少次，反应了受 long-running OLAP query 影响的情况下累积的版本数。since_oldest_tx 存储的是 oldest_tx 的低 16 位（2 字节）。每当事务更行时都会检查当前 oldest_tx 的低 16 位是否和 since_oldest_tx 相等，是的话就把 #update 加 1，否的话就重置 #update 计数器。当 #update 超过 worker 总数时就将其转成 FatTuple 的存储方式。 FatTuple # 当 tuple 被频繁更新时会被自动转换为 FatTuple，把所有的数据版本 delta 都 inline 的存储在 main index 的 FatTuple 中。因为每个 FatTuple 只需要存储 #W 个数据版本，作者在 FatTuple 的基础上实现了 on-demand precise garbage collection（OPGC），OPGC 模式下会首先将 newest_olap 到 oldest_oltp 这个 dead zone 内的 delta GC 掉，如果还没有足够的槽位再去收集所有正在运行事务的 start ts，确定所有的 dead zone 后 GC 所有不需要的 delta。关于 precise garbage collection 可以参考《Long-lived Transactions Made Less Harmful》。\n另外，因为 GC 时只会扫描 Tombstone Index 和 Delta Index，为了保证 FatTuple 中的旧版本也能被 GC，在 page eviction 时需要将 FatTuple 转成 Delta Index。\nDurability and Recovery # 因为所有事务操作都有 WAL，所以前面提到的数据结构比如 Commit Log、Graveyard Index、Tombstone Index、Delta Index 都没有 durability 要求。\n恢复也比较简单，在 analysis 阶段确定 winning transactions，在 redo 阶段 delete 操作直接将 main index 上的 tombstone 删掉。每个 worker 的 Commit Log 初始化为最后一个 winning transaction 的 commit ts。\n事务 abort 比较特殊。worker 的 Commit Log 中只存储了成功提交的事务，事务 abort 时需要将它所有的改动回滚（可能分散在 Delta Index、Tombstone Index 中），将它的 start ts 也回收，用于该 worker 的下一个事务的 start ts。\nEvaluation # 从作者后面的实验来看，Graveyard Index 能够显著降低 long-running OLAP query 对 TPC-C 性能的影响：\n而 FatTuple 的设计也使得 OLAP scan 的性能不受影响： 后面作者还做了关于 “TPC-C + Scan: Scalability”、“Out-of-Memory Breakdown”、“Bulk Loading”、“Out-of-Memory Key/Value”、“Deterministic Execution Under Contention” 等实验，感兴趣的朋友可以详细阅读 “4 EVALUATION” 这一节，从实验结果来看整体效果不错。\n","date":"24 May 2023","permalink":"/posts/vldb-2023-scalable-and-robust-si/","section":"Posts","summary":"夏特古道，2023","title":"[VLDB 2023] Scalable and Robust Snapshot Isolation for High-Performance Storage Engines"},{"content":"","date":"24 May 2023","permalink":"/categories/mvcc/","section":"Categories","summary":"","title":"MVCC"},{"content":" 夏特古道，2023\nIntroduction # 这篇论文不长，主要介绍了 TUM 为 Umbra 实现的 Hybrid Lock，它能同时提供乐观和悲观的上锁方式，通过在不同场景中使用不同的上锁方式来获取更高的性能。本文主要描述 Hybrid Lock 的原理和实现，论文中对其他锁的讨论对我们理解并发和锁也很有启发意义，建议感兴趣的朋友阅读一下原论文：《 Scalable and Robust Latches for Database Systems》\nLock requirements # 作者分析了数据库对锁的能力要求，总结如下：\n并发读需要尽量快，尽量在多核上 scale，因为包括 OLTP 事务在内的大多数工作负载都是以读为主； 对那些采用了代码生成的系统来说，也需要尽量避免外部函数调用； 因为锁可能保护非常细粒度的数据结构，因此锁本身的空间开销也应该尽量小； 需要尽量高效优雅的处理锁冲突，对于数据库来说，可能线程在锁等待时用户已经取消了查询执行，好的锁实现最好也支持锁等待的线程周期性的检查这类事件，及时响应用户的取消操作。 Optimistic and pessimistic locking # 悲观锁和乐观锁是两种主要的上锁方式：\n悲观锁：认为这次操作大概率会和其他并发线程产生冲突，在操作数据前先以共享或互斥的方式上锁，线程之间的锁冲突可以在操作数据之前检测到。比如使用常见的 RwLock，在操作数据前先上 Read Lock 或 Write Lock，只有上锁成功后才操作数据。 乐观锁：认为这次操作不太会和其他线程产生冲突，使用原子变量的方式为保护的数据维护一个版本号，修改数据时增加版本号，读数据的线程通过对比版本号变化检测冲突，如果冲突就需要重试这次读操作。 上面是实现一个乐观锁的伪代码。读的时候如果发现编码在版本号中的 lock 比特位为 1（表明有人在更新数据），或者读取数据后版本号发生了变化（表明数据已经更新了），这次读操作就需要重试。\n乐观锁特别适合用在经常读的热点数据上，比如 B Tree 的 root 节点。使用时需要注意几个问题：\n乐观锁在碰到写的时候会重试正在进行的读操作，需要确保重试安全。比如读取某个 B Tree 节点，可能别的线程触发了 B Tree Merge 操作导致当前要读的节点被删除了，需要确保重试后数据仍然有效，不会出现访问空指针的情况。 上面伪代码传入的是一个 readCallBack，这个 callback 每次重试都会被调用，如果在里面更新一些值，比如 count，那么可能就会因为重试得到错误结果。最安全的做法是仅通过这个 callback 获取数据，等整个读操作返回后才用读到的值去做下一步的计算。 当很多线程并发写时，乐观锁很容易被饿死，所以就像伪代码描述的那样，在经历了最大重试次数后需要能够 fallback 到悲观锁。 乐观锁和悲观锁的性能差异主要来自 cache coherence。从上面伪代码可以看出，乐观锁不会修改原子变量的值，这使得各个 CPU core 上的 cache line 一直有效。而悲观锁则因为这些修改操作使得其他 CPU cache 频频失效，导致性能很差。\nHybrid lock # 作者将不同场景和对应的最佳的上锁方式整理到了上表中，可以看到除了 read-only 和重试代价低的 read-mostly 场景，其他 3 种场景下使用 shared 或 exclusive 的悲观锁性能会更好。为了获得更好的性能，就需要在不同的上下文使用不同的上锁方式。比如 B Tree，访问 read-contended 上层节点时就可以使用乐观锁，而访问叶子结点去读取数据时就使用悲观锁以避免昂贵的冲突重试。\n为了达到这个目标，作者设计了一个 Hybrid Lock。如上图所示，这个 Hybrid Lock 内部包含一个 RWMutex 和一个 atomic\u0026lt;uint64_t\u0026gt; 分别用来实现悲观锁和乐观锁，Hybrid Lock 的伪代码如下所示：\n需要特别注意 unlockExclusive() 里面两个操作的顺序。因为 tryReadOptimistically() 在执行完 readCallBack 后先检查 RWMutex 再检查 version，unlockExclusive() 里面就需要先 version +1 再释放 RWMutex，确保读操作获取到 RWMutex 后对应的 version 已经更新了，这样它才一定能够检测到这个读写冲突并重试。也可以使用 Intel 的 CMPXCHG16B 指令来同时更新 version 和释放锁。\nreadOptimisticIfPossible() 和一开始在乐观锁实现中看到的伪代码稍微有点区别。它会在 tryReadOptimistically() 失败后直接从回退到悲观锁，使用 lockShared() 和 unLockShared() 完成这次读操作。\n作者在 “2.2 Speculative Locking (HTM)” 中也介绍了 Speculative Locking (HTM) 这类硬件支持的琐实现，没有采用它们是因为它们仅在特殊硬件支持，不够通用。\nRWMutex and contention handling # 上面的 Hybrid Lock 还不能完全满足数据库场景的需求，比如在使用普通 RWMutex 时，如果一个线程被锁阻塞，它就会一直阻塞在那，而阻塞过程中用户发起的 cancel query 请求就可能因为长时间的锁等待而迟迟没有被响应。于是 Hybrid Lock 还需要一种能更好的处理锁冲突的 RWMutex 实现。\n“3.1 Busy-Waiting/Spinning”、“3.2 Local Spinning using Queuing” 和 “3.3 Ticket Spinlock” 中作者详细介绍和分析了 spinlock 和它的一些变种。整体而言，没有采用它们的原因是希望避免 spinlock 带来的 priority inversion、wastes resources and energy 以及 cache pollution 的问题。\n作者参考 webkit 设计和实现了 ParkingLot 的 RWMutex。\nParkingLot # 像 pthread mutex 这类琐实现通常都通过内核系统调用来挂起当前线程直到获得锁为止。但内核的系统调用开销很高，所以也有像 Linux 的 futex 这样的自适应锁实现，仅在锁冲突时才调用内核阻塞当前线程。基于 futex 的思路，WebKit 提出了一个叫 ParkingLot 的自适应锁，也就是这篇论文采用的锁实现。这种实现方式非常通用，可移植性强。\n如上图所示，ParkingLot 有 3 个关键要素：\nlock：为方便理解，可以认为它是一个 64 位整数的原子变量，在 ParkingLot 中只使用它的两个比特位，分别是 L 位用来表示是否有线程以 exclusive 的方式持有锁，以及 W 位表示是否有线程在锁等待（发生在某个其他线程以 exclusive 的方式持有锁时）。 ParkingSpace：每个锁都有一个 ParkingSpace，里面存储了 3 个数据。一个是 WaitingThreads 表示有多少线程正在锁等待，一个是条件变量用来挂起或唤醒当前线程，另一是传统的 mutex 用来保护这个 ParkSpace 本身的并发访问（比如修改 WaitingThreads） ParkingLot：lock 地址到对应 ParkingSpace 的全局哈希表。因为冲突的锁数量最多不会超过使用的线程数，ParkingLot 里只有固定的 512 个槽位就足够了，采用拉链法解决哈希冲突。 这种锁实现非常灵活，比如可以在锁等待时可以执行某个 callback，利用这个特性可以在锁等待时检查查询是否被取消，Page 是否已经被缓存替换等。\n上图的 ParkingLot 例子中，当前锁的 L 位为 1 表示已经被其他线程持有。当另一个线程想再次获取锁时，它会在 ParkingLot 中等待。此时它会把锁的 W 位设置为 1 表示有人正在等锁，然后使用这个锁的地址在哈希表中找到该锁对应的 ParkingSpace，然后执行 ParkingSpace 中的逻辑。当第 1 个线程释放锁后，它发现 W 位为 1 有其他线程在等锁，它会找到这个锁的 ParkingSpace，利用里面的条件变量将等待的线程唤醒。\n上面是发现 W 位为 1 进行等待时要执行的 park 函数伪代码。在 park 的过程中，先通过锁地址 lockAddr 找到对应的 parkingSpace，把 parkingSpace 的等待线程数 +1，如果没有设置锁等待超时时间，就直接 wait 到被唤醒，然后执行 callback 看是否继续 wait。如果设置了锁等待超时时间，会每隔 timeoutInMs 唤醒一次，执行 callback 看是否需要继续 wait。callback 由调用方提供，比如检查查询是否取消，这样在锁还没获取到但用户已经取消查询的情况下，当前线程就不用继续等待，直接退出执行。\n不过仔细 review 这个伪代码，除了检查 callback，应该还需要检查 lock 的 W 列判断锁是否还在，不然可能 callback 返回结果一直为 false，线程一直执行 while 循环。\nTakeaways # 以前对锁的使用仅局限于 RWLock，或者干脆 Lock，思维几乎固化。这篇论文让我对锁有了新的认识，HybridLock 和 ParkingLot 的设计确实很巧妙。我想不止数据库，它们用在其他合适的应用场景中应该也能起到不错的效果。\n","date":"24 April 2023","permalink":"/posts/damon-2020-scalable-and-robust-latches-for-database-systems/","section":"Posts","summary":"夏特古道，2023","title":"[DaMoN 2020] Scalable and Robust Latches for Database Systems"},{"content":"","date":"24 April 2023","permalink":"/categories/latch/","section":"Categories","summary":"","title":"Latch"},{"content":" 夏特古道，2023\n简介 # 基于磁盘的 DBMS 通常采用 ARIES 风格的日志恢复机制，它可以处理超过内存的数据和事务，可以在多次崩溃的情况下快速恢复，支持 fuzzy checkpoint 等。然而，ARIES 的中心化日志模块开销很高，不能在现代多核 CPU 上扩展。这篇论文提出适用于多核 CPU 和高性能存储的日志恢复算法。作者扩展了《 Scalable Logging through Emerging Non-Volatile Memory》中的 scalable logging 机制，实现了 continuous checkpoint、高效的页面分配和跨日志文件的 commit 优化。其性能与内存数据库相当。\nScalable Logging # 《 Scalable Logging through Emerging Non-Volatile Memory》中介绍的 Scalable Logging 采用多个日志文件来解决并发事务之间的全局锁竞争问题。每个日志文件供一个或多个工作线程独立使用。\nGSN（global sequence number）：类比分布式时钟，事务和 page 都可以看作是分布式系统中的进程，而 WAL record 则是需要排序的事件。当事务访问 page 时，它的 txnGSN 会被设置为 max(txnGSN, pageGSN)，而后续产生的 WAL record 的 GSN 则为 txnGSN+1。这种 GSN 机制可以在不同日志文件的 WAL record 之间建立类似于分布式时钟的偏序关系。当两个事务访问不同的页面时，它们的 GSN 不同步，后 commit 的事务甚至可以有较小的 GSN（如图 1b 所示）。这种机制也保证了每个日志内部的 WAL record 按 GSN 排序。在恢复过程中，page 的日志记录需要从所有的日志中收集，按 GSN 排序后再使用。\npassive group commit：在事务提交时，首先会将自己的日志 flush，然后加入到 group commit queue 中等待。等队列中小于该事务 GSN 的所有其他事务 commit 后，当前事务才会 commit，从而维护正确的 WAL record 偏序关系。\n这篇论文的方案类似于 scalable logging：每个工作线程都有自己的本地日志文件，WAL record 包含 log type、page ID、transaction ID 以及 before-after image 等信息。在事务提交前，先将该事务的 WAL record 写入 PMEM 缓存中，然后根据作者提出的 Remote Flush Avoidance 机制（简称 RFA）检查是否有其他事务和它修改了共同的 page 产生了依赖关系，并根据需要提交到 group commit 队列中。采用 continuous checkpointing 来平滑 IO，避免突发写入造成事务延迟抖动。\nTwo-Stage Distributed Logging # 每个工作线程都有自己的本地日志文件，每个事务由一个工作线程完成，WAL record 也写入该线程所属的日志文件中。每个工作线程的日志由 3 个阶段构成：\nPMEM 的 log chunk 组成的循环链表。工作线程不断取出 free chunk 写入当前事务的 WAL。当事务的所有 WAL 在此阶段写入完成时，即可向外部返回提交成功。 SSD。每个工作线程都有一个 WAL writer 线程，它将 full chunk 写入 SSD，为第 1 阶段提供可用的 free chunk。 Low-Latency Commit Through RFA # 为保证 ACID 中的 C，提交当前事务时需要确保 GSN 小于它的所有事务都已经执行完并且对应的 WAL 已经被 flush。例如：Txn1 在 page a 上删除了一条数据，并在其本地日志 L1 上写下 GSN 为 12 的 WAL；Txn2 在 page a 上插入了一条数据，并在其本地日志 L2 上写下 GSN 为 13 的 WAL。Txn2 的提交需要等待 Txn1 的提交结束，也就是 GSN 13 之前的 GSN 12 也需要被 flush 到磁盘后才能对外返回执行成功：\nTimeline\nTxn1 (Log=L1)\nTxn2 (Log=L2)\n1\nDELETE(Pa, a1), WAL(L1, GSN=12)\n2\nINSERT(Pa, a2), WAL(L2, GSN=13)\n3\nCOMMIT\n3\nCOMMIT\n这种机制对事务的吞吐量和延迟产生了很大的影响。Scalable logging 采用了 passive group commit 来缓解这个问题。事务的 WAL 在 flush 到本地日志后立即进入全局 commit queue，由 group commit 后台线程周期性地检查所有日志文件中已 flush 的 GSN，以此判断哪些事务可以提交。\n大多数事务是互相独立的，因此即使 GSN 存在大小关系，也可以并行提交。为避免不必要的 group commit，作者提出了一种 RFA（Remote Flush Avoidance）机制来识别互相独立的事务。\nRFA 原理很简单：如果该页面上的修改对应的 WAL 已经 flush，或者该页面没有被其他线程并发修改过，则可以跳过 group commit：\n每个事务开始时，记录所有日志文件的 flush GSN 中的最小值，记为 GSN_flushed。所有进行中的事务写入的 WAL GSN 都一定大于 GSN_flushed。如果 page 的 GSN 不超过 GSN_flushed，则表示该 page 上的修改对应的 WAL 已经 flush 到了各个日志文件中，该事务不依赖于任何未提交的事务。 每个 page 记录了上一次的修改者的日志文件，记为 L_last。每次事务修改 page 时，page 的 L_last 都会更新为修改者的日志文件。如果 page 的 GSN 超过了该事务的 GSN_flushed，则说明该事务依赖正在进行的另一个事务（可能是它自己）。需要检查 page 的 L_last 是否与该事务的日志文件相同，以判断是否有其他人修改了该 page。 每个事务都会维护 needsRemoteFlush 标志。如果其他事务修改了页面，则该标志设置为 true，该事务需要进入 group commit 队列。如果该事务的所有读写操作结束后，needsRemoteFlush 仍然为 false，则可以跳过 group commit 队列，避免 remote flush。 下图形象的对比了传统 ARIES、普通 group commit，以及经过 RFA 优化的 group commit 在处理互相独立事务时的差异：\nContinuous Checkpointing # continuous checkpoint 希望做到只有新增的 WAL 超过某个阈值时才触发 checkpoint。为了避免扫整个 buffer pool，buffer pool 被分成了 S 个 buffer shard，每次增量 checkpoint 时采用 round-robin 的方式选择一个 buffer shard 进行全量 checkpoint。continuous checkpoint 的伪代码如下：\n为了计算 checkpoint 结束后哪些 WAL 日志可以删除，需要如下两个信息：\nbuffer pool 的 checkpointed GSN：checkpointer 维护每个 buffer shard 的 checkpointed GSN（也就是上面伪代码中的 maxChkptedInShard[shard]），表示该 buffer shard 上所有 GSN 小于该值的 WAL 都已经被 flush 到了各个日志文件中。整个 buffer pool 的 checkpointed GSN 是所有 buffer shard 的 checkpointed GSN 的最小值。 活跃事务的最小 GSN：未提交的活跃事务也会影响 WAL 日志清除，因为在事务 abort 时需要靠这些日志产生对应的 compensation log。因此需要维护所有活跃事务的最小 GSN。 利用 checkpointed GSN 和活跃事务的最小 GSN 即可得出可以被安全清理的 WAL GSN，各个日志文件可以根据它来清理不需要的 WAL 日志。\n上图展示了一个增量 checkpoint 的例子。buffer pool 被分成了 3 个 shard，每个 shard 有 2 个 page：\n最近一次增量 checkpoint 发生在 shard b1 上，从中间 checkpointer 维护的 checkpointed GSN 表来看，shard b1 的 checkpointed GSN 是 34。 下一次增量 checkpoint 发生在 shard b2，shard b2 此时的 checkpointed GSN 是 8，也是整个 buffer pool 中最小的 checkpointed GSN，而所有活跃事务的最小 GSN 是 27，意味着目前只有 GSN 小于 8 的 WAL 才能被清理掉。 对 shard b2 做增量 checkpoint，b2 里只有一个 dirty page G，将 G 写入磁盘后，b2 的 checkpointed GSN 就被更新成了 checkpoint 前所有日志文件最小的 GSN 46。buffer pool 的 checkpointed GSN 也从一开始的 8 变成了 24（shard 3 的 checkpointed GSN）。 Page Provisioning # 除了 checkpoint，buffer manager 在进行缓存替换时也会将 dirty page 写入磁盘。LeanStore 的 buffer pool 存在 hot、cool 以及 free 三个区域。为了提升工作线程的事务性能，作者使用了一个 page provider 线程来负责 page 相关的 housekeeping 工作，确保工作线程有足够的 free page 可用：\n挑选 hot page 放入 cool 区域，以及将 cool 区域中的 page 按需移回 hot。 驱逐 cool 区域的 cool page（如果是 dirty page 就将其落盘）将 buffer frame 放入 free 区域供工作线程复用。 工作线程发现 buffer pool 偏离它的状态时（比如 hot area 满了）会唤醒 page provider。page provider 多轮运行后，使得 cool 和 free 区域的 page 数量恢复到预设的状态，在每轮运行中 page provider 首先 unswizzle 固定数目的 hot page，比如 256 个，将它们插入 cool 区域的 FIFO queue 的队首，然后从 FIFO queue 的队尾驱逐 cool page。如果是 clean page 就直接出队，清理内存后放回 free list 中。如果是 dirty page 就先保留在 queue 中，先将其标记为 writeBack，拷贝到本地的 writeBack buffer 中。因为仍然在 cool 区域的 FIFO 队列中，在 writeBack 状态的 page 仍然可以被修改，也可以在 swizzled 和 unswizzled 状态之间转换。当 writeBack buffer 填满后，它们才被一次性写入磁盘，更新对应的 GSN 和 dirty 标志。在下一轮运行过程中，page provider 仍然会从 FIFO queue 的队尾取出一个 cool page，这时候如果还是上一次遇到的 dirty page，因为已经写入磁盘且没经过其他修改，所以也不再 dirty，可以直接当做 clean page 处理。\nTransaction Abort # 作者和 ARIES 一样采用了 STEAL 的策略，允许未提交的更改写到磁盘上。事务 abort 也需要写 compensation WAL。\nRecovery # 和 ARIES 一样，LeanStore 的 recovery 也分成了 3 个阶段。\nlog analysis：每个线程都会扫描包括 PMEM 在内的所有 WAL 日志，分析出执行成功和失败的事务。对执行成功的事务，将对应的 WAL 按照 page ID 进行 partition，对执行失败的事务将他们放到 undo list 中。 redo：每个线程都分配了一个 page ID 范围，将对应的 WAL 按照 (page ID, GSN) 排序，接着逐个 page 进行 redo。 undo：遍历需要 undo 的 WAL，进行逻辑上的 undo。 总结 # 以上就是这篇论文提出的日志和恢复机制的核心部分了，论文后面作者也提供了详细的测试结果，感兴趣的朋友可以阅读原文再详细了解下。\n","date":"9 April 2023","permalink":"/posts/sigmod-2020-rethink-log-checkpoint-recover/","section":"Posts","summary":"夏特古道，2023","title":"[SIGMOD 2020] Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines"},{"content":"","date":"9 April 2023","permalink":"/categories/logging-recoverying/","section":"Categories","summary":"","title":"Logging \u0026 Recoverying"},{"content":" 夏特古道，2023\n简介 # 这篇论文主要讲了 Umbra（TUM 实现的 larger-than-RAM 数据库）的高性能 MVCC 实现。作者将事务分为两类，一类是数据修改量不大可以在内存中完成的常规事务，一类是需要修改大量数据的 bulk operation（比如 bulk load）。作者提出了将所有老版本数据存储在内存的 MVCC 实现方案，可以极大加速常规事务的执行。同时也给出了一种应对 bulk operation 的事务执行策略。\n基本思路 # 版本链（version chain）：每个事务有一个 private version buffer，更新数据时先将旧值从 B tree page 拷贝到 version buffer，标注上当前事务的 transaction ID，然后直接在 page 上原地更新。事务提交时，version chain 上标注的 transaction ID 会被修改为对应的 commit TS。对于表中一行记录来说，所有老版本数据分布在各个事务的 private version buffer 中，以新版本指向老版本的方式组成了 version chain。不再使用的老版本数据会被及时 GC。version chain 上每个数据的版本存储在 chain 的下一个节点上，chain 上最后一个节点没有版本信息。\n时间戳：每个事务生命周期内涉及到 3 个时间戳，事务刚开始时会分配 transaction ID 和 start TS，事务在提交时会分配 commit TS。所有事务的 transaction ID 位于 2^63 到 2^64-1 之间，所有事务的 start 和 commit TS 位于 0 到 2^63-1 之间。transaction ID 使用一个 TS 生成器分配，start 和 commit TS 使用另一个 TS 生成器分配，TS 需要保证单调递增。\n可见性：数据的每个版本都有一个时间戳，要么是 commit TS 要么是 transaction ID。事务在读数据的时候从 page 上的数据开始遍历它的 version chain，只有满足这些条件之一的才对当前事务可见：\n当前数据没有版本信息，比如没有 version chain 的数据，或者 version chain 最末尾的数据 当前数据的版本（存储在 version chain 的下一个节点上）等于当前事务的 transaction ID，表明这个数据是当前事务修改的 当前数据的版本小于等于当前事务的 start TS，表明它是一个在 start TS 之前已提交事务的执行结果 比如上图，假设有个 transaction ID 为 Tc 的活跃事务，它的 start TS 是 T2，它读左边这个表时能看到的数据是：\n第 1 行是 A：A 的版本是 Tc，和它的 T 的 transaction ID 相等，可见 第 2 行是 V：B 的版本是 T5，大于 T2，不可见；而 V 的版本是 T2，小于等于 T2，可见 第 3 行是 C：这个 tuple 没有 version chain，可见 第 4 行是 D：D 的版本是 T1，小于等于 T2，可见 面向常规事务的内存 MVCC # 版本管理 # 为了解决内存不足，同时也不影响在线事务的性能，大家通常都会采用 STEAL 的方式将包含未提交数据的脏页和 version chain 一起写入磁盘。这种做法资源利用率不高，事务的执行性能也不好。Umbra 将数据和他们的 version chain 分开，只将原地更新的最新数据（可能没提交）页写入磁盘，version chain 保留在内存中，这样能够减少写放大的问题，同时因为有 WAL，事务的 durability 也能得到保障。\n如上图所示，Umbra 采用了去中心化的方案，为每个 page 单独维护一个 local mapping table，里面存了数据对象（比如 tuple ID）到内存 version chain 的映射关系：\nlocal mapping table 按 page 拆分后和 page 使用同一个 latch 保护，在读写 local mapping table 时避免了整库大锁 version chain 上的节点处于不同的事务的 virtual version buffer 中，事务在更新节点上的数据版本为 commit TS 时也不需要获取 page 上的 latch 那如何找到某 page 的 local mapping table 呢。如果该 page 已经加载到内存中，会直接在存储 page 内容的 buffer frame 上增加一个指向该 local mapping table 内存指针，这样访问 page 内容时就可以直接通过这个指针找到对应的 local mapping table。\n当内存中的 page 被替换到磁盘后，Umbra 将其 page ID 和 local mapping table 插入到内存中维护的 orphans 哈希表中，这个哈希表存储了所有写入磁盘后的 page 的 local mapping table。等之后磁盘上的 page 再次加载进来时就可以通过访问这个哈希表找到对应的 local mapping table，将它和 page 内容加载到内存的 buffer frame 中去，同时也从这个 orphans 哈希表中移除。\n垃圾回收 # Umbra 采用《 Scalable Garbage Collection for In-Memory MVCC Systems》（同样来自 TUM，2019 年发表在 VLDB）中提出的 Steam GC，将垃圾回收工作分散在不同的组件和工作线程中，尽可能以去中心化方式进行 GC。在 Umbra 中，只有 version buffer 和 local mapping table 这两个地方会产生需要清理的“垃圾”，GC 主要围绕它们进行。\n如何清理各个 version buffer 呢？Umbra 维护了两个 ordered linked list。一个是 active list，用来存放所有活跃事务，一个是 recently committed list，用来存放所有已提交的、有数据修改的事务，只读事务不产生新版本数据，可以忽略。在事务提交时获取 active list 中最小的 start TS，这个 TS 表明现在和将来不会再有任何事物去读比它更旧的数据了，之前的版本可以放心清理掉。接着在 recently committed list 中寻找 commit TS 小于它的已提交事务，清除他们的 version buffer。清理 version buffer 可能使一些 version chain 为空，在后续 local mapping table 的 GC 中被清理。\n对于经常访问的热点 page，它们的数据版本能够很快的 GC 掉。但是其他冷 page 可能就得不到处理，它们对应的 local mapping table 遗留在之前提到的 orphans 哈希表中。为了清理这些 local mapping table，buffer manager 会在 page IO 时 GC 这些 local mapping table。\n那如何清理各个 local mapping table 呢？Umbra 在常规的 page 维护工作（比如 page compaction）中新增了 local mapping table 的清理工作。在这些维护工作结束前遍历 local mapping table，清理空的 version chain，最后再清理空的 local mapping table。为了减少遍历 local mapping table 的次数和开销，Umbra 为 local mapping table 维护了一个 number of empty version chains 的统计信息，只有空 version chain 的比例超过一定阈值（比如 5%）时才去遍历和清理 local mapping table。\n恢复 # 在发生系统故障时，内存中的 MVCC 版本信息（主要是 version buffer 和 local mapping table）可以安全的丢失，因为这些信息已经包含在 WAL 中了。在故障恢复后，所有未提交的事务都被 undo，整个数据库恢复到了一致的状态，没有任何活跃事务和 version buffer，数据只会有一个已提交的最新版本，没有 version chain，local mapping table 也为空。之前用来分配 transaction ID 和 start、commit TS 的时间戳计数器也可以重置，重新开始计数。\nUmbra 的一条 WAL record 包含了多个数据对象的 before-after image，在恢复的 undo 阶段或者正常 abort 事务时需要格外注意。undo 某个数据对象时，需要不断扫描该事务的各个 WAL record，找到这个数据的 before-after image，把 page 数据修改为 before image，把数据版本从 version chain 中清除，同时记录对应的 compensation log record。\n实现细节 # Umbra 在实现上尽可能避免中心化的数据结构和操作，提升系统在多核上的 scale 能力。\n每个工作线程会各自维护自己的 active 和 recently committed list。小事务由单个工作线程执行，按照上面的垃圾回收策略做 thread-local 的 MVCC GC。\n大事务可以使用多个工作线程执行，维护在全局的 active 和 recently committed list 中，因为同时执行的大事务不多，这个全局 latch 的锁竞争不会成为主要性能瓶颈。大事务执行时内部也会维护自己的 version buffer，避免 version 分配时和其他单线程执行的小事务产生争用。\n利用原子变量实现了一个轻量化的 latch 来保护 version chain，使得 version chain 的 GC 不会影响 version chain 的并发读。这个轻量化 latch 可以参考《 Scalable and Robust Latches for Database Systems》（同样来自 TUM，2020 年发表在 VLDB）提出的 HybridLatch，也是在 Umbra 上的工作成果。\n面向 Bulk Operation 的磁盘 MVCC # 并不是所有事务修改的数据量都很小能够在内存中完全放下，因此需要一种机制来处理内存放不下的大事务。而从场景来来看，作者认为这样的操作要么是偶发的系统管理任务，要么是用户不小心写的 buggy query。\n另外作者认为仅需要支持串行 bulk operation 即可，原因是每个 bulk operation 理论上都会极大消耗系统的写带宽，而且因为 bulk operation 涉及的数据量很大，并发的 bulk operation 之间很容易冲突，所以允许多个 bulk operation 并发执行没啥好处。而且为了避免 bulk operation 和用户事务冲突而 abort，在 bulk operation 开始后 Umbra 只允许 read-only 的事务并发执行。\n作者在后面提出了一些思路来解决这些限制，使 bulk operation 可以和其他 bulk operation 以及写事务并发，我们先来看整个 bulk operation 的 MVCC 是如何做的。\nVersioning Protocol # 虚拟版本（virtual version）：bulk operation 产生的数据版本称为虚拟版本，bulk operation 仅会生成 created 和 deleted 两种虚拟版本，内存中常规事务产生的数据版本称为物理版本（physical version）\nepoch：Umbra 维护一个全局单调递增的 epoch 生成器，epoch 的范围在 0 到 2^64-1 之间。每个常规事务和 bulk operation 开始时都会获取 start epoch，bulk operation 所产生的数据虚拟版本就是它的 start epoch 值。常规事务获取的是最近已分配的 epoch，而 bulk operation 获取是下一个未分配的 epoch，当 bulk operation 完成时才更新全局 epoch 到它刚才获取的这个 start epoch。这个全局 bulk operation epoch 每次系统重启前都会持久化到磁盘，epoch 计数器的推进也需要记 WAL 确保 durability。\n可见性：基于上面描述的 epoch 机制，可见性就比较简单了。对每个常规事务来说，当且仅当它的 start epoch 大于等于当前数据的虚拟版本时该虚拟版本代表的 create 或 delete 操作才对该事物可见。\n虚拟版本如何跟之前的 local mapping table 以及 version chain 结合起来对外提供服务呢？\n每个 page header 都存了一个 reference bulk load epoch（后面简称 reference epoch），表示当前 page 的某些数据被一个 start epoch 为该值的 bulk operation 修改过，遍历 version chain 时如果遇到虚拟版本，那这个虚拟版本号就是这个 reference epoch。 每个 page 中的数据都维护了 2 个 flag 用来表示是否有 created 或 deleted 虚拟版本，这两个 flag 和 Tuple ID 编码在一起，占据 Tuple ID 的 2 个比特位，没有额外存储开销。bulk operation 修改数据时，首先把该 page 的 reference epoch 修改为它的 start epoch，然后修改该数据的 created 或 deleted 标志位，在 version chain 中新增虚拟版本。 version chain 上的虚拟版本和常规事务的物理版本一起决定了数据的可见性。当且仅当事务的 start epoch 大于等于该 page 的 reference epoch 时该虚拟版本才可见；而物理版本则遵循之前提到一样可见性约束即可。 以上图为例，包含 bulk operation 在内前后有 3 个事务修改了这个 page：\n最开始，一个以 T1 为 commit TS 的常规事务将第 1 个 tuple 从 X 修改成了 A，第 3 个 tuple 从 Z 修改成了 C。 接着一个 start epoch 为 E6 的 bulk operation 删除了第 1 个 tuple，创建了第 2 个值为 Y 的 tuple。这些信息通过 reference epoch 和对应的标志位记录在了 page 中。之后的事务在遍历 version chain 时如果遇到了虚拟版本就从 page 对应的 reference epoch 来比对 epoch 判断是否可见，通过标志位来判断该数据是 created 还是 deleted。 最后一个以 T2 为 commit TS 的常规事务将第 2 个 tuple 的值从 Y 修改成了 B。 在这个状态下，一个 start TS 为 t1，start epoch 为 E5 的事务只会读到 A 和 C，而如果它的 start epoch 为 E6 则能读到 Y 和 C。\n理论上 epoch 和 transaction TS 可以使用同一个 timestamp 生成器，但有些问题需要解决。比如常规事务在提交时需要修改 version chain，把 transaction ID 改成 commit TS 后才能让数据对外可见。如果 bulk operation 也使用这个时间戳分配器，为了维护可见性约束，那么它也需要以相同的方式获取 transaction ID、start、commit TS 等时间戳，并且以相同的方式修改数据的 transaction ID 为 commit TS，这对修改大量数据的 bulk operation 说是无法忍受的。\n同步 # 如何做到 bulk operation 执行过程中没有其他写事物执行呢？Umbra 使用了一个全局 mutex：\n常规的 read-only 事务：不需要获取这个 mutex 常规的写事务：以 share 的方式获取这个 mutex bulk operation：以 exclusive 的方式获取这个 mutex 作者认为这个全局 mutex 带来的锁竞争是可以忽略的，除非正在执行一个 bulk operation。另外可以在 relation 或者 partition 上进行更细粒度的锁同步来降低这个开销，提高 bulk operation 的并发度。\n在 page 中使用 reference epoch 统一表示所有数据的虚拟版本有一个缺陷：reference epoch 用来表示 page 上所有数据的虚拟版本，不能在单个 page 上同时存储不同的 reference epoch。因此在 bulk operation commit 时不会立即释放这个全局排它锁，而是将其降级为共享锁，允许执行常规的读写事务。等 bulk operation 的数据对所有活跃事务可见之后才释放这个共享锁，允许执行新的 bulk operation。\n作者没有在这里直接讲怎样才算是可见，结合后面的垃圾回收机制来看，只要所有活跃事务的 start epoch 都大于等于这个 bulk operation 的 start epoch 后就对所有活跃事务可见了。也不用担心新的 bulk operation 修改了 page 的 reference epoch 导致原本应该可见的虚拟版本不可见了，因为在后续的 page 访问中，如果发现 page 的 reference epoch 小于所有活跃事务的 start epoch，会先进行一次 GC。这样就能保证每次 page 上进行 bulk operation 前不会有遗留有上次 bulk operation 产生的虚拟版本，而每次 page 上的 bulk operation 结束后都只会留下这次 bulk operation 产生的虚拟版本，不影响数据的可见性。\n识别 Bulk Operations # 有两种方式来识别 bulk operatio。一个是用户显式指定，比如执行类似 SET TRANSACTION BULK WRITE 这样的语句。一个是自动推导和检测，Umbra 在查询优化时如果发现可能写入大量数据，就将当前事务设置为 bulk operation，另外如果在查询执行时检测到 version buffer 消耗了大量内存，也会将其设置 bulk operation。\n垃圾回收 # 因为 bulk operation 不会生成任何物理版本，可以采用更懒惰的垃圾回收方法。每次访问包含虚拟版本的页面时都会检查 reference epoch 是否全局可见（小于等于所有活跃事务的 epoch）。当全局可见时，清除页面上所有数据的虚拟版本标志，重置 reference epoch。这些操作在每次获取到锁，访问页面，清理 local mapping table 的同时进行。\nFurther Considerations # NUMA 架构 # 这里主要讨论了 NUMA 架构下的一些优化思路。面向 NUMA 架构需要尽可能避免 cross-socket 内存访问，尽可能使用本地内存完成事务。当事务执行时尽可能将其调度到 page 和其 version chain 所属的 NUMA region 上执行，需要的话也可以采用之前 HyPer 中使用 Morsel-Driven 计算调度方法。\nSerializability # 这篇论文提供的 MVCC 能够满足 SI 隔离级别。在此基础上通过《 Fast Serializable Multi-Version Concurrency Control for Main-Memory Database Systems》（同样来自 TUM，2015 年发表在 SIGMOD）中提出的方法可以进一步实现 Serializable 隔离级别。\n实验结果 # 作者采用了 TATP 和 TPC-C 两个 OLTP 测试集分别测试重读和重写两个场景，测试中尽量调整 PG 和其他系统的配置使其发挥最佳性能，采用足够大的物理内存使大家尽量在内存中完成事务。从结果看，论文中提出的方法在性能上相比 PG 等有几乎 1 个数量级的优势：\n另外作者测试了 TPC-C 过程中内存里 version chain 和 local mapping table 的内存占用，看起来 version chain 的内存占用有抖动，但即使是波峰也很小只有 10MB 左右，看来 GC 还是比较及时：\n作者还做了其他详细的测试，感兴趣的朋友可以详细读下论文原文。\n","date":"7 April 2023","permalink":"/posts/vldb-2022-memory-opotimized-mvcc/","section":"Posts","summary":"夏特古道，2023","title":"[VLDB 2022] Memory-Optimized Multi-Version Concurrency Control for Disk-Based Database Systems"},{"content":" 图片来源于 旅行摄影师唐僧\nIntroduction # Buffer Manager 和 B+ Tree 在 In-Memory 的负载上有很多性能瓶颈，比如将 Page ID 转换成内存指针的 Hash Table 和它对应的全局 Latch，比如访问 B+ Tree 的每个内存节点时需要获取的 Latch 等。为了达到更好的性能，像 H-Store、Hekaton、HANA、HyPer、或 Silo 这样的内存数据库都摒弃了 Buffer Manager 的设计，把数据和索引直接存储在内存中，通过内存指针而不是 Page ID 来高效的访问这些数据。\nlarger-than-RAM 的负载广泛存在的，但目前支持 larger-than-RAM 的内存数据库比如 Anti-Caching 和 Siberia 又因为不支持缓存替换策略显得不够通用。考虑到 SSD 越来越通用了，价格相比 DRAM 便宜了 10 倍，性能虽然也是 10 倍的差距但比起 SATA 却好太多了。于是从性价比的角度看，SSD 和支持缓存替换的 Buffer Manager 就变得有吸引力了起来。\n这篇论文介绍了 LeanStore 的设计与实现。作者通过去中心化的 Pointer Swizzling 和 Page Replacement 实现了工作在 SSD 上的 Buffer Manager，通过这个 Buffer Manager 和 Optimistic Latch Coupling 等关键技术实现了高性能 B+ Tree，让 LeanStore 既能支持 larger-than-RAM 的工作负载，又能提供和内存数据库一样的性能表现。\nBuilding Blocks # LeanStore 的基石有 3 个，分别是 Pointer Swizzling、Efficient Page Replacement 和 Scalable Synchronizatio。这里先简单看看它们各自的原理，后面会有更详细的介绍。\nPointer Swizzling # 内存中 B+ Tree 的 Page 都由 Buffer Manager 管理，访问 B+ Tree 的内存 Page 需要通过 Page ID 从 Buffer Manager 那获取对应的内存指针。通常 Buffer Manager 使用 Hash Table 来存储 Page ID 到内存指针的映射，方便快速知道一个 Page 在不在内存中以及对应的内存在哪。而为了支持并发安全的 Hash Table 读写操作，这个 Hash Table 上有个全局 Latch，这个 Latch 的锁竞争就是传统 B+ Tree 的性能瓶颈之一。\nLeanStore 去除了这个全局 Hash Table 以及对应的 Latch，采用了 Pointer Swizzling 的方案。B+ Tree 每个 Page 都有一个称为 Swip 的引用来表示该 Page 的 Page ID 或内存指针。Swip 本质上是一个 8 字节整数，通过 Pointer Tagging 使用 1 个比特位来区分这 8 字节是 Page ID 还是内存指针。\nEfficient Page Replacement # 一般 Buffer Manager 采用 LRU 或者 Second Chance 的缓存替换策略，这些策略是有额外开销的，比如追踪所有的 Page 访问操作。另外并发对那些热点 Page（比如 B+ Tree 的根节点）更新 LRU 链表、Second Chance 比特位等也存在无法 Scale 的性能瓶颈。\nLeanStore 不再追踪 Page 的访问信息，因为 Buffer Pool 可用的内存通常都很大，经常访问的热数据相比冷数据会更多，与其花费额外的开销追踪这些热数据增加大多数读写操作的负担，不如去追踪这些不经常访问的冷数据。\nLeanStore 将所有 Unswizzled Page 维护在一个 Cooling Stage 中（占整体 Buffer Pool 的 10%）。这个 Cooling Stage 本质是一个 FIFO 队列，刚被 Unswizzle 的 Page 放在队头，当其移动到队尾时则 Flush Dirty Page。处在内存 B+ Tree 中的 Page 称为 Hot Page，处在 FIFO 队列中的 Page 称为 Cooling Page，而处在磁盘 SSD 上的 Page 称为 Cold Page。上图很好的展示了这三种 Page 之间的转换关系。和 Second Chance 策略类似，如果队列中某个 Cooling Page 被内存中的 B+ Tree 再次访问到，它会从 Cooling Stage 中移除出去，重新变成 Hot Page。\nScalable Synchronization # 一般 Buffer Manager 采用 Latch（数据库的 Latch 和操作系统的 Lock 是对等的概念，指的是编程语言中的 Mutex）进行线程间同步。每个内存 Page 都由一个 Latch 来保护。B+ Tree 上为了支持并发读写而使用的 Lock Coupling（在数据库的语义下称其为 Latch Coupling 更准确些）机制也需要依赖这些 Latch 来实现。B+ Tree 的并发性能很大程度受这些 Latch 的影响，尤其是多线程并发对一些热点 Page（比如 Root 节点）加锁时，并发性能会因为锁竞争进一步降低。为了让 B+ Tree 的并发性能在多核 CPU 上 Scale，LeanStore 使用了如下方案：\n引入 Pointer Swizzling 机制，去掉 Buffer Manager 的 Hash Table 和它对应的全局 Latch 引入 Epoch 机制，不再需要为每个 Page 维护 pin counter，并在访问时修改 counter 引入一组乐观的、基于时间戳的原语，用在 Buffer Manager 管理的数据结构上，大大减少了 Latch 的获取次数。在 LeanStore 里，Swizzled Page 上的 Lookup 不需要获取任何 Latch，Insert/Update/Delete 一般也只需要获取叶子结点的 Latch，B+ Tree 的读写操作拥有非常好的性能表现。 LeanStore # 这里开始详细介绍 LeanStore 的主要组件和各自的技术原理。\nOverview # LeanStore 内部主要分为 3 个组件用于实现 Buffer Manager 需要具备的 3 个功能：\n根据 Page ID 判断对应的数据是否在内存中，返回对应的内存指针：如上图上部分的 Buffer Pool 所示，LeanStore 将每个 Page 的内存指针或 Page ID 包装在 Swip 中，并将 Swip 交给其父亲页面管理，不再依赖中心化的 Hash Table Buffer Pool 满了时需要决定哪些 Page 应该保留在内存中：如上图左下角所示，LeanStore 随机挑选所有子页面都 Unswizzle 的 Page 放入 Cooling Stage 中，不再依赖中心化的 LRU List，也不需要每次遍历 Page 时都维护额外的访问信息。 管理所有正在进行的 IO 操作：如上图右下角所示，LeanStore 通过 In-Flight I/O 模块管理所有 Page 的加载操作。 下面依次来看各个模块是如何实现的。\nSwizzling Details # LeanStore 使用 8 字节整数表示 Swip，当 Swip 的 8 字节表示Unswizzle时称为 Swizzled，当表示 Page ID 时称为 Unswizzled。\nLeanStore 不再有全局的 Hash Table 来存储 Page ID 到内存指针的映射。所有 Page ID 到内存指针的映射关系都被去中心化的存储在了各个 Swip 中。如果每个 Page 有多个 Swip 存储在其他 Page 中，Page 剔除时就需要更新所有存储在各个 Page 上的 Swip，而要正确维护和更新这些 Swip 使它们数据一致就需要引入额外的工作机制，会使系统变得复杂和低效。\nLeanStore 简化了这个问题，所有 Page 都只有一个 Swip 存储在其父节点中，兄弟节点之间不再有 Swip 存在。去除兄弟节点之间的 Swip 后，LeanStore 通过 Fence Keys 实现了 Range Scan，通过 Optimistic Latch Coupling 实现了高性能的并发读写，这个在后面会提到。\nLeanStore 要求只有所有的子 Page Unswizzle 后才能 Unswizzle 父 Page，在这个约束下，Buffer Manager 必须能够遍历 Page 上的所有 Swip，找到能够 Unswizzle 的孩子节点。如上图所示，为了避免将 Page 内部信息暴露给 Buffer Manager，每个类型的 Page 都实现了一个 Iteration Callback。Buffer Manager 通过这个 Callback 遍历 B+ Tree，随机寻找一个可以 Unswizzle 的 Page。\n为了 Unswizzle 一个 Page，Buffer Manager 需要找到其父节点，将存储在父节点的 Swip 修改成 Page ID。而为了找到父节点 ，每个 B+ Tree 内存节点维护了一个指向父节点的内存指针。因为父节点一定在子节点之后 Unswizzle，所以子节点中这个指针在它访问时一定有效。\n之前提到每个节点只会有一个 Swip，这样能够简化 Unswizzle 节点时的 bookkeeping 工作，但其实也每个节点也可以有多个 Swip。比如维护多个 Parent Pointer 指向拥有该节点 Siwp 的所有节点（也是 Unswizzle 时需要修改的 Swip）。\n另外除了 LeanStore 这种紧凑型的 Swip 以外，还可以使用 Fat Swip，里面同时存储 Page ID 和对应的内存指针。\nCooling Stage # 当 Buffer Pool 满的时候，会在 Cooling Stage 中维护被随机 Unswizzle 的 Page（占整个 Buffer Pool 的 10%），他们称之为 Cooling Page，他们的内存指针被存储在一个 FIFO 的队列里，最近 Unswizzle 的 Page 放在队头，每次从队尾取出一个 Cooling Page，根据需要 Flush 脏页，然后复用对应的内存，填充从磁盘加载进来的新 Page。\nUnswizzle 的 Page 在内存 B+ Tree 中只会留下 Page ID，当用户线程在这个 Page 被 Unswizzle 后再次访问它时，它可能还处于 Cooling Stge 的 FIFO 队列里，没 Flush 也没被回收复用。LeanStore 会先尝试从 Cooling Stage 的 FIFO 队列里面寻找 Page ID 对应的 Page，找到了就把它从 Cooling Stage 中剔除，重新 Swizzle 加入到内存的 B+ Tree 中。为了加速这个寻找过程，LeanStore 在 Cooling Stage 中引入了一个 Hash Table。访问这个 Hash Table 需要获取对应的 Latch，但因为这个 Hash Table 和它的 Latch 只有在需要磁盘 IO 的时候才需要获取，而磁盘 IO 的开销比锁竞争大多了，所以综合来看这个 Latch 对性能的影响不大。\nBuffer Pool 的缓存替换，包括 Cooling Stage 的维护、Unswizzle Page 和 Swizzle Page 这些操作都不是由后台线程维护的，而是在处理用户请求需要额外内存（比如创建新 Page 或者根据 Page ID 加载 Page）时进行的，工作线程会根据当前需要 Unswizzle Buffer Pool 中的 Page 使其不超过配置的阈值。\nInput/Output # 当同一个页面同时被多个线程并发加载进 Buffer Pool 时可能出现正确性问题，而且同时加载相同的 Page 也很低效。LeanStore 会管理所有的 IO 请求，这一点和其他 Buffer Manager 一样。它用 Hash Table 来存储 Page ID 到 I/O Frame 的映射，每个 I/O Frame 包含一个 Latch 和一个指向 Buffer Frame 的指针。触发页面加载的线程首先获取 Hash Table 上的 Latch，创建一个 I/O Frame 并获取这个 Frame 的 Latch。然后释放 Hash Table 的全局锁，之后用一个阻塞的系统调用加载页面。其他线程会找到已经存在的 I/O Frame，阻塞在获取它的 Latch 上，当第一个线程完成页面加载释放 Latch 后，该页面对所有线程可见了。\n同样，这个 Hash Table 上的 Latch 相比磁盘 IO 来说开销也不大。\nBuffer-Managed Data Structures # 和存储于 Buffer Pool 中的 Page 一样，其他由 Buffer Pool 管理的数据结构也需要实现这个 Iteration Callback，方便 Buffer Manager 找到能够 Unswizzle 的对象。\n原文这一小节介绍了页面的 Unswizzle 过程，在前面的 “Swizzling Details” 中我们已经包含了这部分内容，这里不再继续重复。\nOptimistic Latches # Optimistic Latch 内部有个版本计数器，每当数据更新发生后就增加版本计数器的值。读数据的时候不需要获取任何 Latch，而是检查读取数据前后版本计数器是否变化，和 Optimistic Concurrency Control 类似。LeanStore 在 Optimistic Latch 的基础上实现了 Optimistic Latch Coupling：\nRead 操作遍历树时不加 Latch，在读数据时通过每个 Optimistic Latch 内的版本计数器来验证是否冲突，冲突则重试 跨越多个叶子结点的 Range Scan 通过 Fence Keys 转换成了各个叶子结点上的 Read Insert、Delete、Update 操作先像 Read 一样不加 Latch 从根节点遍历到叶子结点，然后对需要修改的叶子结点加 Latch，排除其他 Writer，也使潜在 Reader 失效。如果不涉及到树结构变更，则直接修改当前叶子结点，然后释放 Latch。如果需要修改树结构，需要重新遍历整个树，这时对受影响需要修改的内部页面都加 Latch，然后完成这次修改。 这个 Optimistic Latch 挺有意思的，在 TUM 2016 发表在 DaMoN 上的《 The ART of Practical Synchronization》论文中提出，后面有时间仔细研究下。\nEpoch-Based Reclamation # 在 Optimistic Latch 机制下，读 Page 不会对获取它的 Latch，而这时候如果有其他线程想要剔除或者删除这个 Page 就会导致内存问题。为了避免这些潜在问题，LeanStore 引入了基于 Epoch 的 Page 回收机制。\nLeanStore 通过 Epoch Manager 周期性的推进 Global Epoch，每个线程维护一个 Thread Local Epoch，在访问 Page 之前先去读取 Global Epoch 来推进自己 Thread Local Epoch，访问 Page 结束后将自己的 Epoch 设置为特殊值（比如 ∞）表示当前该线程没有访问任何 Buffer Pool 中的数据。\nCooling Stage 的每个 Cooling Page 都有一个对应的 Epoch，这个 Epoch 是在这个 Page 成为 Cooling Page 入队时从 Global Epoch 获取的，所有在这个 Epoch 之后的线程都无法通过内存指针访问到这个 Cooling Page，因为它对应的 Swip 已经被 Unswizzle 了。\n当一个 Cooling Page 到达队尾时，LeanStore 会检查是否它的 Epoch 小于所有活跃的 Thread Local Epoch，确保没有线程还持有这个 Cooling Page 的内存指针，只有满足这个条件时，队尾的 Cooling Page 才能安全的被剔除和复用。在上图中：\nP8 的 Cooling Epoch 是 e4，小于所有 Thread Local Epoch，因此可以安全的剔除 P2 的 Cooling Epoch 是 e7，大于 Thread 3 的 Epoch e3。Thread 3 访问 Page 的时间早于 P2 进入 Cooling Stage，可能持有 P2 的内存指针正在读 P2 的内容，因此不能安全的剔除。 这样一来 Global Epoch 的推进周期就很关键了。推进的太快会导致其他 CPU Core 的 Cache 频繁失效，推进的太慢会导致 Unswizzled Page 迟迟无法释放和复用。\n另外线程访问 Page 的时间也不能太长，不然无法及时推进 Epoch 也会阻塞 Cooling Page 的剔除和复用。Page 上的 Scan 可以划分成一个个小的 Scan 来及时推进 Thread Local Epoch。对于耗时很长的 IO 操作，LeanStore 如果发现当前要获取的 Page 不在内存中，在释放完所有 Lock 后就退出当前 Epoch，然后执行这个 IO 操作把对应的 Page 加载到 Buffer Pool 中，最后再重试一开始要进行的 B+ Tree 操作。这样 IO 操作发生在 Epoch 之外，不会阻塞 Cooling Page 的剔除和释放。\nMemory Allocation and NUMA-Awareness # 除了支持 larger-than-RAM 的数据结构，Buffer Manager 也提供了内存管理的功能。因为 Buffer Pool 中 Page Size 是固定大小的，这个内存管理会更简单，也可以避免内存碎片的问题。Buffer Pool 可以根据需要（比如第一次访问时）由操作系统分配，也可以在启动时预先一次性分配好。\nLeanStore 支持 NUMA-Aware 的内存分配策略，每个 NUMA Node 都有一个 Buffer Pool 和 Free List 的 Partition。当线程需要内存时优先分配当前 NUMA Node 上的内存，只有在本地 Partition 没有空余页面时才在其他 NUMA Node 上分配。\nImplementation Details and Optimizations # 这里讲了一些 LeanStore 的实现和优化细节。\nLeanStore 的 Buffer Frame 和 Page 内容是 Interleave 在一起的，目的是为了提升数据的 Cache Locality，同时避免 CPU Cache Associativity 导致的性能问题。\nLeanStore 每个线程维护了一个 Thread Local Cache，用来存放那些因为 Page 合并操作而删除的 Page。这个 Thread Local Deleted Page Cache 和 Cooling Stage 中的 Cooling Page 一样采用基于 Epoch 的淘汰替换策略。当一个线程需要新的空 Page 时优先从本地的 Deleted Page Cache 获取，本地没有时才去 Cooling Stage 取。一般情况下 Epoch 会及时推进，本地 Deleted Page 也会在之后被当前线程继续复用，保证了良好的 Cache Locality。\nLeanStore 采用了一个后台线程异步的将需要淘汰的 Dirty Page 写回磁盘。这个后台线程周期性的遍历 Cooling Stage 中的 FIFO 队列，把队尾可以淘汰替换的 Cooling Page 写回磁盘。\n为了支持低延迟的 Scan 操作，LeanStore 实现了 IO Prefetch。通过前面提到的 In-Flight IO 组件一次调度多个 Page 的 IO 任务，这些任务会在后台执行，IO 结束后对应的 Page 放在 Cooling Stage 中，等待其他线程访问。\n另外一个优化是支持通过 Hint 使刚被加载和读取的 Page 保持在 Cooling Stage 中，而不是变成 Hot Page 存放在内存 B+ Tree 里，通过这种方式可以避免 Buffer Pool 被污染。\nIn-Memory Evaluation # Experimental Setup # 作者同时实现了一个纯内存 B+ Tree 和一个基于 Buffer Manager 的 B+ Tree 用来对比验证 Buffer Manager 的性能开销。使用 Berkeley DB 6.0（用在 Oracle NoSQL Database）和 WiredTiger 2.9（用在 MongoDB 中），将他们配置为 B+ Tree 模式进行 TPC-C 的对比测试。测试过程中关掉 Transaction、Logging、Compaction 和 Compression 等操作。这里主要是使用 TPC-C 的复杂 workload 和数据访问模式来测试基于 Buffer Manager 的 B+ Tree 性能表现。\n操作系统：Linux 4.8 system with an Intel Xeon E5-2687W v3 CPU (3.1 GHz, Haswell EP, 10 cores, 20 hardware threads)，64 GB 内存。\nTPC-C # 从单线程的测试来看，LeanStore 带 Buffer Manager 的吞吐和内存 B+ Tree 的吞吐差不多，相比 BerkeleyDB 和 WiredTiger 有好几倍的提升。\n接着按照 Pointer Swizzling、Replacement Strategy 和 Optimistic Latch 的的顺序逐渐开启这些优化，来看他们对性能提升的帮助分别有多大。单线程情况下 Pointer Swizzling 和 Replacement Strategy 对性能提升帮助最大。10 线程的结果是单线程结果的 9 倍，接近线性提升。多线程的结果也能看出来 Pointer Swizzling 和 Replacement Strategy 中去掉的两把全局锁对性能提升的巨大收益。\n多线程情况下 LeanStore 相比 WiredTiger、BerkelyDB 的性能差异随着线程增多而增大。LeanStore 的线性扩展能力是最好的。\nScalability on Many-Core Systems # 接着测试了一个有 60 Core 的 NUMA 系统，它由 4 个 15 Core 的 Intel Xeon E7-4870 v2 (Ivy Bridge, 2.3GHz) CPU 组成，没有 L3 Cache。前后分别做了 3 个优化：\nWarehouse Affinity：每个 Worker 线程处理一个 Local Warehouse，是一种针对 TPC-C Workload 的优化，相比 Baseline 性能提升 50.4 倍 Pre-Fault Memory：这个应该是预先分配好 Buffer Pool 的内存，避免系统运行期间因为操作系统分配内存导致的一系列问题，相比 Baseline 性能提升 52.7 倍 NUMA Awareness：远端内存访问降低到了 14%，相比 Baseline 提升了 56.9 倍，这个性能提升更加明显一些。 OUT-OF-MEMORY EVALUATION # 实验硬件采用 PCIe-attached 400GB Intel DC P3700 SSD，可以提供 2,700/1,080 MB/s 的顺序读写吞吐，以及性能相近的随机 IO 吞吐。测试采用 Direct IO，避免操作系统缓存使性能更稳定。\n作者在测试的时候发现了一些操作系统和文件系统的问题：\nwe ran into a number of Linux scalability issues, in particular when performing many random reads and writes simultaneously. We also found the file system (ext4) to be a scalability bottleneck, even though our database is organized as a single large file. TPC-C # LeanStore、BerkeleyDB、和 WiredTiger 的 Buffer Pool 都设置为 20GB，同时内存 B+ Tree 开启 Swap 也加入对照测试。在 500MB/s 的 IO 情况下 LeanStore 的性能一直很高（不过看起来 TPC-C 性能有些波动）。BerkelyDB 的表现比较有意思，作者说是因为它的性能太差了，10 分钟才能填满 Buffer Pool，所以上面这个 1 分钟的图来看它的表现很平稳。\n作者接着在冷启动的情况下对比了不同磁盘的性能表现：\nPCIe SSD (Intel DC P3700 SSD)：8 秒钟达到峰值性能 SATA SSD (Crucial m4)：35 秒达到峰值性能 A magnetic disk (Western Digital Red)：15 分钟后才达到峰值性能 Point Lookups # TPC-C 是一个 Insert Heavy 的测试集，那些大表比如 stock、customer 的访问模式非常随机，最大的表 orderline 访问模式也很奇怪。作者为了进一步评估 LeanStore 的读性能，设计了几个 Read-Only 的和 YCSB Workload C 很像的 Benchmark。\n使用 Uniform 和 Zipf 分布生成了 5GB 的数据集进行了几组点查测试，使用 1GB 的 Buffer Pool，Cooling Stage 容量设置为 10%，Key 固定 8 字节，value 120 字节，总共 41M 个 Key-Value。\n这里展示了不同数据倾斜情况下，固定 Cooling Stage 为 10% 的吞吐表现。可以看到的是数据倾斜越程度越高，测试过程中的 IO 越少，最终的吞吐也越高，算是符合预期。\n这里展示了不同 Cooling Stage 大小在不同数据倾斜情况下的吞吐表现。目的是想看什么样的 Cooling Stage 容量是合适的。以 Cooling Stage 为 10% 的性能表现作为基准（图中的灰色横线）。一些图细看还是比较有意思。综合来看 10% 是个比较不错的 Cooling Stage 配置。\n作者也比较了 1GB Buffer Pool，5GB 数据量，Zipf Factor =1.0 的情况下不同缓存替换算法的 Page 命中率，虽然比 LRU 和 2Q 在小数点后面有所下降，但考虑到维护负担后 LeanStore 的 Replacement Strategy 会有更好的性能。\nScans # 作者最后测试了 Full Table Scan 的性能。使用 TPC-C 400 warehouse 的数据，order 表 0.7GB，orderline 表 10GB。Buffer Pool 从 2GB 测试到 12GB 得到了上面的图。两个表各用一个线程分别在不停的 Scan。\n从 Scan Speed 来看，order 表因为只有 0.7GB 能够全部放在内存中，它的 Scan 性能没有受到 orderline 表和 Buffer Pool 大小（最小也是 2GB）的影响。而 orderline 表的性能在 10GB Buffer Pool 配置下的性能表现比较有意思，在第 25 秒和 50 秒有一个 IO 尖峰，同时 Scan 性能也相应的掉下来了，作者解释说这是因为每次数据都会从头到尾全部扫描一遍，刚好 25 秒读完了所有 orderline 表的数据，所以看到了这么个现象。\nSummary # LeanStore 这篇论文整体看下来感受比较深刻的是非常注重多核 CPU 的 Scalability，通过 Pointer Swizzling、Page Replacement、Optimistic Latch Coupling 等尽量消除 Latch 竞争，极大的提高了系统的吞吐，使得它在处理 larger-than-RAM 负载的情况下还能达到内存数据库的性能，太强了。\n","date":"3 April 2023","permalink":"/posts/icde-2018-leanstore/","section":"Posts","summary":"图片来源于 旅行摄影师唐僧","title":"[ICDE 2018] LeanStore: In-Memory Data Management Beyond Main Memory"},{"content":" 本文封面图片来自 自然摄影师雷雨\n简介 # 这篇论文介绍了 TUM 的通用数据库 Umbra，它基于 SSD，能高效处理任意大小的数据集，是内存数据库 HyPer 的继任者。Umbra 的关键实现包括：不定长 page 和专用 buffer manager，pointer swizzling 和 versioned latch 等多核优化，高效的 log 和 recover 算法，代码生成等。\nUmbra 是 LeanStore（基于 SSD 的定长 page 数据库）的演进，两者有许多相似之处。建议看 Umbra 前先看 LeanStore 的论文，或者看我写的这篇文章： [ICDE 2018] LeanStore: In-Memory Data Management Beyond Main Memory。\nBuffer Manager # LeanStore 的 Buffer Manager 管理定长 page，虽然高效，但需要额外处理可变大小的 tuple，增加系统复杂性和性能开销。\nUmbra 的 Buffer Manager 管理变长 page，如图所示，page 按 size class 分类，每个 size class 的 page 大小是前一个的 2 倍。最小 64 KB，最大能覆盖整个 Buffer Pool。Buffer Manager 只有一个 Buffer Pool，管理所有 size class。Buffer Pool 的内存容量是全局配置的，不需为每个 size class 单独配置。默认情况下，Buffer Pool 只用可用内存的 50%，剩下的给查询执行。\nBuffer Pool Memory Management # 内存碎片化是 Buffer Pool 支持多个 size class 的主要挑战。\nUmbra 通过操作系统的虚拟地址和物理地址映射来解决这个问题。操作系统通过页表将用户的虚拟内存地址转换为实际的物理地址。这样，连续的虚拟内存可以分散地存储在碎片化的物理内存中，同时虚拟内存分配也可以和物理内存分配解耦开，用户程序可以在不实际分配物理内存以及不建立虚拟内存到物理内存地址映射的情况下分配一块虚拟内存。\nUmbra 的 Buffer Manager 使用 mmap 来实现上述目标。具体来说，每个 size class 都被分配了一个足够装下整个 buffer pool 大小的虚拟内存，通过配置 mmap 系统调用使其仅分配虚拟内存地址不分配实际物理内存，然后将每个 size class 内的虚拟内存按照 page size 切分成一个个 chunk，每个 buffer frame 都包含一个指向对应 chunk 的内存指针。这些内存指针创建出来后就不再改变，后续 buffer manager 在需要时就将 page 从磁盘加载到这个虚拟内存地址对应的物理内存中。\n物理内存分配：Umbra 用 pread 读磁盘 page 到 buffer frame，操作系统分配物理内存（可能不连续），建立虚拟地址和物理地址的映射。\n物理内存释放：Umbra 用 pwrite 写 buffer frame 到磁盘文件，用 madvise 传 MADV_DONTNEED 标志，让操作系统回收物理内存。因为 mmap 没映射磁盘文件，madvise 开销很小。\nBuffer Manager 跟踪物理内存使用情况，保证 buffer pool 不超配置容量。Umbra 和 LeanStore 的缓存替换策略相同，内存 page 先进 cooling stage 的 FIFO 队列头，到队列尾再驱逐。\nPointer Swizzling # 为了把页面序列化到磁盘中，页面不能用内存指针而要用 page ID（PID）来引用。一种常见的方法是用一个全局哈希表来映射 page ID 和内存指针，但这样每次访问页面都要获取 latch 来操作哈希表，会导致 latch contention 的性能问题。\n和 LeanStore 一样，Umbra 采用了 pointer swizzling 来解决这个问题。pointer swizzling 是一种在序列化和反序列化时把基于名字或位置的引用转换成直接的内存地址的过程。Umbra 的每个页面都有一个 Swip 来访问子节点，Swip 是一个 64 位整数，最低位是 0 表示它是内存指针，最低位是 1 表示它是 page ID（需要从 cooling stage 或磁盘中加载页面）。\n对于 page ID 的 Swip，Umbra 还用了 6 比特来表示页面所属的 size class，因为 Umbra 使用了变长页面。剩下的 57 比特表示实际的 page ID，范围是 0 到 2^57-1。Umbra 要求每个页面只有一个 Swip，方便缓存替换时更新 Swip 值。\nVersioned Latches # Umbra 用一个 64 位整数的原子变量实现了 versioned latch，支持 exclusive、shared 和 optimistic 三种上锁模式，以减少 latch contention。versioned latch 的 5 比特位表示锁的状态，0 为无锁，1 为 exclusive 锁，2 及以上为 shared 锁。shared 锁允许多个线程同时读取被保护的数据，exclusive 锁则只允许一个线程读写。versioned latch 的剩余 59 比特位表示版本计数器，每次修改数据时递增。optimistic 模式下，线程不会真正上锁，而是通过比较数据读取前后的版本计数器来判断是否需要重试。\nUmbra 和 LeanStore 的区别在于，Umbra 支持 shared 模式上锁，而 LeanStore 只支持 optimistic 模式。这是因为 optimistic 模式可能需要重试，对于读写比重不明或者重试代价高的数据和查询来说，性能损失太大。\n另外 Umbra 的 versioned latch 在后面演进成了 Hybrid-Latch，在 VLDB 2023 的这篇论文中通过伪代码详细介绍了各个模式的 latch 实现，论文不长，感兴趣朋友阅读一下：《 Scalable and Robust Latches for Database Systems》。\nBuffer-Managed Relations # Umbra 用 B+ 树存储 tuple，用单调递增的 8 字节 tuple ID 作为 Key。这样内部节点固定为 64 KiB，扇出为 8192，插入效率均匀。只有当 tuple 超过现有 page 容量时，才会分配新的叶子节点，一般都是 64 KiB。tuple 采用 PAX 格式，固定大小字段列式存储在页面开头，可变大小字段密集存储在页面末尾，插入时可压缩。这种页面布局下，获取部分字段可能导致所有字段都加载到内存中，Umbra 未来计划在将来进行优化，比如用 DataBlock 格式（参考：《 Data Blocks: Hybrid OLTP and OLAP on Compressed Storage using both Vectorization and Compilation》）优化冷数据。\nB+ 树的并发访问通过 versioned latch 实现的 optimistic latch coupling 完成。写线程通过 exclusive latch 修改内部或叶子节点，读线程通过 shared latch 读取叶子节点或加载子节点。非修改遍历用 optimistic latch。Umbra 的 B+ 树节点没有兄弟指针，通过 fence key 实现了 range scan。为避免重试和整树遍历，读线程持有父节点的 optimistic latch，读完一个叶子节点后尽量直接从父节点读兄弟节点。\nRecovery # Umbra 采用 ARIES 恢复算法。因为采用了变长 page，在磁盘空间复用时不能让多个小 page 复用大 page 的磁盘空间。比如下面这个例子：\n一个 128 KiB 的数据库文件，它当前完全被一个 128 KiB 的页面占用。将此页面加载到内存中，删除它，并创建两个新的 64 KiB 页面，这些页面重用该文件中的磁盘空间。系统崩溃时，可能只有 WAL 写入磁盘了，而实际的新页面数据还没写入。在恢复期间，ARIES 会从该文件读取第二个 64 KiB 页面的 LSN，而实际上读取上来的是已删除的 128 KiB 页面的某些数据，并错误地将其解释为了 LSN，导致后续故障恢复出现异常情况。\nUmbra 解决思路比较简单，仅在 size class 内部支持磁盘空间复用。\nFurther Considerations # Umbra 采用了变长 page，实现了对应的 buffer manager。其他模块都需要在这个基础上进行适配，论文提了字符串和统计信息收集两个方面。\nString Handling # 因为 page size 是变长的，一个 string 也不需要被拆分成多段了，Umbra 简单的将其存成 length + data 两部分：\nUmbra 将字符串分为两个部分来存储：一个包含元数据的 16 字节 header 和一个包含实际数据，可变大小的 body。header 和其他固定大小的属性一样存储在页面开头的列式布局中，而实际字符串数据则存储在页面末尾，长字符串也不需分割（放不下就触发分裂，然后 buffer manager 分配一个能放下它的叶子结点）。\nheader 的前 4 位用于存储字符串长度，而后 12 位根据字符串长度有两种编码格式：\n对于不长于 12 字节的短字符串，它们的数据直接存储在 header 的后 12 位内 后 12 位的 4 位用于存储字符串的前四个字符，允许 Umbra 快速进行一些字符串比较，剩下的 8 位存储其数据的指针或某个已知位置的偏移量 由于长字符串存储在其他物理页面上，这些页面可能没有加载到内存中，因此这些字符串需要特殊处理：Umbra 引入了三种存储类别，即持久、瞬态和临时存储。存储类别编码在字符串头部存储的偏移量或指针值的两个比特中：\n持久存储：对持久存储的字符串引用（例如查询常量）在数据库运行时间内始终有效 瞬态存储：对瞬态存储的字符串引用，只有在当前工作单元正在处理时有效 临时存储：由查询执行实际创建的字符串（例如 UPPER 函数）具有临时存储期 Statistics # Umbra 中支持的统计信息主要是每个表上的随机采样和每个列上可更新的 HyperLogLog。Umbra 实现了一个可扩展的在线蓄水池采样算法，参考：《 Scalable Reservoir Sampling on Many-Core CPUs》。相比 HyPer 仅支持定期更新样本的方式，Umbra 可以用最小的开销确保优化器始终具有最新的样本。\nCompilation \u0026amp; Execution # Umbra 大致上采用了 HyPer 的执行策略：逻辑查询计划被转换为高效的并行机器码，然后执行以获得查询结果。Umbra 采用了比 HyPer 更细粒度的物理执行计划。HyPer 的物理执行计划本质上是一个整体代码片段，Umbra 的物理执行计划则被表示为模块化状态机。以下面的 query 为例：\nselect count(*) from supplier group by s_nationkey 对上面这个 TPC-H 查询，HyPer 会采用两个 Pipeline 来执行它，第一个 Pipeline 扫描 supplier 表并执行 group by 操作，第二个 Pipeline 扫描每个 group 的数据并打印查询输出。在 Umbra 中，这些 Pipeline 进一步分解为 step，每个 step 可以是单线程的也可以是多线程的。上述 query 的 pipeline 和 step 如下图所示：：\n在生成的代码中，每个 step 对应一个单独的函数，可以由 Umbra 的 runtime system 调用。在查询执行时，通过这些 step 完成 pipeline 内的状态转换，step 的执行由 Umbra 的查询执行器协调。多线程 step 采用 morsel-driven 的方式执行。\n这样做的好处有很多：\n可以在每次调用 step 函数后暂停查询执行，比如系统 IO 负载超过某个阈值可以及时暂停。 查询执行器可以在运行时检测并行 step 是否仅包含单个 morsel，这种情况下不必将所需的工作分派到另一个线程，从而避免潜在的上下文切换开销。 可以轻松支持 pipeline 内的多个并行 step，如上图所示 另一个和 HyPer 不同的地方是代码不是直接生成成 LLVM IR，而是在 Umbra 中实现了一个自定义的轻量级 IR，这使 Umbra 能在不依赖 LLVM 的情况下高效（省去一些额外开销，能够比 LLVM 更高效）地生成代码。\nUmbra 不会立即将 IR 编译为优化后的机器码。Umbra 采用了自适应编译策略，用来权衡每个 step 的编译和执行时间。step 的 IR 先被转换为高效的字节码，由 Umbra runtime system 解释执行。对于并行 step，自适应执行引擎会跟踪执行进展以决定编译是否有益，如果是，则将 Umbra IR 转换为 LLVM IR，然后交给 LLVM JIT 编译后执行。\nExperiments # 作者测试了 TPC-H 和 JOB 两个 Benchmark 10GB 的数据，每个查询重复五次，选取最快的重复结果（也就是充分预热后的结果）。和 Hyper 相比，Umbra 的性能提升明显，主要来自自适应的 IR 编译。特别是在 JOB 上，Umbra 的 geometric mean 提升为 3.0×，在 TPC-H 上为 1.8×。在这些查询中，HyPer 实际上在查询编译上花费的时间远远超过查询执行时间，最多达到 29×。\n而如果仅看纯计算开销，Umbra 也要好于 Hyper。平均而言，在 JOB 上执行时间波动约为 30%，在 TPC-H 上波动约为 10%。JOB 上的相对差异较大，大多数是由于 HyPer 和 Umbra 选择了不同的执行计划。尽管 Umbra 的基数估计要好得多，但它偶尔会选择比 HyPer 更差的逻辑计划。这是因为 Hyper 里出现了负负得正的 Join Order 优化效果。\n作者还单独测试了 buffer manager，感兴趣的朋友可以详细阅读论文这一节，本文不再展开。\n","date":"1 April 2023","permalink":"/posts/cidr-2020-umbra/","section":"Posts","summary":"本文封面图片来自 自然摄影师雷雨","title":"[CIDR 2020] Umbra: A Disk-Based System with In-Memory Performance"},{"content":"依稀记得 CMU 有篇 paper 讲了使用 mmap 的各种问题，好奇之前大家使用 mmap 过程中遇到了哪些问题，于是抽空读了这篇论文，加深了我对数据存储、文件 IO 的理解。\nBackground # MMAP Overview # 使用 mmap 读写文件内容的步骤：\n程序调用 mmap，得到指向内存映射文件内容的内存指针 操作系统保留程序的虚拟地址空间的一部分，但不加载文件的任何部分 程序使用指针访问文件的内容 操作系统尝试检索页面 由于指定的虚拟地址没有有效的映射，因此操作系统会触发 page fault，将文件的引用部分从磁盘加载到物理内存页中 操作系统向 page table 添加一个条目，将虚拟地址映射到新的物理地址 当前的 CPU Core 将此条目缓存在其本地的 TLB 中，以加速将来的数据访问 Evict page 会出现一个叫 TLB Shootdown 的严重性能问题。操作系统需要从 page table 和每个 CPU Core 的 TLB 中删除它们的映射关系。清除触发中断的 CPU Core 的本地 TLB 很简单，但清除其他 CPU Core 上的 TLB 会很麻烦，因为当前 CPU 不为远端 TLB 提供一致性，操作系统必须发出昂贵的处理器间中断来刷新远端 CPU Core 的 TLB。从后面的实验结果能看到 TLB shootdown 会带来非常巨大的性能损失。\nPOSIX API # mmap：这个调用会使操作系统将文件映射到 DBMS 的虚拟地址空间中，DBMS 可以使用普通的内存操作读写文件内容，通常使用下面两个 flag 来控制 update page 的行为：\nMAP_SHARED：操作系统会先将 page 缓存在内存中，由操作系统控制何时最终写回文件 MAP_PRIVATE：创建一个仅对调用者可访问的 copy-on-write 映射，page update 不会持久化到文件中。 madvise：使得 DBMS 可以向操作系统提供数据访问模式的 hint，可以是整个文件的粒度，也可以是特定 page 范围。《 madvise(2) — Linux manual page》 列举了完整的 hint 和它们对应的功能。论文描述了三个常用的 hint：MADV_NORMAL、MADV_RANDOM 和 MADV_SEQUENTIAL\nMADV_NORMAL：默认 hint，它会获取访问的 page，以及它前 15 个和后 16 个 page。对于 4KB 的 page，即使调用者只请求单个 page，也会导致操作系统从磁盘中读取总共 128KB 的数据 MADV_RANDOM：只读必要的页面，对于大于内存的 OLTP workload 来说是更好的选择。MADV_SEQUENTIAL：预读给定范围内的页面，访问后不久可能被释放，对于具有顺序扫描的OLAP workload 来说会更好。 mlock：使得 DBMS 可以将 page 固定在内存中，确保操作系统永远不会将它们 evict 出内存。不过即使 page 被固定在内存中，操作系统也可以随时将脏页（修改后的 page）刷到文件中。DBMS 不能使用 mlock 来保证脏页永远不被持久化，误用可能带来事务正确性问题。\nmsync：显式地将指定的内存范围 flush 到文件。不显式调用 msync，DBMS 不能保证更新被持久化到文件中。\nMMAP Gone Wrong # 上面是一些使用过（比如 MongoDB），或正在使用 mmap 作为 buffer manager（比如 LevelDB），或提供 mmap 作为 buffer manager 选项（比如 WiredTiger）的 database，作者还举了 MongoDB、InfluxDB、SingleStore 等几个使用 mmap 遇到问题的案例，这些故事比较有意思，有时间可以详细了解下。\nProblems With MMAP # 主要是 4 个问题：transactional safety、I/O stalls、error handling，以及 performance issues。前面 3 个都能通过一些额外的稍显复杂的机制来解决或绕过，但性能问题除非修改操作系统内核代码否则无法有效避免。\nTransactional Safety # 核心问题是：由于透明分页，操作系统可以在任何时候将脏页刷新到二级存储器中，而不管写入事务是否已提交。DBMS 无法阻止这些刷新，并在其发生时也不会收到任何警告。\n论文将解决办法分成了 3 种：OS copy-on-write、user space copy-on-write，以及 shadow paging。\nOS Copy-On-Write # MongoDB 的 MMAPv1 存储引擎采用了这个策略。做法是：使用 mmap 和 MAP_PRIVATE flag 创建两个数据库文件副本，启用操作系统页面的写时复制功能，它们最初都指向相同的物理页面。第一个副本作为主副本，第二个副本则作为私有工作区供事务更新使用。当事务提交时，DBMS 将相应的 WAL 记录刷新到二级存储器，并使用单独的后台线程将提交的更改应用于主副本。\nOS copy-on-write 的问题：\n在允许冲突事务运行之前，DBMS 必须确保提交的事务的最新更新已传播到主副本，这需要额外的 bookkeeping 来跟踪待更新的页面。 随着更新的不断发生，私有工作区将持续增长，DBMS 最终可能会在内存中拥有两个完整的数据库副本。也有办法来解决这个问题，只是实现比较复杂，也会引入额外性能开销，这里不再详细描述。 User Space Copy-On-Write # SQLite、MonetDB 和 RavenDB 采用了这个策略，做法是手动将受影响的页面从 mmap 的内存复制到用户空间中一个单独维护的内存 buffer 内。 在处理更新操作时，DBMS 仅将数据更新应用于单独维护的内存副本并创建相应的 WAL 记录。写完 WAL 即可提交事务，同时也将副本的页面复制到mmap的内存中，交给操作系统去刷盘。\nShadow Paging # LMDB 基于 System R 的 shadow paging 设计采用了这种策略，做法是维护数据库的单独主副本和影子副本，这两个副本都由 mmap 支持。 处理更新操作时，DBMS 首先将受影响的页面从主副本复制到影子副本，然后应用更新操作。commit 更新操作需要使用 msync 将修改后的影子页面刷新到二级存储器，然后更新指针以将影子副本设置为新的主副本，原始主副本作为新的影子副本。\nShadow paging 需要注意的地方：必须确保事务不会冲突或看到部分更新。例如，LMDB 通过仅允许单个写入者来解决此问题。\nI/O Stalls # mmap 不支持异步读取。因为 DBMS 无法知道页面是否在内存中，如果访问到已经驱逐的页面就会导致意外的 I/O 停顿。这一点和传统的 buffer manager 可以使用 libaio、io_uring 异步读写页面有很大不同。\n如何避免呢，其中一个思路是使用 mlock：用 mlock 来固定 DBMS 在不久的将来希望再次访问的页面。但使用 mlock 也是有限制的，因为固定太多页面可能会对并发运行的进程甚至操作系统本身造成一些严重问题，操作系统通常限制单个进程可以锁定的内存量。在这个限制下使用 mlock，DBMS 还需要仔细跟踪和取消固定不再使用的页面，以便操作系统可以驱逐它们。\n另一种思路是通过 madvise 告诉操作系统查询的读写模式。例如，需要执行顺序扫描的 DBMS 可以向 madvise 提供 MADV_SEQUENTIAL flag，告诉操作系统在读取页面后驱逐页面并预取下一个将要访问的连续页面。但这种方式也不是很完美：\nmadvise flag 只是 hint，操作系统可以自由忽略 向操作系统提供不恰当的 hint（例如，随机访问模式下使用 MADV_SEQUENTIAL）可能会对性能产生严重影响 还有一种办法是采用额外的后台线程 prefetch 这些可能访问的 page，尽可能 block 它们而不是主线程。不过这样的代码复杂度很高，违背了一开始因为简单而使用 mmap 的初心。\nError Handling # 使用 mmap 时，DBMS 需要在每次访问页面时验证 checksum，因为操作系统可能会在上一次访问后的某个时间点透明地驱逐页面。而在传统的文件 IO 模式下，只需要在页面从文件中读取时验证 checksum 即可。\n使用 mmap 会导致损坏的页面被静默地持久化到文件中，这仍然是因为操作系统可能会透明地驱逐页面导致的。而在传统的文件 IO 模式下，可以在数据写入页面时进行一些约束检查以确保数据没有写坏。\n当使用 mmap 时，优雅地处理 I/O 错误变得更加困难，任何与 mmap 支持的内存交互的代码都可能产生 SIGBUS，DBMS 必须通过繁琐的信号处理程序来处理它们。\nPerformance Issues # 基于 mmap 的文件 I/O 有三个关键性能瓶颈：page table 上的锁竞争、单线程页面驱逐、以及 TLB shootdown。作者认为 mmap 透明分页的最大缺点就是性能问题，如果没有操作系统级别的重新设计，这些问题无法避免。\n单线程页面驱逐：作者发现，对于高带宽二级存储设备（比如 PCIe 5.0 NVMe）上的大于内存的 DBMS 工作负载，操作系统的页面驱逐机制无法扩展到多线程上。可能之前的文件 I/O 带宽不高，这个性能问题没有暴露出来。\nTLB shootdowns：发生在页面驱逐期间，当前线程所在 CPU Core 需要使远程 Core 的 TLB 映射失效时。刷新本地 TLB 的成本很低，但发出处理器间中断以同步远程 TLB 可以花费数千个 CPU 周期。\nExperimental Analysis # 使用 fio 测试 direct IO 的性能，硬件环境：\nAMD EPYC 7713 processor (64 cores, 128 hardware threads) 512 GB RAM, of which 100 GB was available to Linux (v5.11) for its page cache Samsung PM1733 SSD (rated with 7000 MB/s read and 3800 MB/s write throughput). Random Reads # 随机读 2TB 的磁盘内容来模拟 larger than memory OLTP 负载，OS page cache 只有 100GB，这个场景理论上来说会有 95% 的 page fault。\n从测试结果来看，fio 基准测试表现稳定，每秒读取接近 900K 次，延迟和 NVMe 预期的 100us 差不多，说明 fio 已经充分利用里 NVMe SSD 的读能力。\nMADV_RANDOM 的表现非常不稳定：在前 27 秒内与 fio 初始表现相似，然后在大约五秒钟内降至接近 0，最后恢复到 fio 性能的一半。QPS 掉底是因为页面缓存已满，迫使操作系统开始从内存中逐出页面。右图 TLB shutdown 也能大致匹配上。\nTLB shootdown 信息可以在 /proc/interrupt 中观测到，TLB shootdown 开销大的原因有：\n发送一个处理器间中断来刷新每个核的 TLB 操作系统只使用一个进程（kswapd）来驱逐页面，在作者的实验中，该进程的 CPU 已经用满 操作系统必须同步页表，带来了极大的锁竞争开销 Sequential Scan # 作者首先使用单个 SSD 运行了实验，然后在 10 个 SSD 上使用 RAID 0 重新运行了相同的工作负载（注：软件 RAID 0 是一种 RAID 0 的实现方式，它将两个或多个磁盘驱动器组合在一起，以提高读写速度。RAID 0 不提供数据冗余，因此如果其中一个驱动器出现故障，则所有数据都将丢失）：\n图 3，fio 可以利用一个 SSD 的全部带宽，同时保持稳定的性能 图 3，mmap 的性能最初与 fio 类似，但可以再次观察到，在大约 17 秒页面缓存填满后，性能急剧下降。此外，如预期的那样，对于这种工作负载，MADV_NORMAL 和 MADV_SEQUENTIAL 比 MADV_RANDOM 表现更好。 图 4，观察到 fio 和 mmap 之间的性能差异约为 20 倍，和图 3 使用单个 SSD 的结果相比也几乎没有性能提升。 Related Work # 作者列举了几个使用 mmap 的研究方向，比如 failure-atomic msync、自己实现系统调用 kmmap 的 Tucana 和 Kreon，以及将 mmap 用在其他方面比如利用 mmap 将冷数据持久化到第二存储等。\n最后作者认为像 pointer swizzling 这样设计实现轻量级 buffer manager 才是正确的技术方向，其中两篇 paper 分别来自 TUM 继 Hyper 之后的新系统 LeanStore 和 Umbra，后面有时间详细研究下：\n《In-Memory Performance for Big Data》，2014，VLDB 《LeanStore: In-Memory Data Management beyond Main Memory》，2018，ICDE 《Umbra: A Disk-Based System with In-Memory Performance》，2020，CIDR ","date":"31 March 2023","permalink":"/posts/cidr-2022-mmap-problems/","section":"Posts","summary":"依稀记得 CMU 有篇 paper 讲了使用 mmap 的各种问题，好奇之前大家使用 mmap 过程中遇到了哪些问题，于是抽空读了这篇论文，加深了我对数据存储、文件 IO 的理解。","title":"[CIDR 2022] Are You Sure You Want to Use MMAP in Your Database Management System?"},{"content":" Disk I/O # Managing Non-Volatile Memory in Database Systems, 2018, SIGMOD Design Tradeoffs of Data Access Methods, 2016, SIGMOD Designing Access Methods: The RUM Conjecture, 2016, EDBT The Five Minute Rule 20 Years Later and How Flash Memory Changes the Rules, 2008, ACM Queue The 5 Minute Rule for Trading Memory for Disc Accesses and the 5 Byte Rule for Trading Memory for CPU Time, 1987, SIGMOD B Tree Families # Efficient Locking for Concurrent Operations on B-Trees, 1981, TODS The Ubiquitous B-Tree, 1979 Buffer Management # Virtual-Memory Assisted Buffer Management, 2023, SIGMOD Memory-Optimized Multi-Version Concurrency Control for Disk-Based Database Systems, 2022, VLDB Are You Sure You Want to Use MMAP in Your Database Management System?, 2022, CIDR Spitfire: A Three-Tier Buffer Manager for Volatile and Non-Volatile Memory, 2021, SIGMOD In-Memory Performance for Big Data, 2014, VLDB Log \u0026amp; Recover # Rethinking Logging, Checkpoints, and Recovery for High-Performance Storage Engines, 2020, SIGMOD FineLine: log-structured transactional storage and recovery, 2018, VLDB Scalable Logging through Emerging Non-Volatile Memory, 2014, VLDB Codec \u0026amp; Compression # Mostly Order Preserving Dictionaries, 2019, ICDE Adaptive String Dictionary Compression in In-Memory Column-Store Database Systems, 2014, EDBT Dictionary-based Order-preserving String Compression for Main Memory Column Stores, 2009, SIGMOD Integrating Compression and Execution in Column-Oriented Database Systems, 2006, SIGMOD How to Wring a Table Dry: Entropy Compression of Relations and Querying of Compressed Relations, 2006, VLDB LSM Tree \u0026amp; Its Variants # Revisiting the Design of LSM-tree Based OLTP Storage Engine with Persistent Memory, 2021, VLDB LSM-based Storage Techniques: A Survey, 2019, VLDBJ WiscKey: Separating Keys from Values in SSD-conscious Storage, 2016, FAST The Log-Structured Merge-Tree (LSM-Tree), 1996 Learned Index # From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees, 2020 Learning Multi-dimensional Indexes, 2019 The Case for Learned Index Structures, 2018 Compaction Optimizations # TODO\nColumn Store # Column Sketches: A Scan Accelerator for Rapid and Robust Predicate Evaluation, 2018, SIGMOD BitWeaving: Fast Scans for Main Memory Data Processing, 2013, SIGMOD Column Imprints: A Secondary Index Structure, 2013, in SIGMOD SQL Server Column Store Indexes, 2011, SIGMOD Cache Conscious Indexing for Decision-Support in Main Memory, 1999, VLDB Bitmap Index Design and Evaluation, 1998, SIGMOD Distributed Storage # Replication \u0026amp; Consistency # Ark: A Real-World Consensus Implementation, 2014, CoRR Replica Placement # Adaptive HTAP through Elastic Resource Scheduling, 2020, SIGMOD MorphoSys: Automatic Physical Design Metamorphosis for Distributed Database Systems, 2020, VLDB Autoscaling Tiered Cloud Storage in Anna, 2019, VLDB Automated Demand-driven Resource Scaling in Relational Database-as-a-Service, 2016, SIGMOD Transaction \u0026amp; Concurrenct Conctrol # Scalable and Robust Snapshot Isolation for High-Performance Storage Engines, 2022, VLDB Memory-Optimized Multi-Version Concurrency Control for Disk-Based Database Systems, 2022, VLDB An Empirical Evaluation of In-Memory Multi-Version Concurrency Control, 2017, VLDB Serializable Snapshot Isolation in PostgreSQL, 2012, VLDB Serializable Isolation for Snapshot Databases, 2009, TODS Making Snapshot Isolation Serializable, 2005, TODS A Critique of ANSI SQL Isolation Levels, 1995, SIGMOD Systems # Leanstore:\nLeanStore: In-Memory Data Management Beyond Main Memory, 2018, ICDE Umbra:\nUmbra: A Disk-Based System with In-Memory Performance, 2020, CIDR Distributed NoSQL Systems:\nPNUTS to Sherpa: Lessons from Yahoo!’s Cloud Database, 2019, VLDB Cassandra - A Decentralized Structured Storage System, 2010, SOSP PNUTS: Yahoo!’s Hosted Data Serving Platform, 2008, VLDB Dynamo: Amazon’s Highly Available Key-value Store, 2007, SOSP Bigtable: A Distributed Storage System for Structured Data, 2006, OSDI ","date":"10 March 2023","permalink":"/posts/readings-storage/","section":"Posts","summary":"Disk I/O # Managing Non-Volatile Memory in Database Systems, 2018, SIGMOD Design Tradeoffs of Data Access Methods, 2016, SIGMOD Designing Access Methods: The RUM Conjecture, 2016, EDBT The Five Minute Rule 20 Years Later and How Flash Memory Changes the Rules, 2008, ACM Queue The 5 Minute Rule for Trading Memory for Disc Accesses and the 5 Byte Rule for Trading Memory for CPU Time, 1987, SIGMOD B Tree Families # Efficient Locking for Concurrent Operations on B-Trees, 1981, TODS The Ubiquitous B-Tree, 1979 Buffer Management # Virtual-Memory Assisted Buffer Management, 2023, SIGMOD Memory-Optimized Multi-Version Concurrency Control for Disk-Based Database Systems, 2022, VLDB Are You Sure You Want to Use MMAP in Your Database Management System?","title":"Essential Readings on Database Storage"},{"content":"SQL 优化中，索引选择、Join Reorder、子查询优化是最难处理的几个问题。这是一篇 2001 年发表的经典论文，论文提出了 Apply、SegmentApply 算子和一批原子优化规则完成子查询去关联和相关优化。近期因为工作原因重温了这篇论文，我把我认为比较关键的一些内容整理出来，希望能帮助到各位朋友们。\n子查询和常用执行策略 # 子查询提高了应用开发的灵活性，简化了用户写 SQL 的心智负担，在 SQL 的 scalar expression 中随处可见，比如下面出现在 WHERE 子句中的例子，查询每个客户的订单总额，找出总额大于 1M 的客户。\n这样的子查询有很多执行策略，比如按照 SQL 描述的那样采用 correlated execution 的方式，读取一行 customer 的数据，替换子查询 where o_custkey = c_custkey 的 c_custkey 部分，然后执行子查询求每个客户的订单总和：\n也可以采用先 outer join 再聚合的方式，先得到每个 customer 的所有订单，然后再按照 c_custkey 求每个客户的订单总额，等价的 SQL 如下：\n还可以采用先聚合再 outer join 的方式，先对 orders 中的数据按照 o_custkey 聚合求出每个 customer 的订单总额，然后再和 customer 表 join 得到最终结果，等价的 SQL 如下：\n哪种策略更好取决于很多因素，比如如果 customer 表数量不大，orders 表在 o_custkey 上有个索引，那么也许采用 correlated execution 的方式会更好。而如果 customer 表的数据量很大，每个 customer 的订单量也很多，那先聚合再 outer join 也许效果会更好，反之可能先 outer join 再聚合会更好。\n如上图所示，这篇论文采用了原子的、正交的优化规则，结合 Volcano/Cascades 搜索框架同时枚举出这些可能的执行计划，再利用 CBO 选择最佳执行计划。为实现这一目的，需要先后经历这些过程：\nAlgebrize into initial operator tree：将 AST 转换成 SQL 算子树，这里面利用 Apply 算子代表 correlated execution 策略，任意复杂的子查询都可以通过 Apply 来分解表示。 Remove correlation：通过一系列的 Apply 下推和消除规则，完成关联子查询的去关联优化，消除 Apply 算子。 Simplify outerjoin：Apply 消除后通常会产生许多 outer join，可以利用过滤条件的 null-rejection 属性将 outer join 转换成 inner join，进而可以触发其他优化规则，比如 join reorder。 Reorder groupby：将 group by 操作下推到 join 之前，或者放到 join 之后执行，这样能够诞生出更多子查询执行策略，扩展 CBO 的优化空间。 我们先来看什么是 Apply 算子，再看看如何将 AST 中的子查询转换成 Apply。\nApply 算子的定义 # 论文在 1.3 小结给出了 Apply 算子的形式化定义，Apply 的语义启发自 LISP，可以简单看做一个 nested loop join：外层驱动表读取一行数据，执行内部的 subquery。和普通 Join 算子一样，Apply 连接左右两表的方式可以是 cross join、left outer join、left semi join、 left anti join 等，只要能产生满足 SQL 语义的正确结果就行。\n我们一开始提到的查询订单总额并过滤的 SQL：\n它转换出来的 Apply 算子如下：\n如果 Apply 里面没有关联子查询，它其实等价于 nested loop join。Apply 算子的执行过程大致如下：\n读取一行驱动表的数据，上面例子是 CUSTOMER 表。\n根据该行驱动表的数据更新 Apply 算子所在子树的关联变量，上面例子中，假设读取上来的一行数据中 C_CUSTKEY 得值是 123，那么 O_CUSTKEY=C_CUSTKEY 绑定上这个运行时读上来的值后就变成了 O_CUSTKEY=123 这样的 filter。\n执行 Apply 算子得到结果，返回给更上层的 SQL 算子。在上面的例子中，O_CUSTKEY=C_CUSTKEY 的 C_CUSTKEY 被更新后，Apply 算子为根的子 plan 就可以独立执行返回结果了。经过 SGb（scala group by，没有 group by key 仅返回一行结果的 group by 算子）得到一行数据，和外层 CUSTOMER 的这一行数据 join 后也仅产生一行数据，然后经过后续的 1000000\u0026lt;x 的过滤条件，完成后续计算。\n了解了 Apply 算子的定义，那如何通过 Apply 算子进行子查询的优化和执行呢。要完成子查询优化，我们需要先将 AST 转换成 Apply 算子。\n将 AST 改写成包含 Apply 的执行计划 # 子查询通常出现在 scalar expression 中，比如上面例子，在构造 WHERE 子句的 SELECT 算子时发现一个 scalar expression 内嵌了一个 subquery，这个 subquery expression 里的 statement 描述了这个 subquery 代表的 scalar value 是如何计算出来的。\nApply 改写的基本思路比较简单，只需要在使用这个 Subquery 的 SQL 算子之前构造好为其提供 Subquery 结果的 Apply 算子即可。在上面这个例子中，我们需要在 SELECT 算子之前构造好 Apply，执行计划变成：CUSTOMER -\u0026gt; Apply -\u0026gt; SELECT，其中 CUSTOMER 作为 Apply 算子的左子树，而 Subuquery 里的语句构造出来的执行计划作为 Apply 的右子树，原先在 SELECT 中的子查询也需要替换成其他等价的 scalar expression，比如当 Apply 是 left outer join 时，IN subquery 就可以替换成对右孩子（代表子查询执行结果）某个字段的 IsNotNull 函数。\n考虑到一方面子查询可能嵌套，另一方面一个 scalar expression 也可能包含多个子查询，而且我们在改写完子查询后还需要替换成新的 scalar expression，因此最好采用 bottom up 的方式递归构造子查询对应的 Apply 算子和新的用于替代当前它的新的 scalar expression：\nbottom up 的遍历 scalar expression 树，如果发现 child expression 已经被改写了，就更新其 child 信息\n遍历自己，如果需要进行子查询改写，则根据子查询的类型生成相应的 Apply 算子和改写后的 scalar expression。构造 Apply 时将当前 scalar expression 所在 SQL 算子的孩子节点作为新 Apply 算子的左孩子，子查询对应的执行计划作为新 Apply 算子的右孩子，根据 subquery 的语义选择适当的 join 类型（left outer join、cross join 等）。新的 scalar expression 使用构造出来的 Apply 算子的结果做为输入。\n通过这样 bottom up 的遍历后，我们就得到了最终的 Apply 算子和需要替换的 scalar expression，然后继续遍历 AST 的其他部分，构造剩余的 query plan，完成从 AST 到执行计划的转化。\n因为整个过程是递归的，每个子查询都会遍历到并转换成相应的 Apply 算子和 scalar expression，当这种方法应用到包含多个子查询的场景时，最终会生成一棵类似下面这样的 Apply 左深树：\nIN、NOT IN 的改写 # IN 可以改写成 Distinct + Apply：先通过 Apply 计算出 left outer join 的结果，然后将 Selection 中的子查询改为 Apply 产生的右表 column 的 IsNotNull 即可，如果是 NOT IN 就改写成 IsNull。\n比如 where t.id in (select id from s) 就改写成了：\n对子查询的结果加 Distinct 去重是为了确保驱动表的数据经过 left outer join 后不会发生膨胀，如果能 join 上则只会有一条数据产生，如果没有 join 上则会把右表部分填补成 NULL 后也产生一条结果。因此 Apply 的输出结果里有完整的左表数据，我们也能根据输出结果中右表部分的数据是否为 NULL 来判断是否 join 上（也就是 IN 子查询的结果应该是 true 还是 false）。\nEXISTS、NOT EXISTS 的改写 # 和上面 IN、NOT IN 大致相同。\ncompare predicate 的改写 # 保留原本的 compare operator，把相关的 parameter 用子查询的结果替换。子查询根据需要改写成 Aggregate 或者 MaxOneRow（新算子，用于确保用户没有写 group by 但是又要求结果只产生一行数据的情况）。\nApply 的下推和消除 # Appy 算子本身是可以执行的，但通常如果内表没有索引，或者外表的数据量非常大，Apply 的执行效率会非常低，为了搜索可能更优的执行计划，需要进行 Apply 相关的优化，将 Apply 转换成没有关联子查询的普通 Join，然后利用 Join 相关的优化规则搜索可能更优的执行计划。\n下面是论文给出的所有 Apply 消除会用到的原子优化规则。SQL Server 采用了 Volcano/Cascades 优化框架，每个优化规则只需要匹配一个 Plan 片段即可，因此 Apply 下推直至消除所需使用的优化规则都可以一遍遍的应用到新产生的执行计划中，直至优化结束：\n这些公式看起来复杂，但理解起来不难，可以把他们分为 2 类：一类是直接消除 Apply 算子的，可以认为是 Apply 消除算法的终止条件，另一类就是不断进行 Apply 下推寻找 Apply 消除机会的，我们分别来介绍他们。\n规则 1 - 2：Apply 消除 # 规则 1：当 Apply 的左表和右表没有任何 join 条件时，不管这是个什么 join（cross join、left outer join, left semi join、left anti join），都可以将 Apply 转换成普通 Join 算子。\n规则 2：对 Apply 而言，右孩子的 Filter 可以等价转换成 Apply 的 join 条件，完成这次转换后，Apply 就和普通 nested loop join 没有什么区别了，可以安全的将其转换成普通 Join 算子。\n需要注意的是：\n规则 1 和 2 能够生效的公共条件是 Apply 的右孩子中，E 代表的执行计划不包含 R 的相关列。 理论上来说 Apply 转换后的执行计划不一定什么情况下都更好，比如当左表的数据量很小，右表在相关联列上有索引，或者右表已经提前物化到了计算节点上时，可能 Apply 算子这种 nested loop 的执行方式会更好，之前在一些客户场景中遇见过这样的情况。保险的做法是把 Apply 算子代表的老执行计划也留下来，通过 CBO 来选择。怎么样让 CBO 选择更准确就是另一个问题了。 规则 3 - 9：Apply 下推，寻找消除的机会 # 规则 1 和 2 描述了 Apply 消除的终止条件，而规则 3-9 则描述了在各个场景下 Apply 如何下推，行成可以触发规则 1 和 2 的执行计划。\n规则 3 和 4 分别描述了 Apply 下推过 selection 和 pojection 的场景。 规则 5 和和 6 分别描述了 Apply 下推过 union [all] 和 intersect [all] 的场景 规则 7 描述了 Apply 下推过 cross join 的场景，需要注意的是： 下推后 R 表在左右两个 Apply 算子中分别被读了 1 次，如果底层没有采用 DAG 的 execution model 使得 R 表可以只读一次被多个 parent 复用可能会有很大的性能损耗，使得下推后的 Apply 不如下推前执行效率好。 下推后取代原始 Apply 的是一个 inner join，为了确保 R 表的每行数据仍然只会产生一行结果，这个 inner join 的 equal condition 需要在 R 表的某个唯一索引、主键或者其他可以唯一决定一行数据的字段上（比如 row id）。 规则 8-9 分别描述了 Apply 下推过 vector aggregation 和 scalar aggregation 的场景。 因为下推过 Aggregation 后要确保 R 的每行数据只产生一行结果，我们需要保证新产生的 Aggregation 的 group by key 也是某个能唯一确定 R 表一行数据的主键、唯一索引、或者类似 row id 一样的字段。 另外对于规则 9 来说，先后的 aggregation 分别是 scalar aggregation 和 vector aggregation，需要特别注意某些聚合函数对 empty set input 和 null input 的结果差异。 Apply 下推和消除的例子 # 与 Aggregate 相关的所有优化规则 # 在子查询改写成 Apply 算子，以及 Apply 算子下推、消除的过程中，会诞生 Aggregate 和 Join 算子，因此扩大了搜索空间，诞生了更多优化的可能。\n论文的 3 小结 “COMPREHENSIVE OF AGGREGATION” 详细介绍了 Aggregate 相关的优化。其中一些聚合相关的优化在另一篇比较早期的经典论文《 Eager aggregation and lazy aggregation》中也提到了，感兴趣的朋友可以去看看。\nReordering Aggregate Around Filter # 为啥是 reorder 不是单纯的 pull up 或者 push down，主要原因还是这两种策略都有各自的最佳场景。reorder 的目的是枚举出所有这些可能的执行计划，把选择交给 CBO 的代价模型去评估。\nwe can move a filter around a GroupBy if and only if all the columns used in the filter are functionally determined by the group- ing columns in the input relation.\n这个比较好理解，当且仅当 Filter 是对整个 group 过滤时才能将 Aggregate push down 或者将 Aggregate 从 Filter 下面 pull up。判断 function dependency 的简单方法是检测 Filter 使用的 column set 是否是 Aggregate 中 group by 的 column set 的子集。\nReordering GroupBy Around Join # 那什么情况下 Aggregate 可以完全的 push down 或 pull up 过 Join 算子呢。我们以 Aggregate pull up 为例：\nJoin 可以看做是先 Cross Join 再 Filter，因此要想使 Aggregate 能够 pull up 或 push down Join，也要求 Join Condition 使用的 column set 是 Aggregate 中 group by 的 column set 子集，满足 functional dependency。 左右两个执行计划的结果集大小需要保持一致。对于左边的执行计划来说，结果集的大小取决于 S 表的数据量和每一行 S 表数据能 join 上多少行 Aggregate 后的结果。这意味着 Aggregate pull up 后需要按照 S 表的某个主键或唯一索引以及原来的 R 表上的 group by 字段进行分组。 还有一点在 push down 时需要满足的条件：Aggregate 中聚合函数使用的 column set 必须全部来自 R 表，这样才能顺利将 Aggregate push 到 R 表上面。 因为 Join 看做是 Cross Join 后再 Filter，在 Aggregate pull up 或 push down 的时候，如果发现不满足条件的 join condition，我们可以单独抽取一个 Filter 出来计算这些 condition 使得 join condition 满足条件 1。\n对于 left semi join 或 left anti semi join，他们也可以看做是先 join 再 filter，因此 Aggregate push down 或 pull up 可以采用同样的方式。\nReordering GroupBy Around Outer Join # push down Aggregate 到 outer join 下面会稍微特殊一些。对于 outer join 来说，没有 match 上的数据会用 NULL 来填补，之后聚合函数对 NULL 值进行计算。需要注意的是，虽然大多数聚合函数（比如 sum）在输入是 NULL 时输出也是 NULL，但也有特殊情况，比如 count(*)，在输入为 NULL 的时候输出为 0。\n如果我们直接把 Aggregate 下推到右表上，没有 join 上的数据会输出 NULL，就和左边的执行计划在计算 count(*) 时输出 0 结果不一致了。因此我们在聚合下推后需要在原来 outer join 上面补充一个 Project 算子，用来模拟 NULL input 时各个聚合函数应该输出的结果。\nLocal Aggregate # 理论上来说，任何 Aggregate 都可以拆成 Local Aggregate + Global Aggregate 两阶段聚合的形式。当我们不能完全把 Aggregate 下推到 Join 下面时，适当的拆分 Local Aggregae 使 Local Aggregate 下推到 Join 下面，可以减少 Join 的数据量提升整体执行性能。\nLocal Aggregate 有一个很好的性质：它的 group by column 可以在原来 Global Aggregate 的基础上随意扩充，因为扩充 group by column 仅仅是进一步划分 Global Aggregate 一个 group 数据的子集，对最终的 Global Aggregae 来说不会产生正确性影响。我们可以利用这个特点来构造满足下推条件的 Local Aggregate：\n将 join condition 中使用的 R 表上的 column 添加到 Local Aggregate 的 group by column 中，使得 join condition 代表的 Filter 作用在 Local Aggregate 的 group 上。 对于 Global Aggregate 中的聚合函数，如果它使用了左表数据，可以把该聚合函数在 Local Aggregate 中改写成 count(*)，并在 Local Aggregate 后增加一个 Project 和 count(*) 的结果一起计算模拟出曾经 Global Aggregate 需要的 Local Aggregate 结果，比如 sum(s.a) 可以改写成 s.a*count(*)。 SegementApply # 什么是 SegmentApply # Segmented Apply 可以看做是一种 partitioned nested loop join。它将左表的数据按照指定的 column 先进行 partition，然后把每个 partition，也就是论文中说的 segment，作为输入交给右表，完成关联子查询的计算，最后返回这一个 partition 对应的结果。\n在关联子查询去关联后，经常发生两个几乎一样的 sub plan join 到一起的情况，比如 TPC-H Q17 去关联后的等价 SQL 如下，lineitem 表和 lineitem 表上的聚合结果发生了一次 join：\n从语义上来说，这个 join 是希望对 lineitem 表按照 l_partkey 分组，求出每组的 l_quantity 平均值，并输出低于平均值的 20% 的所有 lineitem 数据。用 SegmentApply 可以描述成下图所示的执行计划：\nTransform Join to SegmentApply # 观察 join condition，看是否是两个来自相同 table 或 sub plan 的相同 column 在做等值比较，找到这样的 pattern 后，这些 column 就可以用来作为 common sub plan 的 segment key。\n个人理解：判断 join condition 两个 column 是否是同一个 column 可以用 column 的血缘分析来解决。当 join 确定后，common sub plan 也确定了，比较粗糙的做法是直接认定 join 的左子树为 common sub plan ，然后看右孩子是否是 aggregate，以及这个 aggregate 的子树是否和 join 的左子树相等。\nMoving SegmentApply Around Joins # 回到上面 TPC-H Q17 的例子，如果我们能够先执行 lineitem 和 part 表的 join 再进行 SegmentApply，说不定 Join 能够裁剪一部分 lineitem 表的数据，使得后续的 SegmentApply 能够轻松一些，提升整体执行效率。这就要求我们能够变换 SegmentApply 和 Join 的执行顺序。\n要将 Join push down 到 SegmentApply 的左孩子下面，当且仅当 Join 中使用的 column 是 SegmentApply 的 segment key 和 T 表某个主键或唯一索引的 column 子集：\nJoin 可以看做是 Cross Join + Filter，我们要求下推前后 Join Filter 的效果保持一致，那么就需要这个 Join Filter 是作用在 segment 这个粒度上的，因此也就要求 Join Filter 使用的 column 是 segment key 的子集（满足 functional dependency） 如果 Join 中使用了其他 Filter 也有办法绕过，因为 Join 是 Cross Join + Filter，那么可以再单独提取一个 Filter 保留在原地，把不满足下推条件的 Join Condition 保留在这个 Filter 中 对于一行 R 表 match 多行 T 表的情况，如果 T 表有主键或唯一索引，我们就可以建立起主键或唯一索引到每一行数据的 functional dependency 关系，并且因为 Join 的 Filter 也是作用在 R 表的 segment key 和 T 表的这个主键或唯一索引上，我们可以认为 Join 的 Filter 是对 R 表的 segment key + T 表的主键所指向的更细粒度的 segment 进行过滤。因此 Join 下推后，这个 T 表的主键和唯一索引也需要作为 segment key 补充到新的 SegmentApply 上去，保证和原来执行计划的语义一致性。 经过这个优化，以及在 SegmentApply 中消除掉等价的 column 后，TPC-H Q17 的执行计划就变成了下图所示：\n总结 # 以上就是这篇论文的主体内容，论文还在 2.4 和 2.5 小结介绍了其他几种子查询的类型，比如当子查询出现在 SELECT 列表中时，为了确保 scalar function 的语义引入了 Max1Row 算子，感兴趣的朋友们可以去读一读原文，我们不在文章中继续展开了。\n将子查询改写成 Apply、SegmentApply 等算子使我们可以应付绝大多数场景的子查询问题，论文提出的各种优化规则将子查询优化的问题分治成了一个个小问题，也让代码实现和维护变的更加简单，非常值得学习。\n","date":"25 November 2022","permalink":"/posts/sigmod-2001-orthogonal-optimization-of-subqueries-and-aggregation/","section":"Posts","summary":"SQL 优化中，索引选择、Join Reorder、子查询优化是最难处理的几个问题。这是一篇 2001 年发表的经典论文，论文提出了 Apply、SegmentApply 算子和一批原子优化规则完成子查询去关联和相关优化。近期因为工作原因重温了这篇论文，我把我认为比较关键的一些内容整理出来，希望能帮助到各位朋友们。","title":"[SIGMOD 2001] Orthogonal Optimization of Subqueries and Aggregation"},{"content":" 1. 背景 # DuckDB 是我非常喜欢的一个数据库，它基于 libpg_query 实现了 SQL Parser，语法和 PostgreSQL 一致，内嵌 SQLite 的 REPL CLI，编译好后可直接运行 CLI 交互式输入 SQL 得到结果。架构简单、分析性能优秀、代码干净好读，极易上手。\n10 月初偶然间翻看 duckdb 的代码，发现他的执行引擎和计算调度采用了类似 Hyper 在《 Morsel-Driven Parallelism: A NUMA-Aware Query Evaluation Framework for the Many-Core Age》中提出的 Morsel-Driven 的方式，实现了 push-based execution model，它的 pipeline breaker 语义和也 Hyper 在《 Efficiently Compiling Efficient Query Plans for Modern Hardware》 中定义的一致，不同的是 Hyper 期望通过 LLVM JIT 的方式对数据一行一行计算使其尽量保存在寄存器中，DuckDB 采用向量化使一批数据尽可能保存在 CPU Cache 中。\n之前做 TiDB 时研究过很多 Hyper 和 Vectorize 的论文，也做过几次分享，一直希望实现一个简单 demo 验证下效果，正好 DuckDB 采用了类似实现，这就勾起了我浓烈的好奇心。因此利用周末时间研究了下 DuckDB 是如何实现 push-based execution model 的，这里分享给大家，希望帮助到同样感兴趣的朋友们。\n2. 执行框架概览 # DuckDB 启动时会创建一个全局的 TaskScheduler，启动 nproc-1（main 函数）个后台线程。这些后台线程启动后会不停地从位于 TaskScheduler 的 Task 队列中取出和执行 Task。DuckDB 通过这个后台线程池和公共 Task 队列完成了 Query 的并发执行。\nDuckDB 基于 Query 的物理执行计划 PhysicalOperator tree 构造了 Pipeline DAG。每个 Pipeline 代表了物理执行计划中一段连续的 PhysicalOperator 算子，由 source、operators 和 sink 构成。当且仅当 Pipeline 的所有 dependency 都执行完后，该 Pipeline 才可被执行。Pipeline 的 sink 代表了需要消费掉所有输入数据才能对外返回结果的 PhysicalOperator。Pipeline DAG 可以看做是另一种视角下的物理执行计划。\nDuckDB 为每个 Pipeline 构造多个 ExecutorTask 使得 Pipeline 可以被多个线程并发执行，Pipeline 的 source 和 sink 需要是并发安全的。后台线程取得 ExecutorTask 后会通过其中的 PipelineExecutor 执行 Pipeline，当执行完一个 ExecutorTask 后，Pipeline 的一个并发任务也就执行完了。\n为了正确调度和执行 Pipeline DAG 中所有的 Pipeline 和它们对应的 ExecutorTask，我们需要能够及时知道某个 Pipeline 的所有并发是否都执行完毕，在其父亲 Pipeline 的所有依赖都被执行完后及时调度父亲 Pipeline 的所有 ExecutorTask，DuckDB 采用了 Event 来生成和调度对应的 ExecutorTask。\n每个 Pipeline 的 Event 都记录了需要执行的总并发数和完成的并发数。在构造 Pipeline DAG 后，DuckDB 会为其构造一个对应的 Event DAG，Pipeline 通过 Event 完成了 ExecutorTask 的调度和执行。每当一个 ExecutorTask 完成，该 Event 的完成并发数就会加 1，当该 Pipeline 的所有 ExecutorTask 都完成后，Event 中的总并发数和已完成并发数相等，标志着该 Event 也完成，该 Event 会通知其父亲 Event，父亲 Event 一旦检测到所有 dependency Event 都执行完，就会调度自己的 ExecutorTask，从而驱动后续的 Pipeilne 计算。\nExecutorTask 中的 Pipeline 是以 push 的方式执行的：先从 Pipeline 的 source 获取一批数据，然后将该批数据依次的通过所有中间的 operators 计算，最终由 sink 完成这一批初始数据的最终计算。典型的 sink 比如构造 hash table：当前 Pipeline 的所有 ExecutorTask 执行完后，最终的 hash table 才构造好，才能用来 probe 产生结果。\n为了返回结果给客户端，当前 Query 的主线程会不断调用 root PipelineExecutor 的 pull 接口。需要注意的是，这个接口名字的 pull 指的仅仅是从最顶层 Pipeline 拿结果数据，在计算顶层 Pipeline 的时候仍然是从 source 到最后一个 PhysicalOperator push 计算过去的。root PipelineExecutor 拿到一批 source 数据代表着 root Pipeline 依赖的所有 PipelineTask 都执行完毕，之后 root PipelineExecutor 内部以 push 的方式执行完这一批数据得到结果，将结果返回给客户端，用户就可以看到 Query 执行结果了。\n以上就是 DuckDB 执行框架的大致介绍。因为要特殊考虑一些算子的特殊优化，所以实际实现会稍微复杂一些。比如 UNION ALL，DuckDB 会在一段 PhysicalOperator 链条上构造多个 Pipeline。考虑到 partitioned hash join 的高效实现，DuckDB 也会在一段 PhysicalOperator 链条上构造多个 Pipeline，和 UNION ALL 不同的是，这些 Pipeline 之间有执行顺序的依赖关系。最终构造出来的可能就是有多个 root 的 Pipeline DAG。\n本文以当前（2022-11-14）DuckDB master 分支的 commit 为例，学习 DuckDB push-based execution model 涉及到的关键代码路径，感兴趣的朋友可以试试 clone 代码编译和调试玩玩。在 DuckDB 中，Pipeline 的构造、Event 的调度都发生在 Executor::InitializeInternal() 函数中，本文后续的内容也将围绕这里面的关键函数展开，其中几个关键的函数为：\nroot_pipeline-\u0026gt;Build(physical_plan)：top-down 的构造 Pipeline DAG ScheduleEvents(to_schedule)：基于除了 root Pipeline 以外的其他 Pipeline 构造 Event DAG，完成初始 Event 和 ExecutorTask 的调度。 Executor::InitializeInternal() 函数的完整代码如下：\nvoid Executor::InitializeInternal(PhysicalOperator *plan) { auto \u0026amp;scheduler = TaskScheduler::GetScheduler(context); { lock_guard\u0026lt;mutex\u0026gt; elock(executor_lock); physical_plan = plan; this-\u0026gt;profiler = ClientData::Get(context).profiler; profiler-\u0026gt;Initialize(physical_plan); this-\u0026gt;producer = scheduler.CreateProducer(); // build and ready the pipelines PipelineBuildState state; auto root_pipeline = make_shared\u0026lt;MetaPipeline\u0026gt;(*this, state, nullptr); root_pipeline-\u0026gt;Build(physical_plan); root_pipeline-\u0026gt;Ready(); // ready recursive cte pipelines too for (auto \u0026amp;rec_cte : recursive_ctes) { D_ASSERT(rec_cte-\u0026gt;type == PhysicalOperatorType::RECURSIVE_CTE); auto \u0026amp;rec_cte_op = (PhysicalRecursiveCTE \u0026amp;)*rec_cte; rec_cte_op.recursive_meta_pipeline-\u0026gt;Ready(); } // set root pipelines, i.e., all pipelines that end in the final sink root_pipeline-\u0026gt;GetPipelines(root_pipelines, false); root_pipeline_idx = 0; // collect all meta-pipelines from the root pipeline vector\u0026lt;shared_ptr\u0026lt;MetaPipeline\u0026gt;\u0026gt; to_schedule; root_pipeline-\u0026gt;GetMetaPipelines(to_schedule, true, true); // number of \u0026#39;PipelineCompleteEvent\u0026#39;s is equal to the number of meta pipelines, so we have to set it here total_pipelines = to_schedule.size(); // collect all pipelines from the root pipelines (recursively) for the progress bar and verify them root_pipeline-\u0026gt;GetPipelines(pipelines, true); // finally, verify and schedule VerifyPipelines(); ScheduleEvents(to_schedule); } } 3. TaskScheduler 和后台线程池 # 在 DuckDB 启动时会创建一个全局的 TaskScheduler，在后台启动 nproc-1（main 函数）个后台线程，启动线程是在 TaskScheduler::SetThreadsInternal() 函数中进行的，从主线程启动线程池的调用堆栈如下，感兴趣的朋友们可以根据这些关键函数看看线程是如何启动起来的：\n(lldb) bt * thread #1, queue = \u0026#39;com.apple.main-thread\u0026#39;, stop reason = breakpoint 1.1 * frame #0: 0x00000001074e29ee duckdb`duckdb::TaskScheduler::SetThreadsInternal(this=0x00006060000015e0, n=12) at task_scheduler.cpp:239:4 frame #1: 0x00000001074e5335 duckdb`duckdb::TaskScheduler::SetThreads(this=0x00006060000015e0, n=12) at task_scheduler.cpp:199:2 frame #2: 0x0000000107099f45 duckdb`duckdb::DatabaseInstance::Initialize(this=0x0000616000000698, database_path=0x0000000000000000, user_config=0x00007ff7bfefd420) at database.cpp:159:13 frame #3: 0x000000010709f2ce duckdb`duckdb::DuckDB::DuckDB(this=0x0000602000000a90, path=0x0000000000000000, new_config=0x00007ff7bfefd420) at database.cpp:170:12 frame #4: 0x000000010709f689 duckdb`duckdb::DuckDB::DuckDB(this=0x0000602000000a90, path=0x0000000000000000, new_config=0x00007ff7bfefd420) at database.cpp:169:100 ... 后台线程启动后的主逻辑在 TaskScheduler::ExecuteForever() 中，在每个后台线程的生命周期内，它们会不停从 TaskScheduler 的公共队列中取出 Task，调用 Task::Execute() 函数完成 Task 的执行：\nvoid TaskScheduler::ExecuteForever(atomic\u0026lt;bool\u0026gt; *marker) { #ifndef DUCKDB_NO_THREADS unique_ptr\u0026lt;Task\u0026gt; task; // loop until the marker is set to false while (*marker) { // wait for a signal with a timeout queue-\u0026gt;semaphore.wait(); if (queue-\u0026gt;q.try_dequeue(task)) { task-\u0026gt;Execute(TaskExecutionMode::PROCESS_ALL); task.reset(); } } #else throw NotImplementedException(\u0026#34;DuckDB was compiled without threads! Background thread loop is not allowed.\u0026#34;); #endif } Pipeline 的各个 Task 就是这样被后台线程并发执行的。要想控制 Pipeline 之间的执行顺序和 Pipeline 内的并发度，只需要设计和控制好各个 ExecutorTask 入队的顺序即可。Pipeline 的执行主要依靠 ExecutorTask，各个算子如果需要自定义计算逻辑和调度规则也是通过实现新的 ExecutorTask 完成。\n4. ExecutorTask 和 Event 驱动的调度模型 # 在 Task 的执行框架内，后台线程会通过 ExecutorTask::Execute() 驱动当前 ExecutorTask 的执行。为了给各个 Pipeline 和 PhysicalOperator 提供灵活的执行方式，DuckDB 内各个 PhysicalOperator 可以各自实现特定的 ExecutorTask 用于完成自身特殊的计算任务和后续 Pipeline Task 的计算调度。ExecutorTask::Execute() 的执行会直接调用子类的 ExecutorTask::ExecuteTask() 函数完成当前 ExecutorTask 的实际执行。\n对于一般的 Pipeline 来说，会构造一个叫 PipelineTask 的 ExecutorTask 子类。PipelineTask::ExecuteTask() 的代码逻辑如下：\nTaskExecutionResult ExecuteTask(TaskExecutionMode mode) override { if (!pipeline_executor) { pipeline_executor = make_unique\u0026lt;PipelineExecutor\u0026gt;(pipeline.GetClientContext(), pipeline); } if (mode == TaskExecutionMode::PROCESS_PARTIAL) { bool finished = pipeline_executor-\u0026gt;Execute(PARTIAL_CHUNK_COUNT); if (!finished) { return TaskExecutionResult::TASK_NOT_FINISHED; } } else { pipeline_executor-\u0026gt;Execute(); } event-\u0026gt;FinishTask(); pipeline_executor.reset(); return TaskExecutionResult::TASK_FINISHED; } 在 PipelineTask::ExecuteTask() 中，通过 PipelineExecutor::Execute() 完成当前 ExecutorTask 的执行后它会去调用 Event::FinishTask() 函数进行 ExecutorTask 完成后各个 Event 子类自定义的收尾工作，在 Event::FinishTask() 函数如果发现当前 Event 的所有 Task 都执行完毕就会清理当前 Event 相关内容，并调用父亲 Event 的 CompleteDependency()：\nvoid Event::Finish() { D_ASSERT(!finished); FinishEvent(); finished = true; // finished processing the pipeline, now we can schedule pipelines that depend on this pipeline for (auto \u0026amp;parent_entry : parents) { auto parent = parent_entry.lock(); if (!parent) { // LCOV_EXCL_START continue; } // LCOV_EXCL_STOP // mark a dependency as completed for each of the parents parent-\u0026gt;CompleteDependency(); } FinalizeFinish(); } 在 Event::CompleteDependency() 中，如果发现所有 dependency Event 都已经执行完毕，则会开始调度执行父亲 Event 的 Task。如果父亲 Event 没有 task 需要执行，则会再调用父亲 Event 的 Finish() 函数直接在当前线程中完成父亲 Event 的执行和收尾：\nvoid Event::CompleteDependency() { idx_t current_finished = ++finished_dependencies; D_ASSERT(current_finished \u0026lt;= total_dependencies); if (current_finished == total_dependencies) { // all dependencies have been completed: schedule the event D_ASSERT(total_tasks == 0); Schedule(); if (total_tasks == 0) { Finish(); } } } 从上面代码可以看到 Event 调度 Task 是通过 Event::Schedule() 函数完成的，这是个 Event 的纯虚函数，不同的子类 Event 需要自行实现。Pipeline 执行过程中使用的 Event 类型不多，最常见的是：\nPipelineInitializeEvent：主要用来初始化当前 Pipeline 的 sink，会调度 1 个 PipelineInitializeTask PipelineEvent：主要用来表示 Pipeline 的执行操作，可能会调度多个 ExecutorTask 到执行队列中。PipelineEvent 的 Schedule() 函数主要调用 Pipeline::Schedule() 完成 ExecutorTask 的计算调度，这里不再展开，感兴趣的朋友们可以继续追踪代码看看其中的实现细节 PipelineFinishEvent：主要用来标记当前 Pipeline 执行结束，在 Event::Finish() 检测到当前 Event 结束，调用到 PipelineFinishEvent::FinishEvent() 时完成 Pipeline::Finalize()，用来做 Pipeline 的清理操作 PipelineCompleteEvent：用来更新 Executor 中已结束的 Pipeline 的 counter completed_pipelines，Executor 主线程会不断检测 completed_pipelines，当发现所有中间 Pipeline 都执行完后，主线程会开始执行 root Pipeline，返回结果给客户端。 我们在上面 Executor::InitializeInternal() 的函数中看到，DuckDB 的 Pipeline 分为了 2 部分：\n中间 Pipelines：包含所有除了 root 以外的 Pipeline。DuckDB 基于这些 Pipeline 之间的依赖关系构建了相应的 Event DAG，通过调度最底层没有任何依赖 Event 的 ExecutorTask 初始化了 TaskScheduler 的执行队列，进而催动了所有中间 Pipeline 的执行。 root Pipelines：在构建 Event DAG 的时候不会将这部分 Pipeline 考虑进去，这部分 Pipeline 也不会被 TaskScheduler 启动的后台线程异步执行。在完成中间 Pipeline 的初始调度后，主线程后续的工作和 root Pipeline 的执行过程我们在后面的小结来看。 Pipeline 的初始调度正是由主线程执行 Executor::ScheduleEvents() 触发的，正式的调度逻辑是由 Executor::ScheduleEventsInternal() 完成的，这个函数的大致逻辑如下。概括来说就是寻找没有任何 dependency 的 Event，通过执行这些 Event.Schedule() 构造 ExecutorTask，放入 TaskScheduler 的工作队列，激活后台工作线程，开始 Pipeline 执行以及其他 Event 和 ExecutorTask 的连锁反应：\nvoid Executor::ScheduleEventsInternal(ScheduleEventData \u0026amp;event_data) { auto \u0026amp;events = event_data.events; D_ASSERT(events.empty()); // create all the required pipeline events for (auto \u0026amp;pipeline : event_data.meta_pipelines) { SchedulePipeline(pipeline, event_data); } // set up the dependencies across MetaPipelines auto \u0026amp;event_map = event_data.event_map; for (auto \u0026amp;entry : event_map) { auto pipeline = entry.first; for (auto \u0026amp;dependency : pipeline-\u0026gt;dependencies) { auto dep = dependency.lock(); D_ASSERT(dep); auto event_map_entry = event_map.find(dep.get()); D_ASSERT(event_map_entry != event_map.end()); auto \u0026amp;dep_entry = event_map_entry-\u0026gt;second; D_ASSERT(dep_entry.pipeline_complete_event); entry.second.pipeline_event-\u0026gt;AddDependency(*dep_entry.pipeline_complete_event); } } // verify that we have no cyclic dependencies VerifyScheduledEvents(event_data); // schedule the pipelines that do not have dependencies for (auto \u0026amp;event : events) { if (!event-\u0026gt;HasDependencies()) { event-\u0026gt;Schedule(); } } } 5. PipelineExecutor 和 Pipeline 内基于 Push 的执行模型 # PipelineExecutor::Execute() 函数通过调用 Pipeline 中各个 PhysicalOperator 的相应接口，以 Push 的方式完成了当前 Pipeline 的执行，执行逻辑可以概括为：\n先调用 FetchFromSource() 从 Pipeline 的 source PhysicalOperator 中获取计算结果作为 source DataChunk，这里会调用 source 的 GetData() 接口。 再调用 ExecutePushInternal() 依次执行 Pipeline 中 operators 列表中的各个 PhysicalOperator 和最后一个 sink PhysicalOperator 完成这批数据后续的所有计算操作。对于普通 operator 会调用它的 Execute() 接口，对最后的 sink 会调用它的 Sink() 接口。PipelineExecutor::ExecutePushInternal() 可以看做是 Pipeline 内的数据消费者。 最后调用 PushFinalize() 完成当前 ExecutorTask 的执行，这里会调用 sink 的 Combine 接口，用以完成一个 ExecutorTask 结束后的收尾清理工作。 bool PipelineExecutor::Execute(idx_t max_chunks) { D_ASSERT(pipeline.sink); bool exhausted_source = false; auto \u0026amp;source_chunk = pipeline.operators.empty() ? final_chunk : *intermediate_chunks[0]; for (idx_t i = 0; i \u0026lt; max_chunks; i++) { if (IsFinished()) { break; } source_chunk.Reset(); FetchFromSource(source_chunk); if (source_chunk.size() == 0) { exhausted_source = true; break; } auto result = ExecutePushInternal(source_chunk); if (result == OperatorResultType::FINISHED) { D_ASSERT(IsFinished()); break; } } if (!exhausted_source \u0026amp;\u0026amp; !IsFinished()) { return false; } PushFinalize(); return true; } PhysicalOperator 同时包含了 source、operator、sink 所需要的所有接口，各个 PhysicalOperator 需要实现对应的接口完成相应的计算逻辑。比如 partitioned hash join 因为会分成 3 个阶段分别作为 sink、operator 和 source 角色，它同时实现了所有的接口。\n6. 主线程和 root Pipeline 的执行 # root Pipelines 比较特殊：在构建 Event DAG 的时候不会将这部分 Pipeline 考虑进去，这部分 Pipeline 也不会被 TaskScheduler 启动的后台线程异步执行，这部分 Pipeline 要想得到执行也需要等待所有中间 Pipeline 执行结束。\nroot Pipeline 的执行是主线程通过调用 PipelineExecutor 的 Execute 函数完成的。主线程通过 TaskScheduler 启动的多个后台线程，通过 Event 触发和调度新一轮 Pipeline 的 ExecutorTask，Pipeline 就能够被后台执行了。剩下的问题就是主线程如何知道中间 Pipeline 执行结束，以及如何执行 root Pipeline 拿到最终结果返回给客户端。另外各个 Pipeline 在异步执行过程中可能会遇到一些 ERROR，主线程如何及时知道这些 ERROR 并返回给客户端也是需要处理的一个问题。\n主线程完成中间 Pipeline 的初始调度后，因为 root Pipeline 在中间结果没有准备好之前也不能计算，这时为了加速查询的执行最好的办法就是主线程也参与到中间 Pipeline 的执行当中去。我们看到主线程会停留在 PendingQueryResult::ExecuteInternal() 的 while 循环这里：\nunique_ptr\u0026lt;QueryResult\u0026gt; PendingQueryResult::ExecuteInternal(ClientContextLock \u0026amp;lock) { CheckExecutableInternal(lock); while (ExecuteTaskInternal(lock) == PendingExecutionResult::RESULT_NOT_READY) { } if (HasError()) { return make_unique\u0026lt;MaterializedQueryResult\u0026gt;(error); } auto result = context-\u0026gt;FetchResultInternal(lock, *this); Close(); return result; } PendingQueryResult::ExecuteTaskInternal() 经过几次函数调用后最终会来到 PipelineExecutor::Execute() 函数。这个函数初始一看可能会比较绕，但想要实现的功能是：\n在所有中间 Pipeline 没有执行完之前一直和后台线程一起参与计算：如果从队列中取出来了一个 ExecutorTask 就尝试调用它的 Execute(TaskExecutionMode::PROCESS_PARTIAL) 函数完成小批量数据的计算，现在默认是 50 个 DataChunk。 如果所有 Pipeline 都执行完了，此时 completed_pipelines 与 total_pipelines（记录中间 Pipeline 的数量，不包含 root Pipeline）相等，Executor 会释放所有中间 Pipeline，标记 execution_result 为PendingExecutionResult::RESULT_READY。 在这个小 while 循环中，如果没有取到 task，或者执行了 Task 的小部分任务后，都会去检测其他线程执行过程中是否有 Error 产生，用户是否 cancel 了 query 等等，一旦遇到错误产生，就会分别通过 CancelTasks() 和 ThrowException() 取消后台异步 Task 的执行并将错误抛给主线程的上层。\nPendingExecutionResult Executor::ExecuteTask() { if (execution_result != PendingExecutionResult::RESULT_NOT_READY) { return execution_result; } // check if there are any incomplete pipelines auto \u0026amp;scheduler = TaskScheduler::GetScheduler(context); while (completed_pipelines \u0026lt; total_pipelines) { // there are! if we don\u0026#39;t already have a task, fetch one if (!task) { scheduler.GetTaskFromProducer(*producer, task); } if (task) { // if we have a task, partially process it auto result = task-\u0026gt;Execute(TaskExecutionMode::PROCESS_PARTIAL); if (result != TaskExecutionResult::TASK_NOT_FINISHED) { // if the task is finished, clean it up task.reset(); } } if (!HasError()) { // we (partially) processed a task and no exceptions were thrown // give back control to the caller return PendingExecutionResult::RESULT_NOT_READY; } execution_result = PendingExecutionResult::EXECUTION_ERROR; // an exception has occurred executing one of the pipelines // we need to cancel all tasks associated with this executor CancelTasks(); ThrowException(); } D_ASSERT(!task); lock_guard\u0026lt;mutex\u0026gt; elock(executor_lock); pipelines.clear(); NextExecutor(); if (HasError()) { // LCOV_EXCL_START // an exception has occurred executing one of the pipelines execution_result = PendingExecutionResult::EXECUTION_ERROR; ThrowException(); } // LCOV_EXCL_STOP execution_result = PendingExecutionResult::RESULT_READY; return execution_result; } 一旦 execution_result 的状态变为 RESULT_READY，就意味着我们结束了所有中间 Pipeline 的执行，Executor::ExecuteTask() 会一直返回 RESULT_READY，最外层的 for 循环也会退出，从而进入下一阶段，也就是执行 root Pipeline。root Pipeline 被 Executor 所持有，它的执行也是在 Executor::FetchChunk() 中完成的：\nunique_ptr\u0026lt;DataChunk\u0026gt; Executor::FetchChunk() { D_ASSERT(physical_plan); auto chunk = make_unique\u0026lt;DataChunk\u0026gt;(); root_executor-\u0026gt;InitializeChunk(*chunk); while (true) { root_executor-\u0026gt;ExecutePull(*chunk); if (chunk-\u0026gt;size() == 0) { root_executor-\u0026gt;PullFinalize(); if (NextExecutor()) { continue; } break; } else { break; } } return chunk; } 虽然从函数名来看 Executor 调用了 Pipeline::ExecutePull() 函数，但其实这个函数内部实现仍旧是 push 的方式，先从 source 拿到一批数据，然后再依次的经过所有 operators 的计算得到最终结果。\n7. Pipeline 的构造 # Pipeline 的执行框架我们已经大概了解，最后一个问题就是 PhysicalOperator tree 是如何转换成 Pipeline DAG 的了。Pipeline 主要由 source、operators 和 sink 这三部分构成，从物理执行计划划分 Pipeline 第一个遇到的问题是如何确定 Pipeline 的 sink 和 source。\nDuckDB 采用了和 Hyper 一样的 Pipeline breaker 定义：那些需要消化掉所有孩子节点的数据后才能进行下一步计算输出结果的算子。典型的比如构造 hash join 或 hash aggregate 的 hash table，或者 sort 和 TopN 算子的排序操作，需要完全消费掉孩子节点的数据 后，才能得到正确结果进行下一阶段的数据。\n算子的具体实现决定了 Pipeline 的构造。物理执行计划转成 Pipeline 是由其中的各个 PhysicalOperator 完成的，几个关键函数：\nExecutor::InitializeInternal()：把物理执行计划（PhysicalOperator tree）转成 Pipeline 的入口，所有构造出来的 Pipeline 都存储在该查询的 Executor 中。 PhysicalOperator::BuildPipelines()：构造 Pipeline 的是通过 top down 的遍历 PhysicalOperator tree 完成的，Pipeline 的 sink 会先被确定下来（要么是整个物理执行计划的根节点，要么是上一个 Pipeline 的 source 节点）。Executor 通过该函数遍历每个 PhysicalOperator，决定将其加入当前 Pipeline 的 operators 列表还是做为当前 Pipeline 的 source。遇到当前 Pipeline 的 source 时就需要结束构造当前 Pipeline 了，然后将该 source 作为下一个 Pipeline 的 sink，继续 top down 的遍历 PhysicalOperator tree 和构造新的 Pipeline。 PhysicalOperator::BuildChildPipeline()：切分 Pipeline，构造 Pipeline 之间的依赖关系。 PhysicalOperator::BuildPipelines() 是个虚函数，搜索代码可以看到，像 PhysicalJoin、PhysicalRecursiveCTE、PhysicalUnion 这些有多个孩子节点的以及一些比较特殊的算子都重载了这个虚函。其他没有重载该函数的算子，默认的 BuildPipelines() 函数如下：\nvoid PhysicalOperator::BuildPipelines(Pipeline \u0026amp;current, MetaPipeline \u0026amp;meta_pipeline) { op_state.reset(); auto \u0026amp;state = meta_pipeline.GetState(); if (IsSink()) { // operator is a sink, build a pipeline sink_state.reset(); D_ASSERT(children.size() == 1); // single operator: the operator becomes the data source of the current pipeline state.SetPipelineSource(current, this); // we create a new pipeline starting from the child auto child_meta_pipeline = meta_pipeline.CreateChildMetaPipeline(current, this); child_meta_pipeline-\u0026gt;Build(children[0].get()); } else { // operator is not a sink! recurse in children if (children.empty()) { // source state.SetPipelineSource(current, this); } else { if (children.size() != 1) { throw InternalException(\u0026#34;Operator not supported in BuildPipelines\u0026#34;); } state.AddPipelineOperator(current, this); children[0]-\u0026gt;BuildPipelines(current, meta_pipeline); } } } PhysicalOperator::BuildPipelines() 不仅构建了 PhysicalOperator 和 Pipeline 的关系，也构建了 Pipeline 之间的依赖关系：如果某个 PhysicalOperator 是 Pipeline breaker，那么它不仅会作为当前 Pipeline 的 source，也会作为下一个 Pipeline 的 sink，Pipeline breaker 算子隐含了 Pipeline 之间的计算先后关系，只有上游 Pipeline 完全完成计算后才能开启下游 Pipeline 的计算。\n一个简单的单表聚合为例，它的执行计划和对应的 Pipeline 可以表示成下图，其中 Pipeline 1 依赖 Pipeline 2：\n7.1. 从 PhysicalUnion 构造 Pipeline # 我们以 Union All 为例介绍一个稍微复杂有多个 child 的情况。DuckDB 的 Union All 用 PhysicalUnion 来表示，每个 PhysicalUnion 有 2 个孩子节点。如果用户 SQL 中有 N 个表 Union All，那么就会构造出 N-1 个 PhysicalUnion 算子。PhysicalUnion 仅仅用来汇总多个数据源，传递孩子节点的数据给它的父节点完成计算。\nPhysicalUnion 有多个 child 数据源，意味着 PhysicalUnion 往下 top down 构造 Pipeline 的时候需要分别给各个孩子节点传递不同的 Pipeline，那这 2 个 Pipeline 的 sink 应该是什么呢。考虑到 PhysicalUnion 没有计算逻辑仅汇总数据的特殊性，DuckDB 让这 2 个 Pipeline 共享当前传递过来的 Pipeline 的 sink 和当前的 operators 列表，然后各自在自己的 operators 列表中新增自己的算子，设置自己的 sink。\n这样的 Pipeline 分裂可以使 PhysicalUnion 父节点的计算逻辑和对应的中间状态在这 2 个 Pipeline 之间复用，虽然 PhysicalUnion 孩子节点的计算逻辑位于不同 Pipeline 之间各自独立产生计算结果，但 PhysicalUnion 之后的计算逻辑和中间状态在不同 Pipeline 之间是共用的，可以确保计算的正确性。\n不过这样的 Pipeline 构造带来了额外的问题，我们上面提到 Pipeline breaker 确定了 Pipeline 之间的计算调度关系，并且每个 Pipeline 还可以独立设置自己的并发度。对于 PhysicalUnion 所处的 Pipeline 来说，这个 Pipeline 的 sink 同时属于多个 Pipeline（PhysicalUnion 分裂出来的），只有这些 Pipeline 都完成执行后才能执行他们的下游 Pipeline。所以后面在 Pipeline 调度的时候这里还需要特殊处理下。\n一个简单的 UNION ALL 为例，它的执行计划和对应的 Pipeline 可以表示成下图：\n7.2. 从 PhysicalJoin 构造 Pipeline # 理解了 Union All 的 Pipeline 构造，我们再来看看稍微复杂点的 Join。PhysicalJoin 的 Pipeline 构造相对来说要复杂一点，需要我们先大致了解下 DuckDB 中 PhysicalJoin 的实现。\nDuckDB 的 Hash Join 采用了 partitioned hash join，当数据量比较大的时候可以通过 repartition 将数据落盘避免 OOM，这个多线程版本的 partitioned hash join，主要分为 3 个阶段：\n并发读取和计算所有 build 端的数据，当所有数据都读完后检查总数据量是否能全部放在内存中，如果不能就将 build 端的数据 repartition，选出第一批能放在内存中的 partition 为它们构造 hash table，剩下的数据存放在磁盘上。 并发读取和计算所有 probe 端的数据，这时读上来的数据要么属于内存中的 partition，要么属于磁盘上的 partition，先把属于磁盘上的 partition 的数据落盘，用属于内存中的 partition 的数据去 probe 此时 build 端的放在内存中的 hash table，得到结果返回给上层。 并发处理磁盘上的数据：挑选一批 build 端能放入内存的 partition，构造 hash table，然后 probe 端去并发的 probe 得到结果进行下一步计算。循环这样的处理过程直到所有磁盘上的 partition 都 join 完成。 这 3 个过程也分别对应了 3 个基本的 Pipeline，可以表示成下图，其中 Pipeline 2 依赖 pipeline 1，Pipeline 3 依赖 Pipeline 2：\n8. DuckDB 执行模式的一些感受和思考 # 8.1. 计算调度的复杂性 # 这是一个最直观的感受。相比 Pull 模型，Push 模型在实现时需要多考虑如何控制 Pipeline 的计算调度，也需要考虑一个 Pipeline 内数据消费速度的问题（这个我们还没在本文涉及），这些代码增加了工程实现的复杂度，也增加了问题诊断的复杂度。\n8.1. PipelineBreaker 的作用 # 我喜欢把计算抽象为数据和计算两个部分，就像 CPU 的 L1 Cache 分为 L1D 和 L1I 一样，之前思考 Pipeline breaker 的时候更多是从计算性能角度，这次在思考 Pipeline 之间的依赖关系和 ExecutorTask 的调度时才意识到这个容易被忽略的地方：Pipeline 其实也反应了两个比较大的计算过程之间的先后关系。这个关系在 Volcano 模型的 Pull 中没有那么明显，只是写代码时为了保证正确性其实会应用这些依赖关系，比如 hash join 在 build side 没有结束时就不会对外返回结果。\n8.3. 除了带来性能提升外，这种并发 Push 执行模型还有其他优势吗？ # 从数据库、数据仓库执行引擎的经验来看，遇到最多的线上问题可以分为两类：查询跑的慢并发，以及查询吃的内存太狠导致查询自己或者进程 OOM。DuckDB 因为有了基于 ExecutorTask 的计算调度机制，我们就有机会从源头来控制：如果内存或 CPU 资源有限就少调度些 ExecutorTask 来执行。这样至少能够把查询失败的问题变成查询变慢的问题，然后查询变慢的时候再去看资源使用率满不满，这样至少能守住服务可用性的 SLO，也比较符合一般的问题排查思路。\n9. 参考材料 # issues/1583 Move to push-based execution model Push-Based Execution in DuckDB, by Mark Raasveldt: slides, video ","date":"14 November 2022","permalink":"/posts/duckdb-execution-model/","section":"Posts","summary":"1.","title":"[DuckDB] Push-Based Execution Model"},{"content":" 一些让人印象深刻的书 # 今年看的少听的多，发现樊登读书是个不错的平台，能够帮助快速掌握一本书的大致内容，判断是否有值得深度学习的兴趣，于是一口气把曾经想看但是没来得及看的书都听了一遍。\n《 产品故事地图》，教会了我如何讲故事，以及站在用户是主角的角度，主角的目标是什么，主角实现目标过程中遇到了什么挑战，做过哪些尝试，最终你的产品是如何帮助主角解决问题的，解决的效果怎么样等。这样的故事架构极大地帮助了我们了解用户了解产品。\n《 你要如何衡量你的人生》，这本书是由《创新者的窘境》的作者克里斯坦森所写，其中提到，人生最重要的三个方面：有意义能持续激励我们的工作和事业，良好的人际关系（包括伴侣、家庭和朋友），做正直的人永远不要尝试打开先例。\n《 管理成就生活》，关注结果，聚焦关键。这本书既有对如何进行有效管理的原则，也有实际的实操指南，是一本非常值得推荐的好书。\n《 深度思维：透过复杂直抵本质的跨越式成长方法论》，其中 5 why 和 5 so 的思考方法运用起来虽然痛苦，但却能极大逼迫我们找到最根本的问题和诉求。\n《 李诞脱口秀工作手册》，其中关于逐字稿的讲述至今记忆尤新，先写再删，删到不能再删为止，这个不仅适用于脱口秀，也适用于我们日常写作和说话，把重点突出出来。\n一个简单地罗列 # 思维、情绪、沟通相关的：\n《 次第花开》 《 深度思维：透过复杂直抵本质的跨越式成长方法论》 《 自卑与超越》 《 心力》 《 不妥协的谈判》 《 事实：用数据思考，避免情绪化决策》 《 用事实说话：透明化沟通的8项原则》 《 即兴演讲：掌控人生的关键时刻》 《 李诞脱口秀工作手册》 创新、创业、管理相关的：\n《 产品故事地图》 《 管理成就生活》 《 凤凰项目：一个IT运维的传奇故事》 《 从 0 到 1：开启商业与未来的秘密》 《支付战争》 《硅谷钢铁侠》 《创新者的窘境》 《一生的旅程》 《重新定义公司》 《颠覆者：周鸿祎自传》 《亚马逊编年史》 《创新与企业家精神》 《让大象飞》 《精益创业》 《指数型组织》 《权力：为什么只为某些人所拥有》 成长相关的：\n《你要如何衡量你的人生》 《能力陷阱》 《跨越式成长》 《终身成长》 《逆商》 《刻意练习》 《活出生命的意义》 《复杂》 《硅谷传奇：盖伊的创意启示录》 ","date":"28 January 2022","permalink":"/posts/2021-books/","section":"Posts","summary":"一些让人印象深刻的书 # 今年看的少听的多，发现樊登读书是个不错的平台，能够帮助快速掌握一本书的大致内容，判断是否有值得深度学习的兴趣，于是一口气把曾经想看但是没来得及看的书都听了一遍。","title":"2021 读书总结"},{"content":"","date":"28 January 2022","permalink":"/categories/summary/","section":"Categories","summary":"","title":"Summary"},{"content":" 整理自《高可用架构》的采访，原文：https://mp.weixin.qq.com/s/7nI_0jmr4Mo6zHzWCvTwpQ\n1. 简单介绍自己\n我叫张建，开源爱好者，PingCAP Engineering Manager，前阿里巴巴 ODPS 执行引擎研发工程师。在查询优化，分布式计算等方面有多年的研发经验。2017 年加入 PingCAP 从事 TiDB SQL 层的产品研发、架构改进、TiDB 社区建设等，目前在负责 TiKV 的产品演进。\n2. 聊聊你最近一年正在做的项目，它的技术价值怎样？它的行业发展状况是怎样？你负责项目的技术亮点和挑战能否展开讲讲？\n最近一年大量精力投入到了 TiDB SQL 层的产品研发中，一方面需要提升优化器的稳定性，另一方面需要增加和完善 TiDB SQL 功能。\n我们都不希望在业务高峰的时候因为某个 SQL 的执行计划变了（通常是变得更差）导致用户服务受到影响，因此需要尽可能确保优化器能够稳定的生成高效的执行计划。有很多方法来提升优化器的稳定性，比如通过 SQL Plan Management 为高频 SQL 固定一个执行计划，或者提升优化器的基数估算以及代价模型的准确度。前者能确保执行计划不变，但是需要考虑 SQL 太多给系统带来的内存和资源消耗问题。后者能帮助优化器选到高效、消耗资源少的执行计划，但至今没有一个标准的解法。参考 Guy Lohman 2014 年的博客 Is Query Optimization a “Solved” Problem? – ACM SIGMOD Blog，SQL 层的优化依旧是数据库领域悬而未决的难题。\n在今年发布的 TiDB 5.0 中 TiFlash 终于支持了 MPP 的分布式并行计算模型。这给 TiDB 优化器提出了更高的挑战：既要能同时产生 MPP 和 TiDB 的执行计划，还要能确保执行计划的稳定性。计算重的大查询要确保走到 MPP 模式，OLTP 的查询要能选到该选的索引。查询优化本质是个搜索算法，这个挑战相当于在扩大搜索空间后要求搜索出的执行计划尽可能高效和稳定。\n此外还有 TiDB 执行引擎的资源控制尤其是内存 OOM 问题。为了避免因为数据库 OOM 导致用户服务受到影响，我们需要尽可能控制爆炸半径，限制内存占用的情况下完成 SQL 执行。我们逐渐统计各个 SQL 算子的内存使用，当发现内存使用过大时，通过取消当前 SQL 的执行来控制爆炸半径，或者进行中间结果落盘来确保当前 SQL 执行成功。这里面有很多技术上的挑战，比如内存统计准确度的问题，中间结果落盘后执行效率的问题，全局内存如何管理的问题等。\n3. 在技术方案落地的过程中，你通常关注哪些问题？如何保证技术方案顺利实施？\n在设计技术方案之前，需要明确要解决哪些用户、哪些应用场景的哪些问题。作为一个通用数据库，TiDB 面临着广泛的用户和应用场景，如果对当前重点优化的场景和要解决的问题没有选择，那么很可能我们辛辛苦苦开发了几个月后仍然没有一个用户对产品满意，仍然各种应用场景都有各种问题，这就太可怕了。有放弃才有选择和聚焦，才能对要解决的问题和收益达成一致，才能让产品飞速地迭代和发展。\n除了技术方案，测试方案也非常重要。TiDB 已经拥有上千家用户，每个暴露出的缺陷都可能造成大范围的影响。为了提升产品的质量，每添加一个功能或改进一个模块都需要大量的测试确保结果不能错，性能不能回退，和其他组件或功能组合使用不会出现新问题，升级后不能 break 用户原有的应用程序等。我们会设计大量的功能测试、性能测试、系统测试、兼容性测试，覆盖到相关改动的方方面面。为了让方案落地的更快，这些测试用例最好提前设计，提前写好，面向测试用例，以终为始，驱动技术方案的开发和落地。\n一个项目组的同学可能分布在全球各个地方，有时区、语言的天然困难。项目开发过程中的进度和风险管理也会遇到些挑战。这种情况下每天一个定时的信息同步会非常有帮助，可以是所有人在一起的视频会议，也可以各自总结到某文档上异步的讨论，能帮助我们尽早发现和解决开发过程中的问题，确保技术方案顺利落地。\n4. 架构师在最近的技术变化的浪潮中，需要面对的挑战都有哪些？如何应对这些挑战？\n常态化的挑战是如何在快速变化的技术环境中，选择对解决的问题，然后寻找和选择对应的技术方案。基础软件永远面对两个大开口，应用场景的多元化，解决方案的多元化，这两个开口的动态性带来了具体的技术挑战。\n基础软件的应用场景特别多，产品在各个应用场景中会遇到着不一样的问题。技术方案的价值是解决用户问题，但比起具体的技术问题，我们更应该关注哪些场景中的问题要优先解决，哪些用户是这样的应用场景，他们分别遇到了什么问题。要回答这些问题需要我们往前走，从 oncall 问题中沉淀和总结，也去和具体的用户沟通了解业务架构，了解数据流转过程，未来的可能的发展变化等，在设计技术方案的时候，既要考虑解决眼下问题，也要尽可能考虑应对将来的问题。\n随着技术的发展，可能会有越来越多更好的方案来解决同样的问题，为了保持架构的先进性，也为了避免一些可能的坑，保持开放的心态持续学习也很重要。阅读论文或博客，与人交流讨论都是非常不错的途径。\n5. 在做技术选型的过程中，你经常考虑的问题有些？\n在查询优化方面，会重点关注执行计划的稳定性和可预测性。执行计划的稳定性前文提过，这里不再展开。执行计划的可预测性指的是系统对用户来说是可预期的，比如 SQL 符合某个特征优化器就一定会选择用户希望使用的索引，MySQL 在这一点上做的就不错，比如 order by 的字段如果有索引就一定会用上。当系统的行为可预测时，系统提供者就说得清楚应用开发最佳实践是什么，喜欢上手实践的用户也能很快根据系统的行为建立自己的知识体系以优化应用程序。\n数据库的可观测性也是非常重要的一个方面。我们需要有精简的指标来反映数据库是否运行良好，当系统的服务出现异常时，我们也要能快速定位到问题发生的原因。在 TiDB 产品发展过程中，我们逐渐为 SQL 执行过程中每个步骤都增加运行时信息的统计信息，完善了慢查询日志，通过 TiDB Dashboard 来帮助大家更直观更快的排查慢查询问题。还有 TiDB 的热力图，能够直观的显示出当前数据的读写热点，方便用户在不熟悉数据库原理的情况下快速定位热点问题。\n6. 云原生领域你看好哪个项目或技术，为什么？\n云原生数据库。\n为了应对流量高峰，通常会按照峰值流量所需的硬件资源来部署数据库服务。但这样的高峰期并不是持续存在，在非高峰期时，多余的硬件资源仍然在支撑服务，但却并没有发挥出它们的价值，白白耗费了许多资源。云原生数据库因其存储计算分离，扩展性极强的特点，使得用户可以按需扩展数据库服务所需要的硬件资源，达到按需计费降低数据库服务所需要的费用开销的目的。而且随着越来越多应用上云以及技术的进步，云服务价格也会越来越便宜，云原生数据库的成本还将进一步降低。\n此外云原生数据库在安全，故障恢复等方面也有极强的优势，这里不再展开。\n7. 请介绍下你这次在 GIAC 演讲的议题或者负责的专题内容\n这次主要为大家分享 TiDB 一路走到 5.0 的架构演进过程。会和大家讲讲我们是如何一步步构建 HTAP 数据库的，过程中遇到了哪些问题，如何解决的等。\n8. 对本次 GIAC 有什么寄语\n本次 GIAC 分会场众多，既可以专注于自己所处的技术领域，看看其他系统是如何解决类似问题的，也可以了解其他技术领域，关注新兴领域的技术趋势，以及一些新的应用问题，分别是如何解决的。希望大家在会议中充分交流，扩宽思路，互相提升。\n","date":"16 July 2021","permalink":"/posts/tidb-arch-evolution/","section":"Posts","summary":"整理自《高可用架构》的采访，原文：https://mp.","title":"TiDB 的架构进化之道"},{"content":" Experience # 2017/07 ~ Present, Senior Engineering Manager: As Engineering Manager of the TiDB SQL Team, responsible for the quality and improvement of the SQL layer (like session and privilege management, DDL, charset \u0026amp; collation, query optimizer, distributed execution, etc.), organization development, and the growth of the TiDB community.\n2015/07 ~ 2017/07, Development Engineer: Develop and optimize the Runtime component of Alibaba Cloud MaxCompute, including but not limited to SQL execution, query optimization, distributed computing framework.\nTalks # 2020/12/27, Beijing, 开放原子开源基金会, \u0026ldquo;How to build up TiDB community\u0026rdquo; 2020/07/29, Beijing, PingCAP Infra Meetup, \u0026ldquo;Horoscope and TiDB Query Optimizer\u0026rdquo; 2019/12/15, Shenzhen, OSC 源创会年终盛典, \u0026ldquo;TiDB 的架构设计与演进 - HTAP 之路\u0026rdquo; 2019/12/04, Beijing, GitHub 中国见面会, \u0026ldquo;TiDB and Open Source Community\u0026rdquo; 2019/05/29, Austin, Percona Live 2019, \u0026ldquo;Deep Dive into the TiDB SQL Layer\u0026rdquo; 2019/03/16, Beijing, Druid 中国用户组第 6 次 Meetup, \u0026ldquo;TiDB SQL Optimizer\u0026rdquo; 2018/12/22, Shanghai, PingCAP Infra Meetup, \u0026ldquo;An Introduction To TiDB SQL Layer\u0026rdquo; 2018/12/08, Beijing, ACMUG 2018 年会, \u0026ldquo;An Introduction To TiDB SQL Layer\u0026rdquo; 2018/10/20, Beijing, 掘金 \u0026amp;\u0026amp; 饿了么技术沙龙, \u0026ldquo;Deep Dive Into TiDB SQL Layer\u0026rdquo; 2018/02/03, Beijing, PingCAP Infra Meetup No.63, \u0026ldquo;深入了解 TiDB 新执行框架\u0026rdquo; 2017/09/16, Beijing, PingCAP Infra Meetup No.56, \u0026ldquo;MonetDB/X100: Hyper-Pipelining Query Execution\u0026rdquo; Contact Me # Twitter: https://twitter.com/zhangjian1012 Linkedin: https://www.linkedin.com/in/zhangjian1012/ Github: https://github.com/zz-jason Email(base64): empzYXJpZWxAZ21haWwuY29t ","date":"17 February 2021","permalink":"/about/","section":"Jian Zhang","summary":"Experience # 2017/07 ~ Present, Senior Engineering Manager: As Engineering Manager of the TiDB SQL Team, responsible for the quality and improvement of the SQL layer (like session and privilege management, DDL, charset \u0026amp; collation, query optimizer, distributed execution, etc.","title":"About Me"},{"content":"2020 年转瞬即逝，过去我可能会看论文比较多，今年稍微有些变化，读书比较多，因此利用这篇文章简单总结下 2020 年阅读过的书籍吧，很多都蛮不错的，后面找时间针对某些单独写一个阅读感受，标注重点。以下内容按阅读先后顺序排序：\n《 OKR 工作法》 《 组织能力的杨三角（第二版）》 《 运营之光》 《 增长黑客》 《 高绩效教练》 《 掌控：开启不疲、不焦虑的人生》 《 小岛经济学》 《 看懂世界格局的第一本书》 《 学会写作：自我进阶的高效方法》 《 敏捷革命》 《 创业维艰：如何完成比难更难的事》 《 开放式组织》 《 奈非文化手册》 《 贝佐斯的数字帝国：亚马逊如何实现指数级增长》 ","date":"30 December 2020","permalink":"/posts/2020-books/","section":"Posts","summary":"2020 年转瞬即逝，过去我可能会看论文比较多，今年稍微有些变化，读书比较多，因此利用这篇文章简单总结下 2020 年阅读过的书籍吧，很多都蛮不错的，后面找时间针对某些单独写一个阅读感受，标注重点。以下内容按阅读先后顺序排序：","title":"2020 读书总结"},{"content":" 前言 # 最近和朋友聊到事务隔离级别（Isolation Level），发现好多东西记得不牢靠。于是捡起 《 A Critique of ANSI SQL Isolation Levels》 重新阅读一把，记录阅读笔记，方便将来再次忘记的时候快速查阅（毕竟中文读的比英文快）。如果文中有描述不恰当的地方，欢迎批评指正。\nANSI SQL-92 根据事务并发执行时可能发生的各种异常现象来定义相应的事务隔离级别，期望通过设置合适的事务隔离级别来避免某些异常现象。在确保业务正确性的情况下，尽量满足性能指标（一般来说事务隔离级别越高，事务的并发性能也越低）。\n这篇论文在 1995 年发表于 SIGMOD，它指出了 ANSI SQL-92 在事务并发运行时的异常现象描述上存在的一些问题，提出了相应的纠正意见，给出了各事务隔离级别的强弱关系，讨论了快照隔离级别（Snapshot Isolation）的特点以及它和其他事务隔离级别的强弱关系。是理解事务隔离级别非常重要的一篇论文。其中讨论的所有异常现象和事务隔离级别可用下图总结：\n锁和事务隔离级别 # Degree 0：只需要确保每个写操作的原子性就行。在基于锁的事务并发控制中，写操作上 Short Duration Write Lock（写完立马释放），读不上锁。这种事务隔离级别下同时允许 Dirty Read 和 Dirty Write（将在后面介绍）。Degree 0-3 是 Jim Gray 在《 Granularity of Locks in a Shared Data Base》中定义的 4 种隔离级别。并分别和 Read Uncommitted、Read Committed 以及 Serializable 对应，后面不再单独介绍。\nP0 (Dirty Write): w1[x]\u0026hellip;w2[x]\u0026hellip;(c1 or a1)。事务 T1 先修改了数据 x，接着事务 T2 在事务 T1 提交或者回滚前又修改了 x，如果之后不管事务 T1 提交或者回滚了，数据的约束关系（比如 x=y 的约束）都很难维护起来。P0 是 ANSI SQL-92 中没有描述的一种异常现象，但因为很难保证事务一致性，因此作者认为所有隔离级别都应该避免 P0。 Read Uncommitted：在 Degree 0 的基础上能够避免 P0 的事务隔离级别。在基于锁的事务并发控制中，可以通过读不上锁，写上 Long Duration Write Lock（写的时候加锁，事务提交或者回滚时才释放）来实现 Read Uncommitted 隔离级别。\nA1 (Dirty Read): w1[x]\u0026hellip;r2[x]\u0026hellip;(a1 and c2 in any order)：事务 T1 先修改 x 的值，事务 T2 之后读取 x 的值，读到了事务 T1 未提交的修改值，之后事务 T1 回滚，T2 提交。 P1 (Dirty Read): w1[x]\u0026hellip;r2[x]\u0026hellip; (c1 or a1)：只要事务 T2 读到了正在执行的事务 T1 写入的数据，后面不管事务 T1、T2 是提交还是取消，就认为这个 Transaction History 属于 Dirty Read，而不是像 A1 那样，还要看事务是否 T1 回滚 T2 提交。如果某个事务隔离级别能够避免 P1，那么它一定也能同时避免 A1。 为了更好的理解 A1 和 P1，让我们来看这么一个 Transaction History：\nH1: r1[x=50] w1[x=10] r2[x=10] r2[y=50] c2 r1[y=50] w1[y=90] c1 为方面大家阅读，我把事务 T1 的所有操作都加粗了。T1 在进行一个转账事务，把 x 的 40 块钱转给 y，事务 T2 是一个只读事务。数据库对外需要维护的约束是 x+y=100。从只读事务 T2 来看，当 T1 写了 x=10 之后，事务 T2 才去读 x 和 y 的值，读出来 x+y=60，不满足 x+y=100 的约束。这个转账的例子中，它没有显式的违背 A1，因为事务 T1 在最后提交了，而不是回滚了。它也没有显式的违背后面会介绍到的 A2：事务 T2 读了 y，接着事务 T1 又写了 y，但事务 T2 在 T1 写 y 之前就提交了，以后不会再去读 y 了。类似的，它也没有显式的违背后面介绍到的 A3。但显然，这是一个 Non-Repeatable Transaction History。\n不过别担心，它会被包含作者修订后的 P1 中。P1 并没有限制事务 T1 和 T2 在之后应该是提交还是回滚，以及它们提交或者回滚的时间顺序是什么。P1 所代表的 Non-Serializable Transaction History 集合是 A1 的超集：任何两个事务，只要一个事务读到了另一个正在进行中的事务写的值，就属于 P1 所描述的异常行为。我们通过禁止 P1 所描述的所有异常行为就可以禁止掉上面这种不可串行化的 Transaction History。\n通过上面的分析，我们会发现 A1 所描述的 Transaction History 有遗漏，如果只是禁止 A1，仍然会有大量导致异常的 Transaction History 出现，因此 AISN SQL-92 中定义的 Dirty Read 异常行为应该使用 P1 来表示，尽可能多的把一些异常行为包含进去，而不只是狭隘的遵循字面意思。\nRead Committed：在 Read Uncommitted 的基础上能够避免 P1（也能避免 A1）。在基于锁的事务并发控制中，可以通过读的时候上 Short Duration Read Lock（读前上，读完立马释放），写的时候上 Long Duration Write Lock（写的时候加锁，事务提交或者回滚时才释放）来实现 Read Committed 隔离级别。\nP4 (Lost Update): r1[x]\u0026hellip;w2[x]\u0026hellip;w1[x]\u0026hellip;c1。事务 T2 对 x 的修改被事务 T1 后续对 x 的修改覆盖了，之后事务 T1 提交，从外界看来，事务 T2 对 x 的修改丢失了。 P4C (Lost Update): rc1[x]\u0026hellip;w2[x]\u0026hellip;w1[x]\u0026hellip;c1。P4 的 Cursor 版本。如果对 Cursor 陌生，可以看看 MySQL 或者 PostgreSQL 关于 Cursor 的文档。 Cursor Stability：在 Read Committed 的基础上能够避免 P4C（不是 P4）。在基于锁的事务并发控制中，通过对 Cursor 中的读锁进行特殊处理可以实现 Cursor Stability：读锁会一直持有到当前 Cursor 中，直到 Cursor 移动到下一个事务操作时才释放。如果是读满足某个条件的数据，只会上 Short Duration Predicate Lock，读完立马释放。当然，写的时候仍然需要上 Long Duration Write Lock。\nA2 (Non-Repeatable Read): r1[x]\u0026hellip;w2[x]\u0026hellip;c2\u0026hellip;r1[x]\u0026hellip;c1。事务 T1 先读 x 的值，接着事务 T2 修改了 x 或者删除了 x 的值并提交，之后事务 T1 再读一遍 x 的值，要么 x 的值和之前不一致，要么读不到 x 了。 P2 (Non-Repeatable Read): r1[x]\u0026hellip;w2[x]\u0026hellip;(c1 or a1)。只要事务 T2 修改了在这之前事务 T1 读到过的数据，后面不管事务 T1、T2 是提交还是回滚，就认为这个 Transaction History 属于 Non-Repeatable Read。如果某个事务隔离级别能够避免 P2，那么它一定也能同时避免 A2。另外从上面关于 P4 和 P4C 的定义来看，它们都存在一个事物先读 x 另一个事物再写 x 这种事务历史，所以这种能避免 P2 的事务隔离级别也一定能避免 P4 和 P4C。 类似上面 A1 和 P1 的分析，我们可以通过 H2（r1[x=50] r2[x=50] w2[x=10] r2[y=50] w2[y=90] c2 r1[y=90] c1）来证明，ANSI SQL 所描述的 Non-Repeatable Read 的异常现象应该用 P2 的形式化定义，而不是 A2。\nRepeatable Read：在 Cursor Stability 的基础上能够进一步避免 P2。在基于锁的事务并发控制中，可以通过读单个数据的时候上 Long Duration Read Lock，读满足某个条件的数据时上 Short Duration Read Lock，写的时候上 Long Duration Lock 来实现 Repeatable Read 隔离级别。\nA3 (Phantom): r1[P]\u0026hellip;w2[y in P]\u0026hellip;c2\u0026hellip;r1[P]\u0026hellip;c1。和 A2类似，不过这次事务 T1 先根据过滤条件 \u0026lt;search condition\u0026gt; 读取数据，事务 T2 接着新增了满足刚才过滤条件 \u0026lt;search condition\u0026gt; 的数据并提交，之后事务 T1 再次根据过滤条件 \u0026lt;search condition\u0026gt; 读一遍数据，结果和之前不一致。 P3 (Phantom): r1[P]\u0026hellip;w2[y in P]\u0026hellip;(c1 or a1)。和 A3 类似，事务 T1 先根据过滤条件 \u0026lt;search condition\u0026gt; 读取数据，事务 T2 接着新增了满足刚才过滤条件 \u0026lt;search condition\u0026gt; 的数据并提交，后面不管事务 T1、T2 是提交还是回滚，就认为这个 Transaction History 属于 Phantom。如果某个事务隔离级别能够避免 P3，那么它一定也能同时避免 A3。 同样的，类似 H1 H2，我们可以通过 H3（r1[P] w2[insert y to P] r2[z] w2[z] c2 r1[z] c1）来证明，ANSI SQL 所描述的 Phantom 的异常现象应该用 P3 的形式化定义，而不是 A3。\nSerializable：在 Repeatable Read 的基础上能够避免 P3 (Phantom)，是最高的事务隔离级别。在基于锁的事务并发控制中，可以通过读时上 Long Duration Read Lock，写时上 Long Duration Write Lock 来实现 Serializable 隔离级别。\n论文通过如下方式来比较事务隔离级别的强弱关系：\nL1 « L2：表示隔离级别 L1 比 L2 弱（低），L1 能够避免的异常现象是 L2 的子集。 L1 == L2：表示隔离级别 L1 和 L2 相等，L1 和 L2 能够避免的异常现象集合相等。 L1 »« L2：表示隔离级别 L1 和 L2 不可比较，L1 和 L2 分别能避免的异常现象集合没有包含关系，也不相等。 注意上面的方法只适用于比较 Non-Serializable（不可串行化）的事务隔离级别，不同实现方式的 Serializable 之间其实也是有区别的，导致它们能够允许的 Serializable Transaction History 存在些许区别，但不在论文的研究范围内，所以就不展开讲了。\n从上面介绍的几种隔离级别我们可以总结出：Degree 0 « Read Uncommitted Read « Read Committed « Cursor Stability « Repeatable Read « Serializable\n基于锁的事务并发控制下，根据能够避免的异常现象定义的事务隔离级别介绍完毕，下面让我们来看看 Snapshot Isolation。\nMVCC 和 Snapshot Isolation # Snapshot Isolation 是基于多版本事务并发控制的（MVCC）一种事务隔离级别。在 MVCC 系统中，每个值在写的时候都会被分配一个新的版本号（Version）。每个事务开启的时间点记为该事务的 Start Timestamp，提交时需要获取一个 Commit Timestamp，需要比所有正在进行或已完成事务的 Start 和 Commit Timestamp 都大。\n下面我们来看看 Snapshot Isolation 在事务隔离级别中应该处于什么位置。\n每个事物只能读到在它的 Start Timestamp 之前提交的其他事务的数据版本。事务 T1 能成功提交的前提是：在它的 Start Timestamp 和 Commit Timestamp 这段时间区间内，不存在任何在这期间提交的事务 T2 和 T1 修改了同样的数据。如果发生了这样的情况，事务 T1 应该回滚。这个特性叫 First-Committer-Wins。显然，这个特性可以可以用来避免 P0 (Dirty Write) 和 P4 (Lost Update)。因为事务读的时候只能读到 Start Timestamp 那一刻数据库的快照和当前事务进行过的修改，所以不难分析 Snapshot Isolation 能够避免 P1 (Dirty Read)。\n通过上面的分享，我们发性，Snapshot Isolation 是一个比 Degree 0，Read Uncommitted，Read Committed 和 Cursor Stability 更强的隔离级别。但是因为它不能避免避免下面的 H5，所以它比 Serializable 隔离级别弱：\nH5: r1[x=50] r1[y=50] r2[x=50] r2[y=50] w1[y=-40] w2[x=-40] c1 c2。假设数据库需要维护的约束是 x+y\u0026gt;0，单看 T1 和 T2 都能维护这个约束，而且因为它们并没有冲突的修改同一个值，所以 T1 和 T2 都能执行成功，但最终的结果却变成了 x+y=-80，违背了 x+y\u0026gt;0 的约束，破坏了事务一致性。 那接下里就是和 Repeatable Read 隔离级别相比了，我们会发现，它和 Repeatable Read 是不可比较的（Repeatable Read »« Snapshot Isolation），因为有些 Snapshot Isolation 能够避免的异常现象 Repeatable Read 不能避免，同时有一些 Repeatable Read 能够避免的异常现象 Snapshot Isolation 不能避免。\nA5 (Data Item Constraint Violation)：假设 C() 是两个数据 x 和 y 之间的一个约束条件，打破 C() 的异常现象可以分为两类：\nA5A (Read Skew): r1[x]\u0026hellip;w2[x]\u0026hellip;w2[y]\u0026hellip;c2\u0026hellip;r1[y]\u0026hellip;(c1 or a1)。事务 T1 先读了 x 的值，然后事务 T2 接着又更新了 x 和 y 的值并提交。这种情况下，T1 再去读 y 可能就会看到 x 和 y 不满足约束条件的现象。 A5B (Write Skew): r1[x]\u0026hellip;r2[y]\u0026hellip;w1[y]\u0026hellip;w2[x]\u0026hellip;(c1 and c2)。事务 T1 先读了 x 和 y 的值，发现满足约束条件，然后事务 T2 也读了 x 和 y，更新了 x 后再提交，接着事务 T1 如果更新 y 的值再提交。两个事务执行完后就可能发生 x 和 y 之间的约束被打破的情况。上面描述的 H5 就是一个 A5B 的例子，Snapshot Isolation 不能避免 A5B。 Repeatable Read 能够避免的异常现象 Snapshot Isolation 不能避免：因为 A5A 和 A5B 都存在一个事务修改另一个事务读过的数据的情况，所以如果我们能避免 P2（比如事务 T1 读了 x 后，其他事务都不能再写 x），那 A5A 和 A5B 就都能够避免。通过上面的结论，在修正后的 Isolation Level 中，Repeatable Read 是最低的能够避免 P2、A5A、A5B 的事务隔离级别。但 A5B 却不能被 Snapshot Isolation 避免。\nSnapshot Isolation 能够避免的异常现象 Repeatable Read 不能避免：Snapshot Isolation 不可能发生 A3 (Phantom) 所描述的异常行为：r1[P]\u0026hellip;w2[y in P]\u0026hellip;c2\u0026hellip;r1[P]\u0026hellip;c1，因为事务 T1 是看到的是 Start Timestamp 那一刻数据库快照，看不到未提交的 T2 的更改。但我们上面的分析中，Repeatable Read 不能避免 A3。\n另外还要注意到 Snapshot Isolation 不能够完全避免 P3。类似 A5B，事务 T1 根据某个条件读取上来一些数据后，做了修改，事务 T2 也根据同样的条件读取上来同一批数据，但是修改了其他的值，事务 T1 和事务 T2 接下来都能够提交成功。把这样一个多版本系统中的事务历史转换成单版本系统中的事务历史后，会发现这种历史属于 P3 定义的异常行为集合中，因此 Snapshot Isolation 不能完全避免 P3。\n总结 # 以这张图结束本文吧，它总结了各种异常现象和能够避免这些异常现象的事务隔离级别：\n","date":"12 September 2020","permalink":"/posts/sigmod-1995-ansi-sql-isolation-levels/","section":"Posts","summary":"前言 # 最近和朋友聊到事务隔离级别（Isolation Level），发现好多东西记得不牢靠。于是捡起 《 A Critique of ANSI SQL Isolation Levels》 重新阅读一把，记录阅读笔记，方便将来再次忘记的时候快速查阅（毕竟中文读的比英文快）。如果文中有描述不恰当的地方，欢迎批评指正。","title":"[SIGMOD 1995] A Critique of ANSI SQL Isolation Levels"},{"content":"整理自 《 降低 q-error，避免不优执行计划》\n","date":"3 August 2020","permalink":"/posts/vldb-2009-q-error/","section":"Posts","summary":"整理自 《 降低 q-error，避免不优执行计划》","title":"[VLDB 2009] Preventing Bad Plans by Bounding the Impact of Cardinality Estimation Errors"},{"content":"“SQL at SCALE”（出自 PingCAP 官网）是我们对 TiDB 的一个精简概括，而我们 TiDB SQL Engine Team 正是负责这 3 个单词中的 “SQL” 部分，其重要性可见一斑。SQL 在数据库中的大致处理流程可以简短概括为查询优化和执行，这期间涉及到 SQL Parser、优化器、统计信息和执行引擎等模块，他们就是 TiDB SQL Engine Team 目前所负责的模块。接下来我会用简短的篇幅向大家介绍 SQL Engine 的背景知识，以及我们在做的事情，面临的挑战等。\n关于查询优化 # 优化器是 SQL 引擎的大脑，负责查询优化。查询优化的主要工作概括起来很简单：搜索可行的执行计划，从中挑一个最好的。但要做好这两件事却是整个分布式数据库中最难的地方。\n1979 年 Selinger 发布了 \u0026ldquo; Access Path Selection in a Relational Database Management System\u0026quot;，正式拉开了 Cost Based Optimization 的帷幕，这篇论文也被视为 CBO 优化器的圣经。在这之后陆续出现了 Starburst（1988 年）， Volcano Optimizer Generator（1993 年）和 Cascades Framework（1995 年） 等，每年数据库三大顶会中也能看到不少查询优化相关的论文，整个优化器领域可谓是蓬勃发展。但即使如此，优化器也仍然有很多问题未能得到很好的解决，比如：\nGuy Lohman 2014 年在 “ Is Query Optimization a “Solved” Problem?” 中详细讲述的 SQL 算子结果集估算的难题。简单来说，要估算某个表需要扫多少行数据比较容易，但是要再估算更上层的 SQL 算子，比如 Join 或者 Join 之后再 Group By 的结果集有多大，这个就很难了。可以想象的是，估算误差会随着层数的增加而被放大，这个放大有时候是数量级的。此外还会出现负负得正的情况：明明估算错了，但是执行计划却是对的，纠正估算误差后，执行计划反而不对了。\nViktor Leis 等人在 2015 年的论文 How Good Are Query Optimizers, Really? 中讨论了优化器的另一朵乌云：Join Order。如果枚举所有可行的 Join Order，光是考虑左深树，N 个表的 Join 就可能有 N! 种执行计划。目前大家普遍采用一种妥协的方案：当参与 Join 的表比较少时用动态规划来确定 Join 的顺序，表比较多的时候用贪心或者遗传算法（PG 用的模拟退火）来做。但是采用什么样的动态规划和搜索算法也仍然处在热烈的研究中，而算子结果集的估算误差又进一步让这个问题雪上加霜，难上加难。\n作为一个从头到尾完全自己手写的优化器，TiDB 优化器的发展历史也算精彩：一开始我们是 Selinger 的 System R 模型，但是它的扩展性不是很好，搜索空间有限，维护成本也高，于是我们调研后，决定开发 Cascades 模型的新优化器（具体请参考： 十分钟成为 Contributor 系列| 为 Cascades Planner 添加优化规则 和 揭秘 TiDB 新优化器：Cascades Planner 原理解析）。在开发 Cascades Planner 的同时，我们还在做着另外一件非常重要的事情，提升优化器的稳定性：\n优化器的稳定性非常重要。去年之前我们经常遇到选错索引，或者干脆不选索引的问题。这个对业务的影响非常大，有时候一个慢查询可能拖垮整个集群，很多用户都吐槽过这个问题。后来调查研究后，我们引入了 Skyline Pruning 的剪枝优化，极大地提升了优化器选择索引的稳定性。参考： Proposal: Support Skyline Pruning。\n优化器的稳定性非常重要。要稳定的做出好的执行计划，统计信息非常非常关键。以前我们收集统计信息需要整个表都扫描一遍，扫的过程中用蓄水池算法做抽样。小表这样做没啥问题，大表也这样做就不行了：一方面担心对正在运行的业务造成影响，另一方面这种方式也很低效。于是我们结合 TiKV 的存储特点引入了 Fast Analyze，极大的提升了统计信息的搜集速度，也降低了对业务负载的影响。参考： PR/10214。\n优化器的稳定性非常重要。即使我们做了各种优化，解了各种 Bug，仍然会出现执行计划不优的问题。有条件的用户还可以改一改 SQL，那没条件的呢？比如 SQL 是通过第三方工具自动拼接的怎么改？为了解决这些问题，我们决定引入 SQL Plan Management，先实现了给 SQL 绑定执行计划的功能，使得不用更改业务也能抢救 SQL 的执行计划（ Issue/8935）；为了能够应对更多业务场景，更加细粒度的控制优化行为，我们还丰富了 SQL Hint 集合（ Issue/12304）；为了让 SQL 执行计划不会变差，我们为 SQL 确定了 Plan 的 Baseline，并且再往前走一步，我们做了 Baseline 的自动演进，使得执行计划不但不会变坏，而且只会变的越来越好。\n重要的事情重复 3 遍：优化器的稳定性非常重要。\n除了稳定性之外，还有性能问题：\n如何在尽量短的时间内消耗尽量少的硬件资源找到最佳执行计划？ 而目前 TiDB 正在 HTAP 之路上迈出坚实步伐，如何自动识别一条 SQL 是 AP 还是 TP 查询？ 如何为 TP 查询选择合理的索引？ 又如何为 AP 查询做出一个高效的分布式执行计划？ 可以预见，在这条道路上，优化器又将迎接新的困难和挑战，不断自我演进。\n关于查询执行 # 我的第一份工作从执行引擎开始，对它的感情异常深厚。执行引擎的目标是尽量利用计算资源，正确且快速的完成执行计划所描述的计算任务。光有看起来很完美的执行计划，却没有高效的执行引擎，整个 SQL 引擎也是废的。\n执行引擎也是一个热门的研究领域。最经典的执行模型当属 1994 年 Goetz Graefe 发表的 Volcano 迭代器模型，至今仍被广大数据库使用。原因很简单：接口抽象度高，扩展性好，实现起来简单。在数据量不大的 TP 请求中，这种模型足够用了。不过后来大家发现，随着数据量的上升，这玩意的执行性能很差：每完成一条数据的计算，要额外花费的很多 CPU 指令，计算效率非常低。于是有了后来的两大优化方向：Vectorization 和 Compilation，各自的代表分别为：2005 年 Marcin Zukowski 的 ” MonetDB/X100: Hyper-Pipelining Query Execution” 和 2011 年 Thomas Neumann 的 “ Efficiently Compiling Efficient Query Plans for Modern Hardware”。\n除了执行框架，如何利用 CPU 硬件特性优化各种执行算子也被广泛的讨论和研究。比如 2013 年的 “ Multi-Core, Main-Memory Joins: Sort vs. Hash Revisited” 这篇论文详细的探讨和对比了 Hash Join 和 Merge Join 的实现和性能，2015 年的 “ Rethinking SIMD Vectorization for In-Memory Databases” 这篇论文详细讨论了如何利用 SIMD 指令提升 SQL 算子性能。此外，底层软硬件技术的革新带来更多的优化机会，比如还有一系列论文来讨论如何适配 NUMA 架构，提升算子执行性能等。\n作为一个从头到尾完全自己手写的执行引擎，TiDB 执行引擎的发展也非常丰富多彩：一开始我们使用的是传统 Volcano 迭代器模型，后来我们和社区同学在 TiDB 2.0 版本中将其优化成了向量化模型（ Issue/5261），得到了巨大的性能提升： TPC-H 50G, TiDB 2.0 VS 1.0。之后我们和社区同学优化了聚合算子，重构了整个聚合函数的执行框架，执行性能又取得了飞跃的发展（Issue/6952）。再之后，我们和社区同学优化了表达式执行框架，使得表达式执行效率得到了 10 倍的性能提升，这期间 “ 10x Performance Improvement for Expression Evaluation Made Possible by Vectorized Execution and the Community” 这篇文章还占据了 Hacker News 的首页和 DZone Database 头版头条。\n稳定性和易用性也非常重要。为了解决用户 OOM 的问题，我们先后引入了内存追踪和记录的机制，后来干脆让算子落盘真正解决内存使用过多的问题，另外我们也在优化排查问题的调查工具，方便在出问题时快速定位和 workaround。\n如前文所说，目前 TiDB 正在 HTAP 之路上迈出坚实的步伐。执行引擎将在新的征程上肩负着新的使命。在分布式数据库中，广义上的执行引擎需要考虑更多的事情：任务如何调度？shuffle 如何优化？目前三套执行引擎（TiDB、TiKV、TiFlash）三套代码的维护成本如何降低？这些问题都等待着我们去探索和解决，可以预见，在这条道路上，执行引擎又将迎接新的困难和挑战，不断自我演进。\n期待你的加入 # 很开心，TiDB 的优化器和执行引擎是从零开始由我们的小伙伴们纯手工打造的，我们有很大的自由度来发挥自己的创造力；很紧张，上面这些列出来的种种问题我们都会遇到；很荣幸，我们能够和业界大牛、广大开源爱好者们一起来攻克这些难题；也很有成就感，我们能在广大 TiDB 用户的业务中看到这些改进为他们带来的价值。\n我们热爱开源，相信开源能够为我们的产品带来巨大的收益，也愿意为开源奉献，非常期待同样热爱开源的你的加入。如果你：\n热爱和相信开源，聪明且有激情； 敢于挑战上面那些难题，突破极限； 熟悉分布式系统、优化器和执行引擎的实现，熟悉 CPU 硬件特性； 有团队带领经验（加分项）。 那么我们就加入我们吧，一起向这些难题发起挑战，构建一个前沿、稳定的优化器和高效易用的执行引擎。 欢迎联系：zhangjian@pingcap.com\n","date":"25 March 2020","permalink":"/posts/tidb-sql-engine-team/","section":"Posts","summary":"“SQL at SCALE”（出自 PingCAP 官网）是我们对 TiDB 的一个精简概括，而我们 TiDB SQL Engine Team 正是负责这 3 个单词中的 “SQL” 部分，其重要性可见一斑。SQL 在数据库中的大致处理流程可以简短概括为查询优化和执行，这期间涉及到 SQL Parser、优化器、统计信息和执行引擎等模块，他们就是 TiDB SQL Engine Team 目前所负责的模块。接下来我会用简短的篇幅向大家介绍 SQL Engine 的背景知识，以及我们在做的事情，面临的挑战等。","title":"TiDB SQL Engine Team：纯手工打磨前沿的优化器和执行引擎｜PingCAP 招聘季"},{"content":"","date":"13 August 2017","permalink":"/categories/tidb/","section":"Categories","summary":"","title":"TiDB"},{"content":"explain 的用途非常广泛，比如用来查看某个表的信息，查看执行计划等等。explain 的语法可以参考 mysql 文档： EXPLAIN Syntax，另外 EXPLAIN, DESCRIBE, DESC 这几个关键字是等效的，所以很多时候我会直接使用 desc 而不是 explain\nexplain 这个东西，虽然各家都支持，但因为各自内部 planner 的实现不一样，导致 explain 结果的展现形式和内容各有区别。TiDB 在做 explain 的时候就不去考虑兼容 MySQL 了\n还是先讲讲为什么重构吧。主要原因还是 planner 经过了一轮重构后，新的 planner 不支持 explain。但是新老 planner 会共享一些 operator，比如 Projection，Selection，Limit 等。对于这些复用的 operator，可以称之为重构，对于那些新增的 operator，可以说是支持。anyway，我们的目的是给新 planner 支持 explain，以方便 TiDB 进行性能调优或者 debug\n之前实现 explain 的时候，采用的方法是给每个 operator 实现 json.Marsharl() 接口，但是我想把这个接口留着，用来打印详细的 operator 的 log 信息。为什么要这样做呢，其实现有的 explain 结果并不能很精确的定位问题，比如我们 explain 并没有展现每个 operator 的 schema 信息，也没有展现每个表达式返回结果的类型信息等。如果 plan 做错了，我们还是很难有最直观的证据说这个 plan 确实是做错了，得去 debug 打印日志来观察和推断\n在这里 #3809 我重新构建了新 planner 的 explain 的框架，在接下来的几个 pr 中： #3883 #3915 #3953，分别为各个 operator 实现了 ExplainInfo 的生成。Explain 重构完成后，主要有如下几个变化：\n不再使用 json.Marsharl()，而是使用新增的 ExplainInfo() 接口 构造 explain 结果的地方挪到了 planner，而不是之前的 executor 中 结果显示上，新的 explain 结果在格式上更加方便查看，不容易存在跨行情况了 重构之前的 explain 结果也是一个表格，但是会有三列，分别是 id，json，parent，其中 json 列是一个格式化好的 json 字符串，包括各种换行符制表符等等，显示上看起来会比较费劲。重构完成后，一条 SQL 的 explain 结果长这样：\nTiDB \u0026gt; EXPLAIN SELECT a.col2, a.col3, a.col4, a.col5 FROM tblA a, tblB b WHERE a.col1=b.col2 AND b.col1=4; +----------------+--------------+-------------------------------+------+----------------------------------------------------------+-------+ | id | parents | children | task | operator info | count | +----------------+--------------+-------------------------------+------+----------------------------------------------------------+-------+ | TableScan_10 | Selection_11 | | cop | table:b, range:(-inf,+inf), keep order:false | 0.8 | | Selection_11 | | TableScan_10 | cop | eq(b.col1, 4) | 0.8 | | TableReader_12 | IndexJoin_6 | | root | data:Selection_11 | 0.8 | | TableScan_9 | | | cop | table:a, range:(-inf,+inf), keep order:true | 1 | | TableReader_13 | IndexJoin_6 | | root | data:TableScan_9 | 1 | | IndexJoin_6 | Projection_5 | TableReader_12,TableReader_13 | root | outer:TableReader_13, outer key:b.col2, inner key:a.col1 | 0.8 | | Projection_5 | | IndexJoin_6 | root | a.col2, a.col3, a.col4, a.col5 | 0.8 | +----------------+--------------+-------------------------------+------+----------------------------------------------------------+-------+ 上面这条 SQL 来自 issue #3914，里面有完整的建表语句，感兴趣的读者可以试试\n大家可能在上面那个 issue 的 comment 中发现我贴的 dot 图了。这也是 explain 未来可以展示的结果之一，比如可以丰富 explain 的语法，以后直接 explain format=dot select ... 得到 plan 对应的 dot 文件的内容，然后上 webgraphviz 生成 dot 图；或者把结果存文件，用本机安装的 dot 命令生成 dot 图（dot -T png -O xxx.dot）\n","date":"13 August 2017","permalink":"/posts/refactor-explain/","section":"Posts","summary":"explain 的用途非常广泛，比如用来查看某个表的信息，查看执行计划等等。explain 的语法可以参考 mysql 文档： EXPLAIN Syntax，另外 EXPLAIN, DESCRIBE, DESC 这几个关键字是等效的，所以很多时候我会直接使用 desc 而不是 explain","title":"重构 EXPLAIN"},{"content":"","date":"12 August 2017","permalink":"/categories/query-execution/","section":"Categories","summary":"","title":"Query Execution"},{"content":"在 TiDB rc4 版本以前，Projection Elimination 是在 Physical Optimization 阶段完成以后做的。老的 Projection Elimination 只能消除那种做纯拷贝，不交换列的顺序，只改变列的名字的 Projection，Projection 消除后，他的 child 直接使用这个被消除后的 Projection 的 Schema。\n为什么要重构呢？一个原因现在的 Projection Elimination 是在 Physical Optimization 完成以后再做的，如果把这个优化挪到 Logical Optimization 阶段，将会给接下的其他类型的 Logical 或者 Physical 的 Optimization 提供更多更优的选择；另一个原因是现在的 Projection Elimination 并没有把能消除的 Projection 给消除干净，在 Logical Plan build 完成后，整个 Logical Plan 中可能存在的 Projection 可以分为如下几类：\n做表达式计算 剪裁或复制 child 的某些列（schema 中 column 的数量和 child 的不等） 改变 child 某些列的名字（schema 中 column 的数量和 child 的相等） 因为涉及到表达式计算，第一种是无论如何也不能消除的。老的 Projection Elimination 只能消除最后一种，但有些时候第二种我们也能消除。于是我在这 #3687 把 Projection Elimination 给重构了。\n做这个工作的时候我对 TiDB 的 Plan 也不是很熟悉，重构的过程艰辛无比，大家可以在 pr 的 Conversation 页面看到我和 @hanfei1991 @lamxTyler 的大量讨论，重构后，我们的 Projection Elimination 被拆成了两个阶段：一个在 Logical Optimization 阶段，一个在 Physical Optimization 完成后\n为什么要分成两个阶段来做呢？在生成 Physical Plan 的时候，某些 Logical Operator 的 schema 可能跟它对应的 Physical Operator 不同，一个典型的例子就是 LogicalJoin，如果生成的是 PhysicalIndexJoin，可能颠倒左右 child 的列在 schema 中出现的顺序，因此，在 Logical Projection Elimination 阶段，我们要保证整个 Plan 的最顶上一定要有最少一个 Projection，但在做完 Physical Optimization 后这个 Projection 又可能只是改变 child 某些列的名字，因此我们在做完 Physical Optimization 后还要消除一次 Projection，并且这个时候只会存在那种改变 child 某些列名的 Projection\nLogical Optimization 由一个个的 rule 构成，按照先后顺序分别执行这些 rule。Logical Projection Elimanation 对应的 rule 是 projectionEliminater，放在了 columnPruner 后面。还是按照前文说的，我们需要保留至少一个 Projection 来保序。整个 logical plan 需要保序，如果有 Union，那么以 Union 的 child 为 root 的 plan 也需要保序。Projection 的消除是从底向上的，在消除当前这个 Projection 的时候，以当前 operator 为 root 的 plan 已经完成了 Logical Projection Elimination 的过程，当判断到当前的 Projection 属于可以消除的那两种后，还需要判断消除当前这个 Projection 后整个 plan 是否可以保序，只有在能保序的情况下，这个 Plan 才能够消除，换句话说，只有在这个 Projection 和 root 或者 Union 的路径上还有 Projection，它才可以被消除\n最后，在 Physical Optimization 完成后，其实也是整个 Plan 做完了以后，再来扫一下尾。这时候只有第二种 Projection 能够消除了\nProjection Elimination 重构后，如果你用的是 rc3 版本的 TiDB 并且包含了这个 commit，那么可能会出现 bug。在 rc3，如果使用 tikv 的话，会走老 plan，这时候在 Logical Optimization 的 ppdSolver rule 里面会做 Join Reorder，Join Reorder 后不会去更新父亲的 schema，如果父亲不是 Projection，这个时候就可能出现 bug 了，如果遇到这种情况，请升级 rc4，因为 rc4 废弃老 planer 使用新 planner，这个问题在 rc3 并没有修复\n","date":"12 August 2017","permalink":"/posts/refactor-projection/","section":"Posts","summary":"在 TiDB rc4 版本以前，Projection Elimination 是在 Physical Optimization 阶段完成以后做的。老的 Projection Elimination 只能消除那种做纯拷贝，不交换列的顺序，只改变列的名字的 Projection，Projection 消除后，他的 child 直接使用这个被消除后的 Projection 的 Schema。","title":"重构 Projection Elimination"},{"content":"本文基于 MySQL 8.4\nPFS_engine_table_share_proxy # 代表一个 pfs 表。\nPFS_engine_table_proxy # 代表一个 table handle，每个 query 都会创建一个 handle，mysql pfs engine 通过该 handle 的接口和 pfs plugin 交互。接口定义如下：\n参考代码include/mysql/components/services/pfs_plugin_table_service.h\nopen_table # open table: OpenTable 由 table_plugin_table 构造时调用，open table 返回的 handle 保存在 plugin_table_handle 中供后续使用：\ntable_plugin_table::table_plugin_table(PFS_engine_table_share *share) : PFS_engine_table(share, nullptr), m_share(share), m_table_lock(share-\u0026gt;m_thr_lock_ptr) { this-\u0026gt;m_st_table = \u0026amp;share-\u0026gt;m_st_table; this-\u0026gt;plugin_table_handle = m_st_table-\u0026gt;open_table(\u0026amp;m_pos); /* Setup the base class position pointer */ m_pos_ptr = m_pos; } 需要注意：table_plugin_table 之后使用 plugin_table_handle 时不会做 null 检查，如果插件实现者在 open 函数中返回了 nullptr，需要其他接口实现的地方自己去做 null 检查，否则会导致空指针解引用的问题，比如：close table：\n~table_plugin_table() override { delete m_index; m_st_table-\u0026gt;close_table(this-\u0026gt;plugin_table_handle); } rnd_init # 在正式 scan 之前的初始化\nindex_next # Example 1: Table Scan # Example 2: Index Scan # ","date":"1 January 0001","permalink":"/posts/mysql-pfs-table/","section":"Posts","summary":"本文基于 MySQL 8.","title":""},{"content":"","date":"1 January 0001","permalink":"/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"1 January 0001","permalink":"/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"1 January 0001","permalink":"/tags/","section":"Tags","summary":"","title":"Tags"}]